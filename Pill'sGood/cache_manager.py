import os
import json
import pickle
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from pathlib import Path
import pandas as pd
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

class CacheManager:
    def __init__(self, cache_dir: str = "cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # ìºì‹œ í•˜ìœ„ ë””ë ‰í† ë¦¬ë“¤
        self.vector_cache_dir = self.cache_dir / "vectors"
        self.search_cache_dir = self.cache_dir / "search"
        self.embedding_cache_dir = self.cache_dir / "embeddings"
        self.matching_cache_dir = self.cache_dir / "matching"  # LLM ë§¤ì¹­ ê²°ê³¼ ìºì‹œ
        
        for dir_path in [self.vector_cache_dir, self.search_cache_dir, self.embedding_cache_dir, self.matching_cache_dir]:
            dir_path.mkdir(exist_ok=True)
    
    def _get_file_hash(self, file_path: str) -> str:
        """íŒŒì¼ì˜ í•´ì‹œê°’ì„ ê³„ì‚°í•˜ì—¬ ìºì‹œ í‚¤ë¡œ ì‚¬ìš©"""
        if not os.path.exists(file_path):
            return "nonexistent"
        
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def _get_data_hash(self, data: Any) -> str:
        """ë°ì´í„°ì˜ í•´ì‹œê°’ì„ ê³„ì‚°"""
        if isinstance(data, str):
            return hashlib.md5(data.encode()).hexdigest()
        elif isinstance(data, list):
            return hashlib.md5(str(sorted(data)).encode()).hexdigest()
        elif isinstance(data, dict):
            # ë”•ì…”ë„ˆë¦¬ì˜ ê²½ìš° í‚¤-ê°’ ìŒì„ ì •ë ¬í•˜ì—¬ ì¼ê´€ëœ í•´ì‹œ ìƒì„±
            sorted_items = sorted(data.items())
            return hashlib.md5(str(sorted_items).encode()).hexdigest()
        else:
            return hashlib.md5(str(data).encode()).hexdigest()
    
    def get_cache_key(self, source_type: str, identifier: str, data_hash: str = None) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        if data_hash:
            return f"{source_type}_{identifier}_{data_hash}"
        return f"{source_type}_{identifier}"
    
    def is_vector_cache_valid(self, source_type: str, file_paths: List[str]) -> bool:
        """ë²¡í„° ìºì‹œê°€ ìœ íš¨í•œì§€ í™•ì¸"""
        cache_key = self.get_cache_key(source_type, "vector_db")
        cache_dir = self.vector_cache_dir / cache_key
        hash_file = self.vector_cache_dir / f"{cache_key}_hash.json"
        
        if not cache_dir.exists() or not hash_file.exists():
            return False
        
        try:
            with open(hash_file, 'r') as f:
                stored_hashes = json.load(f)
            
            # íŒŒì¼ í•´ì‹œ ë¹„êµ
            current_hashes = {path: self._get_file_hash(path) for path in file_paths}
            return stored_hashes == current_hashes
        except:
            return False
    
    def is_docs_cache_valid(self, source_type: str) -> bool:
        """ë¬¸ì„œ ìºì‹œê°€ ìœ íš¨í•œì§€ í™•ì¸"""
        if source_type == "excel":
            cache_key = self.get_cache_key(source_type, "excel_docs")
        elif source_type == "pdf":
            cache_key = self.get_cache_key(source_type, "pdf_docs")
        else:
            return False
            
        cache_file = self.vector_cache_dir / f"{cache_key}.pkl"
        return cache_file.exists()
    
    def save_vector_cache(self, source_type: str, file_paths: List[str], vector_db: FAISS):
        """ë²¡í„° DB ìºì‹±"""
        cache_key = self.get_cache_key(source_type, "vector_db")
        cache_dir = self.vector_cache_dir / cache_key
        hash_file = self.vector_cache_dir / f"{cache_key}_hash.json"
        
        try:
            # FAISS ë²¡í„° DBë¥¼ ë””ë ‰í† ë¦¬ë¡œ ì €ì¥
            cache_dir.mkdir(exist_ok=True)
            vector_db.save_local(str(cache_dir))
            
            # íŒŒì¼ í•´ì‹œ ì €ì¥
            current_hashes = {path: self._get_file_hash(path) for path in file_paths}
            with open(hash_file, 'w') as f:
                json.dump(current_hashes, f)
            
            print(f"ğŸ’¾ {source_type} ë²¡í„° DB ìºì‹œ ì €ì¥ë¨")
        except Exception as e:
            print(f"âŒ {source_type} ë²¡í„° DB ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
            # ìºì‹œ ë””ë ‰í† ë¦¬ê°€ ë¶€ë¶„ì ìœ¼ë¡œ ìƒì„±ëœ ê²½ìš° ì‚­ì œ
            if cache_dir.exists():
                import shutil
                shutil.rmtree(cache_dir)
            if hash_file.exists():
                hash_file.unlink()
    
    def load_vector_cache(self, source_type: str, embedding_model=None) -> Optional[FAISS]:
        """ë²¡í„° DB ìºì‹œ ë¡œë“œ"""
        cache_key = self.get_cache_key(source_type, "vector_db")
        cache_dir = self.vector_cache_dir / cache_key
        
        if cache_dir.exists():
            try:
                # FAISS ë²¡í„° DBë¥¼ ë””ë ‰í† ë¦¬ì—ì„œ ë¡œë“œ (ë³´ì•ˆ í—ˆìš©)
                if embedding_model is None:
                    from langchain_openai import OpenAIEmbeddings
                    embedding_model = OpenAIEmbeddings()
                vector_db = FAISS.load_local(str(cache_dir), embedding_model, allow_dangerous_deserialization=True)
                print(f"ğŸ“‚ {source_type} ë²¡í„° DB ìºì‹œ ë¡œë“œë¨")
                return vector_db
            except Exception as e:
                print(f"âŒ {source_type} ë²¡í„° DB ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return None
    
    def get_search_cache_key(self, query: str, source_type: str) -> str:
        """ê²€ìƒ‰ ìºì‹œ í‚¤ ìƒì„±"""
        query_hash = hashlib.md5(query.encode()).hexdigest()
        return self.get_cache_key("search", f"{source_type}_{query_hash}")
    
    def get_search_cache(self, query: str, source_type: str) -> Optional[List[Document]]:
        """ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ ì¡°íšŒ"""
        cache_key = self.get_search_cache_key(query, source_type)
        cache_file = self.search_cache_dir / f"{cache_key}.pkl"
        
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    results = pickle.load(f)
                print(f"ğŸ“‚ {source_type} ê²€ìƒ‰ ìºì‹œ íˆíŠ¸: {query[:30]}...")
                return results
            except Exception as e:
                print(f"âŒ {source_type} ê²€ìƒ‰ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return None
    
    def save_search_cache(self, query: str, source_type: str, results: List[Document]):
        """ê²€ìƒ‰ ê²°ê³¼ ìºì‹±"""
        cache_key = self.get_search_cache_key(query, source_type)
        cache_file = self.search_cache_dir / f"{cache_key}.pkl"
        
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(results, f)
            print(f"ğŸ’¾ {source_type} ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ ì €ì¥ë¨")
        except Exception as e:
            print(f"âŒ {source_type} ê²€ìƒ‰ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_matching_cache_key(self, condition: str, medicines_info: Dict[str, Any]) -> str:
        """ì•½í’ˆ-ì¦ìƒ ë§¤ì¹­ ìºì‹œ í‚¤ ìƒì„±"""
        # ì¡°ê±´ê³¼ ì•½í’ˆ ì •ë³´ì˜ í•´ì‹œë¥¼ ì¡°í•©í•˜ì—¬ ìºì‹œ í‚¤ ìƒì„±
        condition_hash = hashlib.md5(condition.encode()).hexdigest()
        medicines_hash = self._get_data_hash(medicines_info)
        return f"matching_{condition_hash}_{medicines_hash[:16]}"
    
    def get_matching_cache(self, condition: str, medicines_info: Dict[str, Any]) -> Optional[Dict[str, bool]]:
        """ì•½í’ˆ-ì¦ìƒ ë§¤ì¹­ ê²°ê³¼ ìºì‹œ ì¡°íšŒ"""
        cache_key = self.get_matching_cache_key(condition, medicines_info)
        cache_file = self.matching_cache_dir / f"{cache_key}.pkl"
        
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    result = pickle.load(f)
                print(f"ğŸ“‚ ë§¤ì¹­ ìºì‹œ íˆíŠ¸: {condition} - {len(medicines_info)}ê°œ ì•½í’ˆ")
                return result
            except Exception as e:
                print(f"âŒ ë§¤ì¹­ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return None
    
    def save_matching_cache(self, condition: str, medicines_info: Dict[str, Any], matching_result: Dict[str, bool]):
        """ì•½í’ˆ-ì¦ìƒ ë§¤ì¹­ ê²°ê³¼ ìºì‹±"""
        cache_key = self.get_matching_cache_key(condition, medicines_info)
        cache_file = self.matching_cache_dir / f"{cache_key}.pkl"
        
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(matching_result, f)
            print(f"ğŸ’¾ ë§¤ì¹­ ê²°ê³¼ ìºì‹œ ì €ì¥ë¨: {condition} - {len(medicines_info)}ê°œ ì•½í’ˆ")
        except Exception as e:
            print(f"âŒ ë§¤ì¹­ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def save_excel_docs_cache(self, source_type: str, excel_docs: List[Document]):
        """Excel ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ìºì‹±"""
        cache_key = self.get_cache_key(source_type, "excel_docs")
        cache_file = self.vector_cache_dir / f"{cache_key}.pkl"
        
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(excel_docs, f)
            print(f"ğŸ’¾ Excel ë¬¸ì„œ ìºì‹œ ì €ì¥ë¨: {len(excel_docs)}ê°œ ë¬¸ì„œ")
        except Exception as e:
            print(f"âŒ Excel ë¬¸ì„œ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def load_excel_docs_cache(self, source_type: str) -> Optional[List[Document]]:
        """Excel ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ìºì‹œ ë¡œë“œ"""
        cache_key = self.get_cache_key(source_type, "excel_docs")
        cache_file = self.vector_cache_dir / f"{cache_key}.pkl"
        
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    excel_docs = pickle.load(f)
                print(f"ğŸ“‚ Excel ë¬¸ì„œ ìºì‹œ ë¡œë“œë¨: {len(excel_docs)}ê°œ ë¬¸ì„œ")
                return excel_docs
            except Exception as e:
                print(f"âŒ Excel ë¬¸ì„œ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return None
    
    def save_pdf_docs_cache(self, source_type: str, pdf_docs: List[Document]):
        """PDF ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ìºì‹±"""
        cache_key = self.get_cache_key(source_type, "pdf_docs")
        cache_file = self.vector_cache_dir / f"{cache_key}.pkl"
        
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(pdf_docs, f)
            print(f"ğŸ’¾ PDF ë¬¸ì„œ ìºì‹œ ì €ì¥ë¨: {len(pdf_docs)}ê°œ ë¬¸ì„œ")
        except Exception as e:
            print(f"âŒ PDF ë¬¸ì„œ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def load_pdf_docs_cache(self, source_type: str) -> Optional[List[Document]]:
        """PDF ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ìºì‹œ ë¡œë“œ"""
        cache_key = self.get_cache_key(source_type, "pdf_docs")
        cache_file = self.vector_cache_dir / f"{cache_key}.pkl"
        
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    pdf_docs = pickle.load(f)
                print(f"ğŸ“‚ PDF ë¬¸ì„œ ìºì‹œ ë¡œë“œë¨: {len(pdf_docs)}ê°œ ë¬¸ì„œ")
                return pdf_docs
            except Exception as e:
                print(f"âŒ PDF ë¬¸ì„œ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return None
    
    def clear_expired_cache(self, max_age_days: int = 7):
        """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
        cutoff_time = datetime.now() - timedelta(days=max_age_days)
        
        for cache_dir in [self.search_cache_dir, self.embedding_cache_dir]:
            for cache_file in cache_dir.glob("*.pkl"):
                if cache_file.stat().st_mtime < cutoff_time.timestamp():
                    cache_file.unlink()
                    print(f"ğŸ—‘ï¸ ë§Œë£Œëœ ìºì‹œ ì‚­ì œ: {cache_file.name}")
    
    def clear_all_cache(self):
        """ëª¨ë“  ìºì‹œ ì‚­ì œ"""
        for cache_dir in [self.vector_cache_dir, self.search_cache_dir, self.embedding_cache_dir, self.matching_cache_dir]:
            for cache_file in cache_dir.glob("*"):
                if cache_file.is_file():
                    cache_file.unlink()
                elif cache_file.is_dir():
                    import shutil
                    shutil.rmtree(cache_file)
        print("ğŸ—‘ï¸ ëª¨ë“  ìºì‹œ ì‚­ì œë¨")
    
    def clear_docs_cache(self, source_type: str):
        """íŠ¹ì • ì†ŒìŠ¤ì˜ ë¬¸ì„œ ìºì‹œë§Œ ì‚­ì œ"""
        if source_type == "excel":
            cache_key = self.get_cache_key(source_type, "excel_docs")
        elif source_type == "pdf":
            cache_key = self.get_cache_key(source_type, "pdf_docs")
        else:
            print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤ íƒ€ì…: {source_type}")
            return
            
        cache_file = self.vector_cache_dir / f"{cache_key}.pkl"
        if cache_file.exists():
            cache_file.unlink()
            print(f"ğŸ—‘ï¸ {source_type} ë¬¸ì„œ ìºì‹œ ì‚­ì œë¨")
        else:
            print(f"ğŸ“ {source_type} ë¬¸ì„œ ìºì‹œê°€ ì´ë¯¸ ì—†ìŒ")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì •ë³´"""
        # ë²¡í„° ìºì‹œëŠ” ë””ë ‰í† ë¦¬ë¡œ ì €ì¥ë˜ë¯€ë¡œ ë””ë ‰í† ë¦¬ ê°œìˆ˜ë¡œ ê³„ì‚°
        vector_cache_count = len([d for d in self.vector_cache_dir.iterdir() if d.is_dir()])
        
        stats = {
            "vector_cache_count": vector_cache_count,
            "search_cache_count": len(list(self.search_cache_dir.glob("*.pkl"))),
            "embedding_cache_count": len(list(self.embedding_cache_dir.glob("*.pkl"))),
            "matching_cache_count": len(list(self.matching_cache_dir.glob("*.pkl"))),
            "total_cache_size_mb": 0
        }
        
        total_size = 0
        for cache_dir in [self.vector_cache_dir, self.search_cache_dir, self.embedding_cache_dir, self.matching_cache_dir]:
            for cache_file in cache_dir.glob("*"):
                if cache_file.is_file():
                    total_size += cache_file.stat().st_size
        
        stats["total_cache_size_mb"] = round(total_size / (1024 * 1024), 2)
        return stats

# ì „ì—­ ìºì‹œ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
cache_manager = CacheManager() 

def print_cache_stats():
    """ìºì‹œ í†µê³„ë¥¼ ì¶œë ¥í•˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜"""
    stats = cache_manager.get_cache_stats()
    print("\nğŸ“Š ìºì‹œ í†µê³„:")
    print(f"  - ë²¡í„° ìºì‹œ: {stats['vector_cache_count']}ê°œ")
    print(f"  - ê²€ìƒ‰ ìºì‹œ: {stats['search_cache_count']}ê°œ")
    print(f"  - ì„ë² ë”© ìºì‹œ: {stats['embedding_cache_count']}ê°œ")
    print(f"  - ë§¤ì¹­ ìºì‹œ: {stats['matching_cache_count']}ê°œ")
    print(f"  - ì´ ìºì‹œ í¬ê¸°: {stats['total_cache_size_mb']}MB")
    print() 