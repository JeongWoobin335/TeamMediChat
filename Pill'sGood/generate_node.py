from qa_state import QAState
from answer_utils import generate_response_llm_from_prompt
from langchain_core.documents import Document
from config import PromptConfig
from prompt_utils import (
    get_role_definition, get_section_structure, get_common_instructions,
    get_medical_consultation_footer
)
import re
import json
from typing import List

def contains_exact_product_name(doc: Document, product_name: str) -> bool:
    return re.search(rf"\[ì œí’ˆëª…\]:\s*{re.escape(product_name)}\b", doc.page_content) is not None

def extract_medicine_from_context(conversation_context: str) -> list:
    """ëŒ€í™” ë§¥ë½ì—ì„œ ì•½í’ˆ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ (LLM ê¸°ë°˜)"""
    if not conversation_context:
        return []
    
    # LLMì—ê²Œ ì•½í’ˆëª… ì¶”ì¶œ ìš”ì²­
    extraction_prompt = f"""
ë‹¤ìŒ ëŒ€í™” ë§¥ë½ì—ì„œ ì–¸ê¸‰ëœ ì•½í’ˆëª…ë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

**ëŒ€í™” ë§¥ë½:**
{conversation_context}

**ì‘ë‹µ í˜•ì‹:**
ì•½í’ˆëª…ë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•´ì£¼ì„¸ìš”.
ì˜ˆ: ì•„ìŠ¤í”¼ë¦°, ì´ë¶€í”„ë¡œíœ, íŒŒë¼ì„¸íƒ€ëª°
"""
    
    try:
        response = generate_response_llm_from_prompt(
            prompt=extraction_prompt,
            temperature=0.1,
            max_tokens=200
        )
        
        # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì•½í’ˆëª…ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        medicines = [name.strip() for name in response.split(',') if name.strip()]
        return medicines
        
    except Exception as e:
        print(f"âŒ ì•½í’ˆëª… ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def extract_medicine_details_from_context(conversation_context: str) -> dict:
    """ëŒ€í™” ë§¥ë½ì—ì„œ ì•½í’ˆì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ (LLM ê¸°ë°˜)"""
    if not conversation_context:
        return {}
    
    # LLMì—ê²Œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ìš”ì²­
    extraction_prompt = f"""
ë‹¤ìŒ ëŒ€í™” ë§¥ë½ì—ì„œ ì–¸ê¸‰ëœ ì•½í’ˆë“¤ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

**ëŒ€í™” ë§¥ë½:**
{conversation_context}

**ì¶”ì¶œí•  ì •ë³´:**
- ì•½í’ˆëª…
- íš¨ëŠ¥/íš¨ê³¼
- ë¶€ì‘ìš©
- ì‚¬ìš©ë²•
- ì£¼ì˜ì‚¬í•­

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "medicines": [
        {{
            "name": "ì•½í’ˆëª…",
            "effects": ["íš¨ëŠ¥1", "íš¨ëŠ¥2"],
            "side_effects": ["ë¶€ì‘ìš©1", "ë¶€ì‘ìš©2"],
            "usage": "ì‚¬ìš©ë²•",
            "precautions": ["ì£¼ì˜ì‚¬í•­1", "ì£¼ì˜ì‚¬í•­2"]
        }}
    ]
}}
"""
    
    try:
        response = generate_response_llm_from_prompt(
            prompt=extraction_prompt,
            temperature=0.1,
            max_tokens=800
        )
        
        # JSON ì‘ë‹µ íŒŒì‹±
        try:
            result = json.loads(response)
            return result.get("medicines", {})
        except json.JSONDecodeError:
            print("âš ï¸ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŒ")
            return {}
            
    except Exception as e:
        print(f"âŒ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {}

def extract_medicine_context(conversation_context: str, medicine_name: str) -> str:
    """ëŒ€í™” ë§¥ë½ì—ì„œ íŠ¹ì • ì•½í’ˆ ì£¼ë³€ì˜ ë¬¸ë§¥ì„ ì¶”ì¶œ"""
    # configì—ì„œ ì„¤ì •ëœ ìµœëŒ€ ë¬¸ë§¥ ê¸¸ì´ ì‚¬ìš©
    context_length = 100 # ì˜ˆì‹œ ê°’, ì‹¤ì œ ì‚¬ìš© ì‹œ í™˜ê²½ ë³€ìˆ˜ë‚˜ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜´
    
    pattern = rf".{{0,{context_length}}}{re.escape(medicine_name)}.{{0,{context_length}}}"
    matches = re.findall(pattern, conversation_context, re.IGNORECASE)
    
    if matches:
        return matches[0]
    return ""

def extract_effect_from_context(context: str) -> str:
    """ë¬¸ë§¥ì—ì„œ íš¨ëŠ¥ ì •ë³´ë¥¼ ì¶”ì¶œ"""
    effect_keywords = ["íš¨ëŠ¥", "íš¨ê³¼", "ë„ì›€", "ê°œì„ ", "ì™„í™”", "ì¹˜ë£Œ", "ì˜ˆë°©"]
    
    for keyword in effect_keywords:
        if keyword in context:
            # í‚¤ì›Œë“œ ì£¼ë³€ ë¬¸ë§¥ ì¶”ì¶œ
            start = max(0, context.find(keyword) - 50)
            end = min(len(context), context.find(keyword) + 100)
            return context[start:end].strip()
    
    return "íš¨ëŠ¥ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

def extract_side_effects_from_context(context: str) -> str:
    """ë¬¸ë§¥ì—ì„œ ë¶€ì‘ìš© ì •ë³´ë¥¼ ì¶”ì¶œ"""
    side_effect_keywords = ["ë¶€ì‘ìš©", "ì£¼ì˜ì‚¬í•­", "ê²½ê³ ", "ì¦ìƒ", "ë¶ˆí¸"]
    
    for keyword in side_effect_keywords:
        if keyword in context:
            start = max(0, context.find(keyword) - 50)
            end = min(len(context), context.find(keyword) + 100)
            return context[start:end].strip()
    
    return "ë¶€ì‘ìš© ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

def extract_usage_from_context(context: str) -> str:
    """ë¬¸ë§¥ì—ì„œ ì‚¬ìš©ë²• ì •ë³´ë¥¼ ì¶”ì¶œ"""
    usage_keywords = ["ì‚¬ìš©ë²•", "ë³µìš©ë²•", "ìš©ë²•", "ë³µìš©", "ì„­ì·¨", "íˆ¬ì—¬"]
    
    for keyword in usage_keywords:
        if keyword in context:
            start = max(0, context.find(keyword) - 50)
            end = min(len(context), context.find(keyword) + 100)
            return context[start:end].strip()
    
    return "ì‚¬ìš©ë²• ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

def generate_final_answer_node(state: QAState) -> QAState:
    print("ğŸ¯ ìµœì¢… ë‹µë³€ ìƒì„± ë…¸ë“œ ì‹œì‘")
    print(f"ğŸ“Š ìƒíƒœ ì •ë³´:")
    print(f"  - final_answer: {state.get('final_answer', 'ì—†ìŒ')}")
    print(f"  - recommendation_answer: {state.get('recommendation_answer', 'ì—†ìŒ')}")
    print(f"  - relevant_docs: {len(state.get('relevant_docs', []))}ê°œ")
    print(f"  - external_parsed: {state.get('external_parsed', 'ì—†ìŒ')}")
    print(f"  - sns_results: {len(state.get('sns_results', []))}ê°œ")
    print(f"  - sns_analysis: {state.get('sns_analysis', 'ì—†ìŒ')}")
    print(f"  - conversation_context: {state.get('conversation_context', 'ì—†ìŒ')[:100] if state.get('conversation_context') else 'ì—†ìŒ'}...")
    print(f"  - user_context: {state.get('user_context', 'ì—†ìŒ')}")
    
    # âœ… ì´ë¯¸ final_answerê°€ ì„¤ì •ëœ ê²½ìš° (ìµœì‹  ì •ë³´ ìš”ì²­ ë“±)
    if state.get("final_answer"):
        print("âœ… ì´ë¯¸ final_answerê°€ ì„¤ì •ë˜ì–´ ìˆìŒ")
        return state

    # âœ… í–¥ìƒëœ RAG ë‹µë³€ì´ ìˆëŠ” ê²½ìš° ë¨¼ì € ë°˜í™˜í•˜ê³  ì¢…ë£Œ (ìµœê³  ìš°ì„ ìˆœìœ„)
    enhanced_answer = state.get("enhanced_rag_answer")
    print(f"ğŸ” enhanced_rag_answer í™•ì¸: {enhanced_answer is not None}, ê¸¸ì´: {len(enhanced_answer) if enhanced_answer else 0}")
    if enhanced_answer:
        # í™˜ê°ì´ ë°œê²¬ëœ ê²½ìš° ì¶”ê°€ ì •ë³´ ì„¹ì…˜ ìˆ˜ì • (ì œê±°í•˜ì§€ ì•Šê³  ê²½ê³  ë©”ì‹œì§€ ì¶”ê°€)
        hallucination_flag = state.get("hallucination_flag")
        hallucination_details = state.get("hallucination_details", {})
        if hallucination_flag is True:
            print("âš ï¸ í™˜ê°ì´ ë°œê²¬ë˜ì–´ ì¶”ê°€ ì •ë³´ ì„¹ì…˜ì— ê²½ê³  ë©”ì‹œì§€ ì¶”ê°€ ì¤‘...")
            import re
            # "ğŸ’¡ ì¶”ê°€ ì •ë³´" ë˜ëŠ” "ğŸ“° ìµœì‹  ì •ë³´" ì„¹ì…˜ ì°¾ê¸°
            additional_info_pattern = r'(ğŸ’¡\s*\*\*ì¶”ê°€ ì •ë³´\*\*|ğŸ“°\s*\*\*ìµœì‹  ì •ë³´\*\*)(.*?)(?=\n\n(?:ğŸ’¡|ğŸ¥|ğŸ“‹|âš ï¸|ğŸ’Š|âœ…|âŒ|$))'
            def add_warning(match):
                section_header = match.group(1)
                section_content = match.group(2)
                # ì„¹ì…˜ ë‚´ìš© ëì— ê²½ê³  ë©”ì‹œì§€ ì¶”ê°€
                warning = "\n\nâš ï¸ **ì°¸ê³ **: ìœ„ ì •ë³´ ì¤‘ ì¼ë¶€ëŠ” ìˆ˜ì§‘ëœ ìë£Œì—ì„œ ëª…ì‹œì ìœ¼ë¡œ í™•ì¸ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì¼ë°˜ì ì¸ ì•½ë¦¬í•™ ì§€ì‹ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                return section_header + section_content + warning
            
            modified_answer = re.sub(additional_info_pattern, add_warning, enhanced_answer, flags=re.DOTALL)
            if modified_answer != enhanced_answer:
                print("âœ… ì¶”ê°€ ì •ë³´ ì„¹ì…˜ì— ê²½ê³  ë©”ì‹œì§€ ì¶”ê°€ ì™„ë£Œ")
                enhanced_answer = modified_answer
            else:
                # íŒ¨í„´ì´ ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´ ë‹¤ë¥¸ íŒ¨í„´ ì‹œë„
                additional_info_pattern2 = r'(ğŸ’¡.*?ì¶”ê°€ ì •ë³´|ğŸ“°.*?ìµœì‹  ì •ë³´)(.*?)(?=\n\n|$)'
                modified_answer = re.sub(additional_info_pattern2, add_warning, enhanced_answer, flags=re.DOTALL)
                if modified_answer != enhanced_answer:
                    print("âœ… ì¶”ê°€ ì •ë³´ ì„¹ì…˜ì— ê²½ê³  ë©”ì‹œì§€ ì¶”ê°€ ì™„ë£Œ (íŒ¨í„´2)")
                    enhanced_answer = modified_answer
        
        print("âœ… enhanced_rag_answer ì‚¬ìš©")
        state["final_answer"] = enhanced_answer
        return state
    
    # âœ… ì•½í’ˆ ì‚¬ìš© ê°€ëŠ¥ì„± íŒë‹¨ì´ ìˆëŠ” ê²½ìš° (ê¸°ì¡´ ë°©ì‹)
    if state.get("usage_check_answer"):
        print("âœ… usage_check_answer ì‚¬ìš©")
        state["final_answer"] = state["usage_check_answer"]
        return state

    # âœ… ë³‘ë ¥ ê¸°ë°˜ ì¶”ì²œì´ ìˆëŠ” ê²½ìš° ë°˜í™˜ (ìš°ì„ ìˆœìœ„ 2ìˆœìœ„)
    if state.get("recommendation_answer"):
        print("âœ… recommendation_answer ì‚¬ìš©")
        state["final_answer"] = state["recommendation_answer"]
        return state

    # âœ… ì‹ ì•½ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš° (ì‹ ì•½ ê´€ë ¨ ì§ˆë¬¸ ì „ìš©)
    sns_results = state.get("sns_results", [])
    sns_analysis = state.get("sns_analysis", {})
    routing_decision = state.get("routing_decision", "")
    current_query = state.get("query", "") or state.get("original_query", "")
    
    if sns_results and len(sns_results) > 0 and routing_decision == "new_medicine_search":
        print("âœ… ì‹ ì•½ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ì‹œì‘")
        print(f"ğŸ“Š ì‹ ì•½ ê²€ìƒ‰ ê²°ê³¼: {len(sns_results)}ê°œ")
        
        # ì‹ ì•½ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±
        try:
            # YouTubeì™€ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²°ê³¼ ë¶„ë¦¬
            youtube_docs = [doc for doc in sns_results if doc.metadata.get("source") == "youtube"]
            news_docs = [doc for doc in sns_results if doc.metadata.get("source") == "naver_news"]
            
            # ê²€ìƒ‰ ì˜ë„ í™•ì¸
            intent = sns_analysis.get("intent", "latest_info")
            detected_drugs = sns_analysis.get("potential_drugs", [])
            
            # ì‹ ì•½ ê´€ë ¨ ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
            new_medicine_prompt = f"""{get_role_definition("expert")} ì‹ ì•½ ê´€ë ¨ ìµœì‹  ì •ë³´ë¥¼ ì‚¬ìš©ìì—ê²Œ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.

**ì‚¬ìš©ì ì§ˆë¬¸:**
{current_query}

**ê²€ìƒ‰ ì˜ë„:**
{intent}

**ê°ì§€ëœ ì•½í’ˆ/í‚¤ì›Œë“œ:**
{', '.join(detected_drugs) if detected_drugs else 'ì—†ìŒ'}

**ğŸ“º YouTube ì˜ìƒ ì •ë³´ ({len(youtube_docs)}ê°œ):**
"""
            
            # YouTube ì˜ìƒ ì •ë³´ ìˆ˜ì§‘ (ë§í¬ í¬í•¨)
            youtube_info_list = []
            for i, doc in enumerate(youtube_docs[:5], 1):
                title = doc.metadata.get("title", "ì œëª© ì—†ìŒ")
                summary = doc.metadata.get("summary", doc.page_content[:300])
                channel = doc.metadata.get("channel_title", "")
                video_id = doc.metadata.get("video_id", "")
                video_url = f"https://www.youtube.com/watch?v={video_id}" if video_id else ""
                
                new_medicine_prompt += f"\n{i}. **{title}**\n"
                if channel:
                    new_medicine_prompt += f"   ì±„ë„: {channel}\n"
                new_medicine_prompt += f"   ë‚´ìš©: {summary[:500]}...\n"
                
                youtube_info_list.append({
                    "title": title,
                    "channel": channel,
                    "url": video_url,
                    "summary": summary
                })
            
            new_medicine_prompt += f"\n**ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ ì •ë³´ ({len(news_docs)}ê°œ):**\n"
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ (ë§í¬ í¬í•¨)
            news_info_list = []
            for i, doc in enumerate(news_docs[:10], 1):
                title = doc.metadata.get("title", "ì œëª© ì—†ìŒ")
                description = doc.page_content.split("\në‚´ìš©: ")[1] if "\në‚´ìš©: " in doc.page_content else doc.page_content[:200]
                pub_date = doc.metadata.get("pub_date", "")
                link = doc.metadata.get("link", "") or doc.metadata.get("original_link", "")
                
                new_medicine_prompt += f"\n{i}. **{title}**\n"
                if pub_date:
                    new_medicine_prompt += f"   ë°œí–‰ì¼: {pub_date}\n"
                new_medicine_prompt += f"   ë‚´ìš©: {description[:400]}...\n"
                
                news_info_list.append({
                    "title": title,
                    "pub_date": pub_date,
                    "url": link,
                    "description": description
                })
            
            new_medicine_prompt += f"""
**ë‹µë³€ ìš”êµ¬ì‚¬í•­:**
1. ìœ„ì˜ ì˜ìƒê³¼ ë‰´ìŠ¤ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì‹ ì•½ ê´€ë ¨ ìµœì‹  ì •ë³´ë¥¼ ì œê³µ
2. {PromptConfig.COMMON_INSTRUCTIONS['natural_tone']}
3. ì´ëª¨ì§€ë¡œ ì„¹ì…˜ì„ ë‚˜ëˆ„ì–´ì„œ ë‹µë³€:
   - ğŸ“° **ìµœì‹  ë‰´ìŠ¤**: ë‰´ìŠ¤ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì‹ ì•½ì˜ ê°œë°œ í˜„í™©, ìŠ¹ì¸ ìƒíƒœ, ì¶œì‹œ ì†Œì‹ ë“±ì„ ìƒì„¸íˆ ì„¤ëª… (ìµœì†Œ {PromptConfig.MIN_NEWS_SECTION_LENGTH}ì)
   - ğŸ“º **ê´€ë ¨ ì˜ìƒ ì •ë³´**: ì˜ìƒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ê°€ ì˜ê²¬, ì„ìƒ ê²°ê³¼, í™˜ì ê²½í—˜ ë“±ì„ í’ë¶€í•˜ê²Œ ì„¤ëª… (ìµœì†Œ {PromptConfig.MIN_VIDEO_SECTION_LENGTH}ì)
   - {PromptConfig.SECTION_STRUCTURE['summary']}: ì „ì²´ ì •ë³´ë¥¼ ìš”ì•½í•˜ì—¬ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ì •ë¦¬ ({PromptConfig.MIN_SUMMARY_LENGTH}-{PromptConfig.MAX_SUMMARY_LENGTH}ì)
4. ì¶œì²˜ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•˜ë˜, í”Œë«í¼ëª…(YouTube, ë„¤ì´ë²„ ë‰´ìŠ¤ ë“±)ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”
   - ì˜ˆ: "ìµœê·¼ ë‰´ìŠ¤ì— ë”°ë¥´ë©´...", "ì „ë¬¸ê°€ ì˜ê²¬ì— ë”°ë¥´ë©´...", "ì „ë¬¸ê°€ë“¤ì€...", "ì—°êµ¬ ê²°ê³¼ì— ë”°ë¥´ë©´..."
   - âŒ "YouTube ì˜ìƒì—ì„œ...", "ë„¤ì´ë²„ ë‰´ìŠ¤ì— ë”°ë¥´ë©´..." ê°™ì€ í‘œí˜„ ê¸ˆì§€
5. ì‹ ì•½ì˜ ê°œë°œ í˜„í™©, ìŠ¹ì¸ ìƒíƒœ, ì„ìƒ ì‹œí—˜ ê²°ê³¼, ì¶œì‹œ ì¼ì • ë“±ì„ í¬í•¨
6. ìµœì†Œ {PromptConfig.MIN_DETAILED_ANSWER_LENGTH}ì ì´ìƒì˜ ìƒì„¸í•˜ê³  í’ë¶€í•œ ë‹µë³€ ì‘ì„±

**ì¤‘ìš”:**
- íš¨ëŠ¥ ë° ì‘ìš©ì›ë¦¬, ì£¼ì˜ì‚¬í•­ ì„¹ì…˜ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš” (ì‹ ì•½ ì •ë³´ ì¤‘ì‹¬)
- ëª¨ë“  ì •ë³´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©í•˜ì—¬ ì„¤ëª…
- ì‚¬ìš©ìê°€ ê¶ê¸ˆí•´í•˜ëŠ” ì‹ ì•½ ì •ë³´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€
- ìµœì‹  ì •ë³´ì„ì„ ê°•ì¡°í•˜ë˜, ê³¼ì¥í•˜ì§€ ì•Šê¸°
- êµ¬ì²´ì ì¸ ë‰´ìŠ¤ ì œëª©ê³¼ ì˜ìƒ ì œëª©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰
- í”Œë«í¼ëª…(YouTube, ë„¤ì´ë²„ ë‰´ìŠ¤ ë“±)ì€ ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”
"""
            
            final_answer = generate_response_llm_from_prompt(
                prompt=new_medicine_prompt,
                temperature=0.7,
                max_tokens=2500
            )
            
            # ê´€ë ¨ ë§í¬ ì„¹ì…˜ ì¶”ê°€
            links_section = "\n\n---\n\nğŸ”— **ê´€ë ¨ ë§í¬**\n\n"
            
            # ë‰´ìŠ¤ ë§í¬
            if news_info_list:
                links_section += "ğŸ“° **ê´€ë ¨ ë‰´ìŠ¤:**\n"
                for news in news_info_list[:5]:  # ìƒìœ„ 5ê°œë§Œ
                    if news["url"]:
                        links_section += f"- [{news['title']}]({news['url']})"
                        if news.get("pub_date"):
                            links_section += f" ({news['pub_date']})"
                        links_section += "\n"
                links_section += "\n"
            
            # YouTube ì˜ìƒ ë§í¬
            if youtube_info_list:
                links_section += "ğŸ“º **ê´€ë ¨ ì˜ìƒ:**\n"
                for video in youtube_info_list[:5]:  # ìƒìœ„ 5ê°œë§Œ
                    if video["url"]:
                        links_section += f"- [{video['title']}]({video['url']})"
                        if video.get("channel"):
                            links_section += f" - {video['channel']}"
                        links_section += "\n"
            
            final_answer += links_section
            
            state["final_answer"] = final_answer
            print("âœ… ì‹ ì•½ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ë‹µë³€ ìƒì„± ì™„ë£Œ")
            return state
            
        except Exception as e:
            print(f"âŒ ì‹ ì•½ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì²˜ë¦¬ë¡œ ë„˜ì–´ê°

    # ğŸ” LLM ê¸°ë°˜ ë§¥ë½ ë¶„ì„ ë° ë‹µë³€ ìƒì„±
    conversation_context = state.get("conversation_context", "")
    user_context = state.get("user_context", "")
    if not current_query:  # ìœ„ì—ì„œ ì •ì˜ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì—¬ê¸°ì„œ ì •ì˜
        current_query = state.get("query", "") or state.get("original_query", "")
    relevant_docs = state.get("relevant_docs", [])
    
    if conversation_context and current_query:
        print("ğŸ”„ LLMì´ ë§¥ë½ì„ ë¶„ì„í•˜ì—¬ ë‹µë³€ ìƒì„±")
        
        # LLMì—ê²Œ ë§¥ë½ ê¸°ë°˜ ë‹µë³€ ìƒì„± ìš”ì²­
        context_aware_prompt = f"""
{get_role_definition("pharmacist")} 
ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ë©° ì˜ì•½í’ˆì— ëŒ€í•œ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

**í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸:**
{current_query}

**ì´ì „ ëŒ€í™” ë§¥ë½:**
{conversation_context[:1000] if conversation_context else "ì—†ìŒ"}

**ê´€ë ¨ ë¬¸ì„œ ì •ë³´:**
{len(relevant_docs)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë¬¸ì„œë“¤ì—ëŠ” Excel DB, PDF, PubChem, YouTube, ë„¤ì´ë²„ ë‰´ìŠ¤ ë“± ë‹¤ì–‘í•œ ì†ŒìŠ¤ì˜ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

**âš ï¸ ë§¤ìš° ì¤‘ìš”: ì •ë³´ ìˆ˜ì§‘ ì›ì¹™**
- ì´ì „ ëŒ€í™” ë§¥ë½ê³¼ ê´€ë ¨ ë¬¸ì„œë¥¼ ëª¨ë‘ í™•ì¸í•˜ì„¸ìš”
- ê° ë¬¸ì„œì—ì„œ ë°œê²¬í•œ ëª¨ë“  ê³ ìœ í•œ ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ í¬í•¨í•˜ì„¸ìš”
- ë¹„ìŠ·í•œ ì •ë³´ë¼ë„ ê° ì†ŒìŠ¤ì˜ í‘œí˜„ì´ë‚˜ ì¶”ê°€ ì„¸ë¶€ì‚¬í•­ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì„¸ìš”

**ë‹µë³€ ìŠ¤íƒ€ì¼:**
- ë§ˆì¹˜ ì¹œêµ¬ë‚˜ ê°€ì¡±ê³¼ ëŒ€í™”í•˜ë“¯ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ
- "ì•„, ê·¸ê±° ê¶ê¸ˆí•˜ì‹œêµ°ìš”!", "ì¢‹ì€ ì§ˆë¬¸ì´ì—ìš”!" ê°™ì€ ìì—°ìŠ¤ëŸ¬ìš´ ë°˜ì‘
- ì „ë¬¸ì ì´ì§€ë§Œ ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…
- í•„ìš”ì‹œ "ë” ê¶ê¸ˆí•œ ê²Œ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”!" ê°™ì€ ë§ˆë¬´ë¦¬

**ë‹µë³€ ìš”êµ¬ì‚¬í•­:**
1. ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ì •í™•íˆ íŒŒì•…í•˜ê³  ì—°ê²°
2. ì‚¬ìš©ìì˜ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€
3. ëŒ€ëª…ì‚¬ë‚˜ ëª¨í˜¸í•œ í‘œí˜„ì´ ìˆë‹¤ë©´ ë§¥ë½ì—ì„œ ì¶”ë¡ í•˜ì—¬ ëª…í™•íˆ í•´ì„
4. í‹°í‚¤íƒ€ì¹´ê°€ ê°€ëŠ¥í•œ ëŒ€í™”í˜• ë‹µë³€
5. {PromptConfig.MIN_CONVERSATIONAL_LENGTH}-{PromptConfig.MAX_CONVERSATIONAL_LENGTH}ì ì •ë„ì˜ ì ì ˆí•œ ê¸¸ì´
6. **ê´€ë ¨ ë¬¸ì„œì˜ ëª¨ë“  ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ í™•ì¸í•˜ê³  í™œìš©í•˜ì„¸ìš”**

**ì¤‘ìš”:**
- ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ì•½í’ˆì´ë‚˜ ì„±ë¶„ì´ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€
- ì‚¬ìš©ìê°€ íŠ¹ì • ì„±ë¶„ì— ëŒ€í•´ ë¬¼ì–´ë´¤ë‹¤ë©´ ê·¸ ì„±ë¶„ì—ë§Œ ì§‘ì¤‘í•´ì„œ ë‹µë³€
- ë¶ˆí•„ìš”í•˜ê²Œ ëª¨ë“  ì •ë³´ë¥¼ ë‹¤ ë‚˜ì—´í•˜ì§€ ë§ê³  ì§ˆë¬¸ì— ë§ëŠ” ì •ë³´ë§Œ ì œê³µ
- **í•˜ì§€ë§Œ ì§ˆë¬¸ì— ê´€ë ¨ëœ ëª¨ë“  ì†ŒìŠ¤ì˜ ì •ë³´ëŠ” ë¹ ì§ì—†ì´ í¬í•¨í•˜ì„¸ìš”**
"""
        
        try:
            # LLMì´ ë§¥ë½ì„ ì´í•´í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìƒì„±
            final_answer = generate_response_llm_from_prompt(
                prompt=context_aware_prompt,
                temperature=0.7,  # ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ìœ„í•´ ì ë‹¹í•œ temperature
                max_tokens=1000
            )
            
            state["final_answer"] = final_answer
            print("âœ… LLM ê¸°ë°˜ ë§¥ë½ ì¸ì‹ ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ LLM ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ë‹µë³€
            state["final_answer"] = f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    else:
        print("âŒ ëŒ€í™” ë§¥ë½ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŒ")
        state["final_answer"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ëŒ€í™” ë§¥ë½ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    return state
