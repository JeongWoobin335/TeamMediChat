"""
í‰ê°€ ì‹œìŠ¤í…œ: ë³¸ ì‹œìŠ¤í…œ(Pill'sGood)ê³¼ GPT-5ì˜ ì‘ë‹µì„ Gemini 3.0 Proë¡œ ë¹„êµ í‰ê°€
"""
import os
import json
import time
from typing import List, Dict, Optional
from datetime import datetime
from dotenv import load_dotenv

# LangChain imports
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate

# ë³¸ ì‹œìŠ¤í…œ import
from main_graph import graph
from qa_state import QAState

load_dotenv()

class EvaluationSystem:
    """í‰ê°€ ì‹œìŠ¤í…œ: ë³¸ ì‹œìŠ¤í…œê³¼ GPT-5 ì‘ë‹µì„ Gemini 3.0 Proë¡œ í‰ê°€"""
    
    def __init__(self):
        """í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        # GPT-5 ëª¨ë¸ (ì¶”ê°€ ê¸°ëŠ¥ ì—†ì´ ìˆœìˆ˜ ëª¨ë¸ ì‘ë‹µ)
        # ì°¸ê³ : GPT-5ê°€ ì•„ì§ ì¶œì‹œë˜ì§€ ì•Šì•˜ë‹¤ë©´ "gpt-4o" ë˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸ë¡œ ë³€ê²½ í•„ìš”
        # GPT-5ëŠ” temperatureì™€ max_tokensë¥¼ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œê±°
        # hallucination_node.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì´ˆê¸°í™”
        self.gpt5_llm = ChatOpenAI(model="gpt-5")
        
        # Gemini 3.0 Pro í‰ê°€ì ëª¨ë¸
        self.evaluator_llm = ChatGoogleGenerativeAI(
            model="gemini-3-pro-preview",  # Gemini 3.0 Pro Preview
            temperature=1.0,  # í‰ê°€ëŠ” ì¼ê´€ì„± ìˆê²Œ
            max_output_tokens=4000  # JSON ì‘ë‹µì´ ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¦ê°€
        )
        
        # í‰ê°€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.evaluation_prompt_template = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì˜ì•½í’ˆ ì •ë³´ ì‹œìŠ¤í…œì˜ ì „ë¬¸ í‰ê°€ìì…ë‹ˆë‹¤. 
ë‘ ì‹œìŠ¤í…œì˜ ì‘ë‹µì„ ê°ê´€ì ì´ê³  ê³µì •í•˜ê²Œ í‰ê°€í•´ì£¼ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
1. ì •í™•ì„± (Accuracy): ì˜í•™ì  ì •ë³´ì˜ ì •í™•ì„± (0-10ì )
2. ê´€ë ¨ì„± (Relevance): ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ê´€ë ¨ì„± (0-10ì )
3. ì™„ì „ì„± (Completeness): í•„ìš”í•œ ì •ë³´ì˜ ì™„ì „ì„± (0-10ì )
4. ìœ ìš©ì„± (Usefulness): ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ì •ë„ (0-10ì )
5. ì „ì²´ ì ìˆ˜ (Overall): ì¢…í•© í‰ê°€ (0-10ì )

ê° ì§€í‘œì— ëŒ€í•´ ì ìˆ˜ì™€ ê°„ë‹¨í•œ ì´ìœ ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."""),
            ("human", """ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ë‘ ì‹œìŠ¤í…œì˜ ì‘ë‹µì„ í‰ê°€í•´ì£¼ì„¸ìš”.

**ì§ˆë¬¸:**
{question}

**ë‹µì§€ (ì •ë‹µ):**
{ground_truth}

**ì‹œìŠ¤í…œ A ì‘ë‹µ (ë³¸ ì‹œìŠ¤í…œ):**
{system_a_response}

**ì‹œìŠ¤í…œ B ì‘ë‹µ (GPT-5):**
{system_b_response}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ í‰ê°€ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
{{
    "system_a": {{
        "accuracy": ì ìˆ˜,
        "relevance": ì ìˆ˜,
        "completeness": ì ìˆ˜,
        "usefulness": ì ìˆ˜,
        "overall": ì ìˆ˜,
        "accuracy_reason": "ì´ìœ ",
        "relevance_reason": "ì´ìœ ",
        "completeness_reason": "ì´ìœ ",
        "usefulness_reason": "ì´ìœ ",
        "overall_reason": "ì´ìœ "
    }},
    "system_b": {{
        "accuracy": ì ìˆ˜,
        "relevance": ì ìˆ˜,
        "completeness": ì ìˆ˜,
        "usefulness": ì ìˆ˜,
        "overall": ì ìˆ˜,
        "accuracy_reason": "ì´ìœ ",
        "relevance_reason": "ì´ìœ ",
        "completeness_reason": "ì´ìœ ",
        "usefulness_reason": "ì´ìœ ",
        "overall_reason": "ì´ìœ "
    }},
    "comparison": {{
        "winner": "system_a" ë˜ëŠ” "system_b" ë˜ëŠ” "tie",
        "reason": "ìŠ¹ì ì„ ì • ì´ìœ "
    }}
}}""")
        ])
    
    def get_our_system_response(self, question: str) -> str:
        """
        ë³¸ ì‹œìŠ¤í…œ(Pill'sGood)ì˜ ì‘ë‹µ ìƒì„±
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            ì‹œìŠ¤í…œ ì‘ë‹µ
        """
        try:
            # QAState ì´ˆê¸°í™”
            initial_state = QAState(
                query=question,
                session_id=f"eval_{int(time.time())}",
                conversation_context="",
                user_context=""
            )
            
            # ê·¸ë˜í”„ ì‹¤í–‰
            result = graph.invoke(initial_state)
            
            # ìµœì¢… ë‹µë³€ ì¶”ì¶œ
            answer = result.get("final_answer", "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            return answer
            
        except Exception as e:
            print(f"âŒ ë³¸ ì‹œìŠ¤í…œ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    def get_gpt5_response(self, question: str) -> str:
        """
        GPT-5ì˜ ì‘ë‹µ ìƒì„± (ì¶”ê°€ ê¸°ëŠ¥ ì—†ì´ ìˆœìˆ˜ ëª¨ë¸ ì‘ë‹µ)
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            GPT-5 ì‘ë‹µ
        """
        try:
            prompt = f"""ë‹¹ì‹ ì€ ì˜ì•½í’ˆ ì •ë³´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹µë³€:"""
            
            print(f"  ğŸ” GPT-5 ëª¨ë¸ í˜¸ì¶œ ì¤‘... (ëª¨ë¸: {self.gpt5_llm.model_name if hasattr(self.gpt5_llm, 'model_name') else getattr(self.gpt5_llm, 'model', 'unknown')})")
            response = self.gpt5_llm.invoke(prompt)
            
            # ì‘ë‹µ ê°ì²´ í™•ì¸
            if not response:
                print(f"  âš ï¸ GPT-5 ì‘ë‹µ ê°ì²´ê°€ Noneì…ë‹ˆë‹¤")
                return "ì‘ë‹µ ê°ì²´ê°€ Noneì…ë‹ˆë‹¤"
            
            # content ì†ì„± í™•ì¸
            if not hasattr(response, 'content'):
                print(f"  âš ï¸ GPT-5 ì‘ë‹µì— content ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤. ì‘ë‹µ íƒ€ì…: {type(response)}")
                print(f"  ğŸ“ ì‘ë‹µ ë‚´ìš©: {str(response)[:200]}")
                return str(response)
            
            content = response.content.strip() if response.content else ""
            
            if not content:
                print(f"  âš ï¸ GPT-5 ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                print(f"  ğŸ“ ì‘ë‹µ ê°ì²´: {type(response)}")
                print(f"  ğŸ“ content ì†ì„± ê°’: {repr(response.content)[:200]}")
            
            return content
            
        except Exception as e:
            print(f"  âŒ GPT-5 ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    def evaluate_responses(self, question: str, ground_truth: str, 
                          system_a_response: str, system_b_response: str) -> Dict:
        """
        Gemini 3.0 Proë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ ì‘ë‹µ í‰ê°€
        
        Args:
            question: ì›ë³¸ ì§ˆë¬¸
            ground_truth: ë‹µì§€ (ì •ë‹µ)
            system_a_response: ë³¸ ì‹œìŠ¤í…œ ì‘ë‹µ
            system_b_response: GPT-5 ì‘ë‹µ
            
        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.evaluation_prompt_template.format_messages(
                question=question,
                ground_truth=ground_truth,
                system_a_response=system_a_response,
                system_b_response=system_b_response
            )
            
            # Gemini 3.0 Proë¡œ í‰ê°€
            response = self.evaluator_llm.invoke(prompt)
            evaluation_text = response.content.strip() if response.content else ""
            
            # ë””ë²„ê¹…: ì‘ë‹µì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if not evaluation_text:
                print(f"âš ï¸ Gemini ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                return {
                    "raw_evaluation": "",
                    "error": "Gemini ì‘ë‹µ ì—†ìŒ"
                }
            
            # ë””ë²„ê¹…: ì‘ë‹µì˜ ì²˜ìŒ ë¶€ë¶„ ì¶œë ¥
            print(f"  ğŸ“ Gemini ì‘ë‹µ (ì²˜ìŒ 300ì): {evaluation_text[:300]}")
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # JSON ë¸”ë¡ ì¶”ì¶œ
                json_text = evaluation_text
                
                # ```json ë˜ëŠ” ``` ë¸”ë¡ ì¶”ì¶œ
                if "```json" in evaluation_text:
                    json_start = evaluation_text.find("```json") + 7
                    remaining_text = evaluation_text[json_start:]
                    json_end = remaining_text.find("```")
                    if json_end != -1:
                        json_text = remaining_text[:json_end].strip()
                    else:
                        json_text = remaining_text.strip()
                elif "```" in evaluation_text:
                    json_start = evaluation_text.find("```") + 3
                    remaining_text = evaluation_text[json_start:]
                    json_end = remaining_text.find("```")
                    if json_end != -1:
                        json_text = remaining_text[:json_end].strip()
                    else:
                        json_text = remaining_text.strip()
                
                # JSON ê°ì²´ ì‹œì‘ ì°¾ê¸°
                json_start_idx = json_text.find("{")
                if json_start_idx != -1:
                    json_text = json_text[json_start_idx:]
                    
                    # JSON ê°ì²´ë§Œ ì¶”ì¶œ (ì¤‘ê´„í˜¸ ë§¤ì¹­)
                    brace_count = 0
                    end_pos = -1
                    for i, char in enumerate(json_text):
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                end_pos = i + 1
                                break
                    
                    if end_pos > 0:
                        json_text = json_text[:end_pos]
                    else:
                        # ë‹«ëŠ” ì¤‘ê´„í˜¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°, ë§ˆì§€ë§‰ } ì°¾ê¸°
                        last_brace = json_text.rfind("}")
                        if last_brace != -1:
                            json_text = json_text[:last_brace + 1]
                
                # ìµœì¢… JSON í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
                if not json_text or not json_text.strip():
                    raise json.JSONDecodeError("JSON í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŒ", json_text, 0)
                
                # JSON íŒŒì‹± ì‹œë„
                evaluation_result = json.loads(json_text)
                print(f"  âœ… JSON íŒŒì‹± ì„±ê³µ")
                return evaluation_result
                
            except json.JSONDecodeError as e:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë³µêµ¬ ì‹œë„
                print(f"  âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                print(f"  ğŸ”§ JSON ë³µêµ¬ ì‹œë„ ì¤‘...")
                
                # ë³µêµ¬ ì‹œë„: ë¶ˆì™„ì „í•œ JSON ìˆ˜ì •
                try:
                    if 'json_text' in locals() and json_text:
                        # ë¶ˆì™„ì „í•œ ë¬¸ìì—´ í•„ë“œ ë‹«ê¸°
                        fixed_json = json_text
                        
                        # ë§ˆì§€ë§‰ ë¶ˆì™„ì „í•œ ë¬¸ìì—´ í•„ë“œ ì°¾ì•„ì„œ ë‹«ê¸°
                        # "key": "value í˜•íƒœë¡œ ëë‚˜ëŠ” ê²½ìš°
                        import re
                        # ë¶ˆì™„ì „í•œ ë¬¸ìì—´ í•„ë“œ íŒ¨í„´ ì°¾ê¸°
                        incomplete_string_pattern = r'"([^"]*)"\s*:\s*"([^"]*)$'
                        matches = list(re.finditer(incomplete_string_pattern, fixed_json, re.MULTILINE))
                        
                        if matches:
                            # ë§ˆì§€ë§‰ ë§¤ì¹˜ì˜ ë¶ˆì™„ì „í•œ ë¬¸ìì—´ ë‹«ê¸°
                            last_match = matches[-1]
                            fixed_json = fixed_json[:last_match.end()] + '"'
                            
                            # ë‹«ëŠ” ì¤‘ê´„í˜¸ ì¶”ê°€
                            open_braces = fixed_json.count('{')
                            close_braces = fixed_json.count('}')
                            missing_braces = open_braces - close_braces
                            if missing_braces > 0:
                                fixed_json += '\n' + '    ' * (missing_braces - 1) + '}' * missing_braces
                            
                            # ë‹¤ì‹œ íŒŒì‹± ì‹œë„
                            evaluation_result = json.loads(fixed_json)
                            print(f"  âœ… JSON ë³µêµ¬ ì„±ê³µ")
                            return evaluation_result
                except:
                    pass
                
                # ë³µêµ¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
                print(f"  ğŸ“ ì¶”ì¶œëœ JSON í…ìŠ¤íŠ¸ (ì²˜ìŒ 1000ì): {json_text[:1000] if 'json_text' in locals() else 'N/A'}")
                print(f"  ğŸ“ ì›ë³¸ ì‘ë‹µ (ì²˜ìŒ 500ì): {evaluation_text[:500]}")
                return {
                    "raw_evaluation": evaluation_text,
                    "error": f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}",
                    "extracted_json": json_text[:2000] if 'json_text' in locals() else ""
                }
                
        except Exception as e:
            print(f"âŒ í‰ê°€ ì˜¤ë¥˜: {e}")
            return {
                "error": str(e)
            }
    
    def run_evaluation(self, questions: List[Dict[str, str]]) -> List[Dict]:
        """
        ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ í‰ê°€ ì‹¤í–‰
        
        Args:
            questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸, ê° í•­ëª©ì€ {"question": "...", "ground_truth": "..."} í˜•ì‹
            
        Returns:
            í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        print(f"ğŸ“Š í‰ê°€ ì‹œì‘: ì´ {len(questions)}ê°œ ì§ˆë¬¸")
        print("=" * 60)
        
        for idx, q_data in enumerate(questions, 1):
            question = q_data.get("question", "")
            ground_truth = q_data.get("ground_truth", "")
            
            print(f"\n[{idx}/{len(questions)}] ì§ˆë¬¸: {question[:50]}...")
            
            # 1. ë³¸ ì‹œìŠ¤í…œ ì‘ë‹µ ìƒì„±
            print("  ğŸ”„ ë³¸ ì‹œìŠ¤í…œ ì‘ë‹µ ìƒì„± ì¤‘...")
            system_a_response = self.get_our_system_response(question)
            print(f"  âœ… ë³¸ ì‹œìŠ¤í…œ ì‘ë‹µ ì™„ë£Œ ({len(system_a_response)}ì)")
            
            # 2. GPT-5 ì‘ë‹µ ìƒì„±
            print("  ğŸ”„ GPT-5 ì‘ë‹µ ìƒì„± ì¤‘...")
            system_b_response = self.get_gpt5_response(question)
            print(f"  âœ… GPT-5 ì‘ë‹µ ì™„ë£Œ ({len(system_b_response)}ì)")
            
            # 3. Gemini 3.0 Proë¡œ í‰ê°€
            print("  ğŸ”„ Gemini 3.0 Pro í‰ê°€ ì¤‘...")
            evaluation = self.evaluate_responses(
                question=question,
                ground_truth=ground_truth,
                system_a_response=system_a_response,
                system_b_response=system_b_response
            )
            print("  âœ… í‰ê°€ ì™„ë£Œ")
            
            # ê²°ê³¼ ì €ì¥
            result = {
                "id": q_data.get("id"),  # ì§ˆë¬¸ ID í¬í•¨
                "question": question,
                "ground_truth": ground_truth,
                "system_a_response": system_a_response,
                "system_b_response": system_b_response,
                "evaluation": evaluation,
                "timestamp": datetime.now().isoformat()
            }
            results.append(result)
            
            # ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (ë°±ì—…)
            self.save_results(results, f"evaluation_results_backup_{idx}.json")
            
            # ì ì‹œ ëŒ€ê¸° (API rate limit ë°©ì§€)
            time.sleep(1)
        
        print("\n" + "=" * 60)
        print("âœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ!")
        
        return results
    
    def save_results(self, results: List[Dict], filename: Optional[str] = None):
        """
        í‰ê°€ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            results: í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            filename: ì €ì¥í•  íŒŒì¼ëª… (Noneì´ë©´ ìë™ ìƒì„±)
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"evaluation_results_{timestamp}.json"
        
        filepath = os.path.join("evaluation_charts", filename)
        os.makedirs("evaluation_charts", exist_ok=True)
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {filepath}")
    
    def generate_summary(self, results: List[Dict]) -> Dict:
        """
        í‰ê°€ ê²°ê³¼ ìš”ì•½ í†µê³„ ìƒì„±
        
        Args:
            results: í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ìš”ì•½ í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        if not results:
            return {}
        
        system_a_scores = {
            "accuracy": [],
            "relevance": [],
            "completeness": [],
            "usefulness": [],
            "overall": []
        }
        
        system_b_scores = {
            "accuracy": [],
            "relevance": [],
            "completeness": [],
            "usefulness": [],
            "overall": []
        }
        
        winners = {"system_a": 0, "system_b": 0, "tie": 0}
        
        for result in results:
            evaluation = result.get("evaluation", {})
            
            if "error" in evaluation:
                continue
            
            # System A ì ìˆ˜ ìˆ˜ì§‘
            if "system_a" in evaluation:
                for metric in system_a_scores.keys():
                    score = evaluation["system_a"].get(metric, 0)
                    if isinstance(score, (int, float)):
                        system_a_scores[metric].append(score)
            
            # System B ì ìˆ˜ ìˆ˜ì§‘
            if "system_b" in evaluation:
                for metric in system_b_scores.keys():
                    score = evaluation["system_b"].get(metric, 0)
                    if isinstance(score, (int, float)):
                        system_b_scores[metric].append(score)
            
            # ìŠ¹ì ì§‘ê³„
            if "comparison" in evaluation:
                winner = evaluation["comparison"].get("winner", "tie")
                winners[winner] = winners.get(winner, 0) + 1
        
        # í‰ê·  ê³„ì‚°
        def calculate_avg(scores):
            return sum(scores) / len(scores) if scores else 0
        
        summary = {
            "total_questions": len(results),
            "system_a_averages": {
                metric: calculate_avg(scores)
                for metric, scores in system_a_scores.items()
            },
            "system_b_averages": {
                metric: calculate_avg(scores)
                for metric, scores in system_b_scores.items()
            },
            "winners": winners
        }
        
        return summary


def load_ground_truth(filepath: str = "evaluation_charts/ground_truth.json") -> List[Dict]:
    """ë‹µì•ˆì§€ íŒŒì¼ ë¡œë“œ"""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
        print(f"âœ… ë‹µì•ˆì§€ ë¡œë“œ ì™„ë£Œ: {len(data)}ê°œ ì§ˆë¬¸")
        return data
    except FileNotFoundError:
        print(f"âŒ ë‹µì•ˆì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
        return []
    except Exception as e:
        print(f"âŒ ë‹µì•ˆì§€ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return []


def main():
    """í‰ê°€ ì‹œìŠ¤í…œ ì‹¤í–‰"""
    # í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    evaluator = EvaluationSystem()
    
    # ë‹µì•ˆì§€ íŒŒì¼ ë¡œë“œ
    ground_truth_data = load_ground_truth()
    
    if not ground_truth_data:
        print("âŒ ë‹µì•ˆì§€ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ì–´ í‰ê°€ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„ (ë‹µì•ˆì§€ í˜•ì‹ì— ë§ì¶° ë³€í™˜)
    # 7, 9, 10ë²ˆë§Œ í‰ê°€
    target_ids = [9, 10]
    questions = []
    for item in ground_truth_data:
        item_id = item.get("id")
        if item_id in target_ids:
            questions.append({
                "question": item.get("question", ""),
                "ground_truth": item.get("ground_truth", ""),
                "id": item_id
            })
    
    print(f"\nğŸ“Š ì¶”ê°€ í‰ê°€ ì‹œì‘: {target_ids}ë²ˆ ì§ˆë¬¸ ({len(questions)}ê°œ)")
    print("=" * 60)
    
    # í‰ê°€ ì‹¤í–‰
    results = evaluator.run_evaluation(questions)
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    evaluator.save_results(results, f"evaluation_results_{timestamp}.json")
    
    # ìš”ì•½ í†µê³„ ìƒì„±
    summary = evaluator.generate_summary(results)
    print("\nğŸ“Š í‰ê°€ ìš”ì•½:")
    print(json.dumps(summary, ensure_ascii=False, indent=2))
    
    # ìš”ì•½ë„ ì €ì¥
    evaluator.save_results([summary], f"evaluation_summary_{timestamp}.json")


if __name__ == "__main__":
    main()

