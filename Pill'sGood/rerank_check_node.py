from qa_state import QAState
from retrievers import compressor  # CrossEncoder ê¸°ë°˜ ë¬¸ì„œ ì••ì¶•ê¸°
from langchain_core.documents import Document
from typing import List
import re

def normalize(text: str) -> str:
    text = re.sub(r"\(.*?\)", "", text)
    text = re.sub(r"[^\wê°€-í£]", "", text)
    return re.sub(r"\s+", "", text.strip().lower())

def contains_product_name(doc: Document, product_name: str) -> bool:
    return normalize(product_name) == normalize(doc.metadata.get("ì œí’ˆëª…", ""))

def rerank_node(state: QAState) -> QAState:
    all_docs: List[Document] = []
    if state.get("pdf_results"):
        all_docs.extend(state["pdf_results"])
    if state.get("excel_results"):
        all_docs.extend(state["excel_results"])
    if state.get("sns_results"):
        all_docs.extend(state["sns_results"])

    if not all_docs:
        # ìµœì‹  ì •ë³´ ìš”ì²­ì¸ ê²½ìš° íŠ¹ë³„í•œ ì‘ë‹µ ìƒì„±
        query = state.get("query", "")
        category = state.get("category", "")
        
        if "ìµœì‹ " in query or "ìƒˆë¡œ" in query or "2024" in query or "2023" in query:
            latest_info_response = f"""ì•ˆë…•í•˜ì„¸ìš”! '{query}'ì— ëŒ€í•œ ìµœì‹  ì •ë³´ë¥¼ ìš”ì²­í•˜ì…¨êµ°ìš”.

í˜„ì¬ ì‹œìŠ¤í…œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ì„œ, ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ìµœì‹  ì •ë³´ë¥¼ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

ğŸ” **ì¶”ì²œ ê²€ìƒ‰ ë°©ë²•:**
1. **ì˜ë£Œ ì „ë¬¸ ì‚¬ì´íŠ¸**: ì‹ì•½ì²˜, FDA ê³µì‹ ì›¹ì‚¬ì´íŠ¸
2. **ì˜ë£Œ ë°ì´í„°ë² ì´ìŠ¤**: PubMed, ClinicalTrials.gov
3. **ì œì•½ì‚¬ ê³µì‹ ë°œí‘œ**: ê´€ë ¨ ì œì•½íšŒì‚¬ ê³µì‹ ë³´ë„ìë£Œ
4. **ì˜ë£Œ ì „ë¬¸ ë§¤ì²´**: ì˜í•™ ì €ë„, ì˜ë£Œ ë‰´ìŠ¤

ğŸ’¡ **ì°¸ê³ ì‚¬í•­:**
- ìµœì‹  ì•½í’ˆ ì •ë³´ëŠ” ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤
- ì •í™•í•œ ì •ë³´ëŠ” ì˜ì‚¬ë‚˜ ì•½ì‚¬ì™€ ìƒë‹´í•˜ì‹œëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤
- ì„ìƒì‹œí—˜ ì¤‘ì¸ ì•½í’ˆì˜ ê²½ìš° ìŠ¹ì¸ ìƒíƒœê°€ ë³€ê²½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤

ë” êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!"""
            
            state["final_answer"] = latest_info_response
            state["relevant_docs"] = []
            state["reranked_docs"] = []
            return state
        
        state["reranked_docs"] = []
        state["relevant_docs"] = []
        return state

    try:
        query = state.get("query", "")
        category = state.get("category", "")

        # âœ… LLM ê¸°ë°˜ ë¬¸ì„œ í•„í„°ë§
        
        from langchain_openai import ChatOpenAI
        from langchain_core.prompts import ChatPromptTemplate
        
        llm = ChatOpenAI(model="gpt-4", temperature=0)
        
        # ë¬¸ì„œ ê´€ë ¨ì„± íŒë‹¨ í”„ë¡¬í”„íŠ¸
        relevance_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ íŒë‹¨í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹¤ìŒ ë¬¸ì„œë“¤ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.
ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œë§Œ ì„ íƒí•˜ì—¬ JSON í˜•íƒœë¡œ ì‘ë‹µí•˜ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
{{
  "relevant_docs": [
    {{
      "index": 0,
      "reason": "ì´ìœ "
    }}
  ]
}}

ê´€ë ¨ì„±ì´ ì—†ë‹¤ë©´ ë¹ˆ ë°°ì—´ì„ ë°˜í™˜í•˜ì„¸ìš”."""),
            ("human", "ë¬¸ì„œë“¤:\n{docs}")
        ])
        
        # ë¬¸ì„œë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        docs_text = ""
        for i, doc in enumerate(all_docs[:10]):  # ìµœëŒ€ 10ê°œë§Œ
            docs_text += f"[{i}] {doc.page_content[:300]}...\n\n"
        
        try:
            # LLMì´ ê´€ë ¨ ë¬¸ì„œ íŒë‹¨
            relevance_result = llm.invoke(relevance_prompt.format(
                query=query,
                docs=docs_text
            ))
            
            # JSON íŒŒì‹± (ë” ì•ˆì „í•œ ë°©ì‹)
            import json
            import re
            
            try:
                # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                content = relevance_result.content
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                
                if json_match:
                    result_data = json.loads(json_match.group())
                    relevant_indices = [item["index"] for item in result_data.get("relevant_docs", [])]
                    
                    if relevant_indices:
                        filtered_docs = [all_docs[i] for i in relevant_indices if i < len(all_docs)]
                        state["relevant_docs"] = filtered_docs[:3]
                        state["reranked_docs"] = []
                        return state
                    else:
                        # SNS ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
                        sns_docs = state.get("sns_results", [])
                        if sns_docs:
                            state["relevant_docs"] = sns_docs[:3]
                            state["reranked_docs"] = []
                            return state
                        else:
                            # ìµœì‹  ì •ë³´ ìš”ì²­ì— ëŒ€í•œ ì‘ë‹µ ì§ì ‘ ìƒì„±
                            query = state.get("query", "")
                            latest_info_response = f"""ì•ˆë…•í•˜ì„¸ìš”! '{query}'ì— ëŒ€í•œ ìµœì‹  ì •ë³´ë¥¼ ìš”ì²­í•˜ì…¨êµ°ìš”.

í˜„ì¬ ì‹œìŠ¤í…œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ì„œ, ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ìµœì‹  ì •ë³´ë¥¼ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

ğŸ” **ì¶”ì²œ ê²€ìƒ‰ ë°©ë²•:**
1. **ì˜ë£Œ ì „ë¬¸ ì‚¬ì´íŠ¸**: ì‹ì•½ì²˜, FDA ê³µì‹ ì›¹ì‚¬ì´íŠ¸
2. **ì˜ë£Œ ë°ì´í„°ë² ì´ìŠ¤**: PubMed, ClinicalTrials.gov
3. **ì œì•½ì‚¬ ê³µì‹ ë°œí‘œ**: ê´€ë ¨ ì œì•½íšŒì‚¬ ê³µì‹ ë³´ë„ìë£Œ
4. **ì˜ë£Œ ì „ë¬¸ ë§¤ì²´**: ì˜í•™ ì €ë„, ì˜ë£Œ ë‰´ìŠ¤

ğŸ’¡ **ì°¸ê³ ì‚¬í•­:**
- ìµœì‹  ì•½í’ˆ ì •ë³´ëŠ” ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤
- ì •í™•í•œ ì •ë³´ëŠ” ì˜ì‚¬ë‚˜ ì•½ì‚¬ì™€ ìƒë‹´í•˜ì‹œëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤
- ì„ìƒì‹œí—˜ ì¤‘ì¸ ì•½í’ˆì˜ ê²½ìš° ìŠ¹ì¸ ìƒíƒœê°€ ë³€ê²½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤

ë” êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!"""
                            
                            state["final_answer"] = latest_info_response
                            state["relevant_docs"] = []
                            state["reranked_docs"] = []
                            return state
                else:
                    pass
            except Exception as e:
                pass
        except Exception as e:
            pass

        # âœ… ê¸°ì¡´ ì œí’ˆëª… ë§¤ì¹­ ë¡œì§
        product_name = state.get("normalized_query") or state.get("cleaned_query")
        product_name = normalize(product_name or "")

        excel_docs = state.get("excel_results", [])
        excel_matched = [doc for doc in excel_docs if contains_product_name(doc, product_name)]

        if excel_matched:
            state["relevant_docs"] = excel_matched[:3]
            state["reranked_docs"] = []
            return state

        # Excelì— ì—†ìœ¼ë©´ ì „ì²´ ë¬¸ì„œ ë¦¬ë­í‚¹
        reranked = compressor.compress_documents(all_docs, query=query)
        state["reranked_docs"] = reranked

        filtered = [doc for doc in reranked if contains_product_name(doc, product_name)]

        if filtered:
            deduped = []
            seen = set()
            for doc in filtered + reranked:
                key = doc.page_content[:100]
                if key not in seen:
                    deduped.append(doc)
                    seen.add(key)
                if len(deduped) >= 3:
                    break
            state["relevant_docs"] = deduped
        else:
            state["relevant_docs"] = reranked[:3]

    except Exception as e:
        state["reranked_docs"] = []
        state["relevant_docs"] = []

    return state
