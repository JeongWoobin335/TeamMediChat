import os
import re
from typing import List, Dict
import requests
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qa_state import QAState
from medical_patterns import *

def setup_youtube_api():
    """ìœ íŠœë¸Œ API ì„¤ì •"""
    api_key = os.getenv("YOUTUBE_API_KEY")
    if not api_key:
        raise ValueError("YOUTUBE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return api_key

def extract_keywords(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    keywords = re.findall(r'\b\w+\b', text.lower())
    return keywords

def search_youtube_videos(query: str, max_videos: int = 10) -> List[Dict]:
    """ìœ íŠœë¸Œì—ì„œ ì•½í’ˆ ê´€ë ¨ ì˜ìƒ ê²€ìƒ‰"""
    try:
        api_key = setup_youtube_api()
        videos = []
        
        # ìœ íŠœë¸Œ ê²€ìƒ‰ API ì—”ë“œí¬ì¸íŠ¸
        search_url = "https://www.googleapis.com/youtube/v3/search"
        
        # ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
        params = {
            'part': 'snippet',
            'q': query,
            'key': api_key,
            'maxResults': max_videos,
            'type': 'video',
            'relevanceLanguage': 'ko',  # í•œêµ­ì–´ ìš°ì„ 
            'videoDuration': 'medium',  # ì¤‘ê°„ ê¸¸ì´ ì˜ìƒ (5-20ë¶„)
            'order': 'relevance'
        }
        
        # ê²€ìƒ‰ ìš”ì²­
        response = requests.get(search_url, params=params)
        response.raise_for_status()
        
        search_results = response.json()
        
        if 'items' not in search_results:
            print(f"âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤: {query}")
            return []
        
        for item in search_results['items']:
            snippet = item['snippet']
            video_id = item['id']['videoId']
            
            # ì˜ìƒ ì •ë³´ ì¶”ì¶œ
            video_info = {
                "title": snippet['title'],
                "description": snippet['description'],
                "channel_title": snippet['channelTitle'],
                "published_at": snippet['publishedAt'],
                "video_id": video_id,
                "thumbnail": snippet['thumbnails']['medium']['url'],
                "source": "youtube",
                "keywords": extract_keywords(snippet['title'] + " " + snippet['description'])
            }
            
            videos.append(video_info)
        
        print(f"âœ… '{query}' ê²€ìƒ‰ ê²°ê³¼: {len(videos)}ê°œ ì˜ìƒ")
        return videos
        
    except Exception as e:
        print(f"âŒ ìœ íŠœë¸Œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

def get_video_transcript(video_id: str) -> str:
    """ìœ íŠœë¸Œ ì˜ìƒì˜ ìë§‰/ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
    try:
        from youtube_transcript_api import YouTubeTranscriptApi
        
        # í•œêµ­ì–´ ìë§‰ ìš°ì„ , ì—†ìœ¼ë©´ ì˜ì–´ ìë§‰
        transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['ko', 'en'])
        
        if transcript_list:
            # ìë§‰ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
            full_transcript = ""
            for transcript in transcript_list:
                full_transcript += transcript['text'] + " "
            
            print(f"âœ… ì˜ìƒ {video_id} ìë§‰ ì¶”ì¶œ ì„±ê³µ: {len(full_transcript)}ì")
            return full_transcript.strip()
        else:
            print(f"âš ï¸ ì˜ìƒ {video_id} ìë§‰ì´ ì—†ìŠµë‹ˆë‹¤")
            return ""
            
    except Exception as e:
        print(f"âŒ ìë§‰ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return ""

def summarize_video_content(content: str, max_length: int = 500) -> str:
    """ì˜ìƒ ë‚´ìš©ì„ ìš”ì•½"""
    try:
        if len(content) <= max_length:
            return content
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì‚¬ìš©
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        
        chunks = text_splitter.split_text(content)
        
        # ì²« ë²ˆì§¸ ì²­í¬ì™€ ë§ˆì§€ë§‰ ì²­í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì•½
        if len(chunks) >= 2:
            summary = chunks[0][:max_length//2] + "...\n\n" + chunks[-1][:max_length//2]
        else:
            summary = chunks[0][:max_length]
        
        return summary
        
    except Exception as e:
        print(f"âŒ ë‚´ìš© ìš”ì•½ ì‹¤íŒ¨: {e}")
        return content[:max_length] if len(content) > max_length else content

def analyze_query_intent(query: str) -> Dict[str, any]:
    """ì¿¼ë¦¬ì˜ ì˜ë„ì™€ í•µì‹¬ ìš”ì†Œë¥¼ ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„"""
    query_lower = query.lower()
    
    # 1. ì˜ë„ë³„ ì ìˆ˜ ê³„ì‚°
    intent_scores = {
        "pain_relief": 0,
        "discomfort_relief": 0,
        "side_effect": 0,
        "experience_review": 0,
        "efficacy": 0,
        "latest_info": 0,
        "general_info": 0
    }
    
    # í†µì¦ ê´€ë ¨ ì˜ë„ ì ìˆ˜
    for pattern in PAIN_PATTERNS:
        if re.search(pattern, query_lower):
            intent_scores["pain_relief"] += 3
            if re.search(r'ë„ˆë¬´|ë§¤ìš°|ì •ë§|ì—„ì²­|ì‹¬í•˜ê²Œ', query_lower):
                intent_scores["pain_relief"] += 2
    
    # ë¶ˆí¸í•¨ ê´€ë ¨ ì˜ë„ ì ìˆ˜
    for pattern in DISCOMFORT_PATTERNS:
        if re.search(pattern, query_lower):
            intent_scores["discomfort_relief"] += 3
    
    # ë¶€ì‘ìš© ê´€ë ¨ ì˜ë„ ì ìˆ˜
    for pattern in SIDE_EFFECT_PATTERNS:
        if re.search(pattern, query_lower):
            intent_scores["side_effect"] += 5
            if re.search(r'ë¶€ì‘ìš©|ë‚˜ë¹ ì¡Œì–´|ì•…í™”|ìƒˆë¡œ\s*ìƒê²¼ì–´', query_lower):
                intent_scores["side_effect"] += 2
    
    # ê²½í—˜ë‹´ ê´€ë ¨ ì˜ë„ ì ìˆ˜
    for pattern in EXPERIENCE_PATTERNS:
        if re.search(pattern, query_lower):
            intent_scores["experience_review"] += 3
            if re.search(r'ê²½í—˜ë‹´|í›„ê¸°|ê²½í—˜|ì‚¬ìš©í›„ê¸°|ë³µìš©í›„ê¸°', query_lower):
                intent_scores["experience_review"] += 1
    
    # íš¨ëŠ¥ ê´€ë ¨ ì˜ë„ ì ìˆ˜
    for pattern in EFFICACY_PATTERNS:
        if re.search(pattern, query_lower):
            intent_scores["efficacy"] += 3
    
    # ìµœì‹  ì •ë³´ ê´€ë ¨ ì˜ë„ ì ìˆ˜
    for pattern in LATEST_PATTERNS:
        if re.search(pattern, query_lower):
            intent_scores["latest_info"] += 3
            if re.search(r'2024|2023|ìƒˆë¡œ|ì‹ ì•½', query_lower):
                intent_scores["latest_info"] += 1
    
    # ì¼ë°˜ ì •ë³´ ê¸°ë³¸ ì ìˆ˜
    intent_scores["general_info"] = 1
    
    # 2. ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì˜ë„ ì„ íƒ
    intent = max(intent_scores, key=intent_scores.get)
    
    # 3. ë¶€ì‘ìš© ì˜ë„ê°€ ìˆëŠ” ê²½ìš° ìš°ì„ ìˆœìœ„ ì¡°ì •
    print(f"ğŸ” ë¶€ì‘ìš© í‚¤ì›Œë“œ ì²´í¬: 'ë¶€ì‘ìš©' in '{query_lower}' = {'ë¶€ì‘ìš©' in query_lower}")
    if "ë¶€ì‘ìš©" in query_lower:
        print(f"âœ… ë¶€ì‘ìš© í‚¤ì›Œë“œ ë°œê²¬! í˜„ì¬ ì˜ë„ ì ìˆ˜: {intent_scores}")
        if intent_scores["side_effect"] > 0 and intent_scores["experience_review"] > 0:
            if intent_scores["side_effect"] >= intent_scores["experience_review"]:
                intent = "side_effect"
                print(f"ğŸ¯ ë¶€ì‘ìš© ì˜ë„ë¡œ ì„¤ì • (ì ìˆ˜ ë¹„êµ)")
            else:
                intent = "side_effect_experience"
                print(f"ğŸ¯ ë³µí•© ì˜ë„ë¡œ ì„¤ì •: side_effect_experience")
        elif intent_scores["side_effect"] > 0:
            intent = "side_effect"
            print(f"ğŸ¯ ë¶€ì‘ìš© ì˜ë„ë¡œ ì„¤ì • (ê¸°ì¡´ ì ìˆ˜)")
        else:
            intent = "side_effect"
            intent_scores["side_effect"] = 6
            print(f"ğŸ¯ ë¶€ì‘ìš© ì˜ë„ë¡œ ê°•ì œ ì„¤ì • (í‚¤ì›Œë“œ ê¸°ë°˜)")
    else:
        print(f"âŒ ë¶€ì‘ìš© í‚¤ì›Œë“œ ì—†ìŒ")
    
    # 4. ì•½í’ˆëª… ì¶”ì¶œ
    words = re.findall(r'\b\w+\b', query_lower)
    potential_drugs = []
    
    common_words = ['the', 'and', 'for', 'with', 'this', 'that', 'what', 'when', 'where', 'how', 'why', 'is', 'are', 'was', 'were', 'have', 'has', 'had', 'do', 'does', 'did', 'can', 'could', 'will', 'would', 'should', 'may', 'might']
    
    for word in words:
        if len(word) > 2 and word not in common_words:
            potential_drugs.append(word)
    
    # 5. ì¦ìƒ ë¶€ìœ„/ì„±ê²© ì¶”ì¶œ
    body_parts = []
    
    for part_name, patterns in BODY_PART_PATTERNS.items():
        if any(re.search(pattern, query_lower) for pattern in patterns):
            body_parts.append(part_name)
    
    # 6. ì¦ìƒ ê°•ë„/ì„±ê²©
    intensity = "moderate"
    
    for intensity_level, patterns in INTENSITY_PATTERNS.items():
        if any(re.search(pattern, query_lower) for pattern in patterns):
            intensity = intensity_level
            break
    
    return {
        "intent": intent,
        "intent_scores": intent_scores,
        "potential_drugs": potential_drugs,
        "body_parts": body_parts,
        "intensity": intensity,
        "original_query": query
    }

def create_search_terms(analysis: Dict[str, any]) -> List[str]:
    """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²€ìƒ‰ì–´ ìƒì„±"""
    search_terms = []
    
    intent = analysis.get("intent")
    potential_drugs = analysis.get("potential_drugs", [])
    body_parts = analysis.get("body_parts", [])
    intensity = analysis.get("intensity")
    
    # 1. í•œêµ­ì–´ ê²€ìƒ‰ì–´ ìš°ì„  ìƒì„±
    if potential_drugs:
        for drug in potential_drugs[:2]:
            if intent == "side_effect":
                search_terms.extend([
                    f"{drug} ë¶€ì‘ìš©",
                    f"{drug} ë¶€ì‘ìš© ê²½í—˜",
                    f"{drug} ë¶€ì‘ìš© í›„ê¸°"
                ])
            elif intent == "experience_review":
                search_terms.extend([
                    f"{drug} ê²½í—˜ë‹´",
                    f"{drug} ì‚¬ìš© í›„ê¸°",
                    f"{drug} ë³µìš© ê²½í—˜"
                ])
            elif intent == "efficacy":
                search_terms.extend([
                    f"{drug} íš¨ê³¼",
                    f"{drug} íš¨ëŠ¥",
                    f"{drug} ë³µìš© ê²°ê³¼"
                ])
    
    # 2. ì¼ë°˜ì ì¸ ì˜ë„ë³„ ê²€ìƒ‰ì–´ (í•œêµ­ì–´)
    if intent == "side_effect":
        search_terms.extend([
            "ê°ê¸°ì•½ ë¶€ì‘ìš© ê²½í—˜",
            "ê°ê¸°ì•½ ë¶€ì‘ìš© í›„ê¸°",
            "ì•½ë¬¼ ë¶€ì‘ìš© ê²½í—˜ë‹´",
            "ë¶€ì‘ìš© ê²½í—˜ë‹´",
            "ê°ê¸°ì•½ ë³µìš© í›„ ë¶€ì‘ìš©",
            "ì•½ë¬¼ ë¶€ì‘ìš© ê²½í—˜",
            "ë¶€ì‘ìš© ê²½í—˜"
        ])
    elif intent == "experience_review":
        search_terms.extend([
            "ê°ê¸°ì•½ ê²½í—˜ë‹´",
            "ì•½ë¬¼ ì‚¬ìš© í›„ê¸°",
            "ë³µìš© ê²½í—˜ë‹´",
            "ì•½ë¬¼ ê²½í—˜"
        ])
    elif intent == "latest_info":
        search_terms.extend([
            "ì‹ ì•½ ì†Œì‹",
            "ì‹ ì•½ ë‰´ìŠ¤",
            "ì‹ ì•½ ê°œë°œ",
            "ì‹ ì•½ ìŠ¹ì¸",
            "ì‹ ì•½ ìƒë¥™"
        ])
    
    # 3. ë¶€ìœ„ë³„ ê²€ìƒ‰ì–´ ì¶”ê°€
    if body_parts:
        for part in body_parts:
            if intent == "side_effect":
                search_terms.append(f"{part} ë¶€ì‘ìš© ê²½í—˜")
            elif intent == "experience_review":
                search_terms.append(f"{part} ì¹˜ë£Œ ê²½í—˜")
    
    # 4. ê°•ë„ì— ë”°ë¥¸ ê²€ìƒ‰ì–´
    if intensity == "severe":
        search_terms.append("ì‹¬í•œ ë¶€ì‘ìš© ê²½í—˜")
    elif intensity == "mild":
        search_terms.append("ê°€ë²¼ìš´ ë¶€ì‘ìš© ê²½í—˜")
    
    # ì¤‘ë³µ ì œê±°
    unique_terms = list(dict.fromkeys(search_terms))
    print(f"ğŸ“Š ìµœì¢… ê²€ìƒ‰ì–´ ëª©ë¡: {unique_terms}")
    return unique_terms[:20]

def filter_relevant_videos(videos: List[Dict], analysis: Dict[str, any]) -> List[Dict]:
    """ì›ë³¸ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì— ë”°ë¼ ì˜ìƒ í•„í„°ë§"""
    relevant_videos = []
    
    intent = analysis.get("intent")
    potential_drugs = analysis.get("potential_drugs", [])
    body_parts = analysis.get("body_parts", [])
    
    for video in videos:
        content_lower = (video["title"] + " " + video["description"]).lower()
        relevance_score = 0
        
        # 1. ì˜ë„ë³„ ê´€ë ¨ì„± ì ìˆ˜
        if intent == "side_effect":
            side_effect_keywords = ['ë¶€ì‘ìš©', 'adverse', 'negative', 'problem', 'issue', 'trouble', 'bad', 'unwanted', 'reaction']
            if any(keyword in content_lower for keyword in side_effect_keywords):
                relevance_score += 3
            else:
                continue
        
        elif intent == "experience_review":
            experience_keywords = ['ê²½í—˜', 'review', 'í›„ê¸°', 'testimonial', 'story', 'used', 'ë³µìš©', 'ì‚¬ìš©', 'took', 'tried']
            if any(keyword in content_lower for keyword in experience_keywords):
                relevance_score += 3
            else:
                continue
        
        elif intent == "latest_info":
            latest_keywords = ['ì‹ ì•½', 'ìƒˆë¡œìš´', 'ìµœì‹ ', 'ê°œë°œ', 'ìŠ¹ì¸', 'ìƒë¥™', 'ì¶œì‹œ', 'new', 'latest', 'development']
            if any(keyword in content_lower for keyword in latest_keywords):
                relevance_score += 3
            else:
                continue
        
        # 2. ì•½í’ˆëª… ê´€ë ¨ì„±
        if potential_drugs:
            for drug in potential_drugs:
                if drug.lower() in content_lower:
                    relevance_score += 4
                    break
        
        # 3. ë¶€ìœ„ ê´€ë ¨ì„±
        if body_parts:
            for part in body_parts:
                if part in content_lower:
                    relevance_score += 2
        
        # 4. ì œëª© ê´€ë ¨ì„± ì ìˆ˜
        if any(keyword in video["title"].lower() for keyword in ['ì•½', 'ê°ê¸°', 'cold', 'flu', 'medicine', 'drug', 'ì‹ ì•½', 'ì¹˜ë£Œ']):
            relevance_score += 1
        
        # ê´€ë ¨ì„± ì ìˆ˜ê°€ ì¼ì • ìˆ˜ì¤€ ì´ìƒì¸ ì˜ìƒë§Œ í¬í•¨
        if relevance_score >= 3:
            video["relevance_score"] = relevance_score
            relevant_videos.append(video)
    
    # ê´€ë ¨ì„± ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
    relevant_videos.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
    
    # ìµœëŒ€ 5ê°œë¡œ ì œí•œ
    return relevant_videos[:5]

def sns_search_node(state: QAState) -> QAState:
    """SNS ê²€ìƒ‰ ë…¸ë“œ (ìœ íŠœë¸Œ ê¸°ë°˜) - ì˜ìƒ ë‚´ìš© ì¶”ì¶œ ë° ìš”ì•½ í¬í•¨"""
    
    print("ğŸ” SNS ê²€ìƒ‰ ë…¸ë“œ ì‹¤í–‰ ì‹œì‘ (ìœ íŠœë¸Œ)")
    
    # ì¿¼ë¦¬ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ
    query = state.get("cleaned_query", "") or state.get("normalized_query", "") or state.get("query", "")
    
    print(f"ğŸ“ ë¶„ì„í•  ì¿¼ë¦¬: {query}")
    
    if not query:
        print("âŒ ì¿¼ë¦¬ê°€ ì—†ì–´ì„œ SNS ê²€ìƒ‰ ê±´ë„ˆëœ€")
        state["sns_results"] = []
        state["sns_count"] = 0
        return state
    
    # 1. ì¿¼ë¦¬ ì˜ë„ ë¶„ì„
    print("ğŸ§  ì¿¼ë¦¬ ì˜ë„ ë¶„ì„ ì‹œì‘")
    analysis = analyze_query_intent(query)
    print(f"ğŸ¯ ê°ì§€ëœ ì˜ë„: {analysis['intent']}")
    print(f"ğŸ“Š ì˜ë„ ì ìˆ˜: {analysis['intent_scores']}")
    print(f"ğŸ’Š ê°ì§€ëœ ì•½í’ˆ: {analysis['potential_drugs']}")
    print(f"ğŸ¦´ ê°ì§€ëœ ë¶€ìœ„: {analysis['body_parts']}")
    
    # 2. ê²€ìƒ‰ì–´ ìƒì„±
    print("ğŸ” ê²€ìƒ‰ì–´ ìƒì„± ì‹œì‘")
    search_terms = create_search_terms(analysis)
    print(f"ğŸ” ìƒì„±ëœ ê²€ìƒ‰ì–´: {search_terms}")
    
    all_videos = []
    
    # 3. ê° ê²€ìƒ‰ì–´ë¡œ ìœ íŠœë¸Œ ê²€ìƒ‰
    print("ğŸ“º ìœ íŠœë¸Œ ê²€ìƒ‰ ì‹œì‘")
    for search_term in search_terms:
        try:
            print(f"ğŸ” '{search_term}' ê²€ìƒ‰ ì¤‘...")
            videos = search_youtube_videos(search_term, max_videos=5)
            print(f"ğŸ“ '{search_term}' ê²€ìƒ‰ ê²°ê³¼: {len(videos)}ê°œ ì˜ìƒ")
            all_videos.extend(videos)
        except Exception as e:
            print(f"âŒ '{search_term}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            continue
    
    print(f"ğŸ“Š ì´ ìˆ˜ì§‘ëœ ì˜ìƒ: {len(all_videos)}ê°œ")
    
    # 4. ì˜ìƒ í•„í„°ë§
    print("ğŸ” ì˜ìƒ í•„í„°ë§ ì‹œì‘")
    filtered_videos = filter_relevant_videos(all_videos, analysis)
    print(f"âœ… í•„í„°ë§ í›„ ì˜ìƒ: {len(filtered_videos)}ê°œ")
    
    # 5. ì˜ìƒ ë‚´ìš© ì¶”ì¶œ ë° ìš”ì•½
    print("ğŸ“¹ ì˜ìƒ ë‚´ìš© ì¶”ì¶œ ë° ìš”ì•½ ì‹œì‘")
    enriched_videos = []
    for video in filtered_videos:
        try:
            # ìë§‰ ì¶”ì¶œ
            transcript = get_video_transcript(video["video_id"])
            
            if transcript:
                # ìë§‰ì´ ìˆìœ¼ë©´ ìš”ì•½
                summarized_content = summarize_video_content(transcript, max_length=800)
                video["transcript"] = transcript
                video["summarized_content"] = summarized_content
                video["has_transcript"] = True
                print(f"âœ… ì˜ìƒ {video['video_id']} ìë§‰ ì¶”ì¶œ ë° ìš”ì•½ ì™„ë£Œ")
            else:
                # ìë§‰ì´ ì—†ìœ¼ë©´ ì œëª©ê³¼ ì„¤ëª…ë§Œ ì‚¬ìš©
                content = f"ì œëª©: {video['title']}\nì„¤ëª…: {video['description']}"
                video["transcript"] = ""
                video["summarized_content"] = content
                video["has_transcript"] = False
                print(f"âš ï¸ ì˜ìƒ {video['video_id']} ìë§‰ ì—†ìŒ, ê¸°ë³¸ ì •ë³´ë§Œ ì‚¬ìš©")
            
            enriched_videos.append(video)
            
        except Exception as e:
            print(f"âŒ ì˜ìƒ {video['video_id']} ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ì •ë³´ëŠ” í¬í•¨
            content = f"ì œëª©: {video['title']}\nì„¤ëª…: {video['description']}"
            video["transcript"] = ""
            video["summarized_content"] = content
            video["has_transcript"] = False
            enriched_videos.append(video)
    
    # 6. Document í˜•íƒœë¡œ ë³€í™˜
    print("ğŸ“„ Document ë³€í™˜ ì‹œì‘")
    sns_docs = []
    for video in enriched_videos:
        # ìš”ì•½ëœ ë‚´ìš©ì„ ì£¼ìš” ì½˜í…ì¸ ë¡œ ì‚¬ìš©
        content = video["summarized_content"]
        
        doc = Document(
            page_content=content,
            metadata={
                "source": "youtube",
                "video_id": video["video_id"],
                "channel_title": video["channel_title"],
                "keywords": video["keywords"],
                "relevance_score": video.get("relevance_score", 0),
                "type": "youtube_video",
                "search_intent": analysis["intent"],
                "detected_drugs": analysis.get("potential_drugs", []),
                "body_parts": analysis.get("body_parts", []),
                "thumbnail": video["thumbnail"],
                "published_at": video["published_at"],
                "has_transcript": video.get("has_transcript", False),
                "transcript_length": len(video.get("transcript", "")),
                "summary_length": len(video.get("summarized_content", ""))
            }
        )
        sns_docs.append(doc)
    
    # ê²°ê³¼ë¥¼ stateì— ì €ì¥
    state["sns_results"] = sns_docs
    state["sns_count"] = len(sns_docs)
    state["sns_analysis"] = analysis
    
    print(f"ğŸ‰ SNS ê²€ìƒ‰ ì™„ë£Œ: {len(sns_docs)}ê°œ ê²°ê³¼")
    print(f"ğŸ“Š ìë§‰ ìˆëŠ” ì˜ìƒ: {sum(1 for v in enriched_videos if v.get('has_transcript', False))}ê°œ")
    
    return state 