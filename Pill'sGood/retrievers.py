import os
import re
import json
import pandas as pd
from typing import List
from langchain_core.documents import Document
from langchain.text_splitter import TokenTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain.retrievers import ContextualCompressionRetriever
from langchain_community.document_loaders import PyPDFLoader
from langchain.agents import initialize_agent, AgentType
from langchain_community.tools.tavily_search import TavilySearchResults
from cache_manager import cache_manager

# === ê³µí†µ ì„¤ì • ===
splitter = TokenTextSplitter(chunk_size=600, chunk_overlap=100)
embedding_model = OpenAIEmbeddings()
llm = ChatOpenAI(model="gpt-4o", temperature=0)
hf_model = HuggingFaceCrossEncoder(model_name="cross-encoder/ms-marco-MiniLM-L-6-v2")
compressor = CrossEncoderReranker(model=hf_model, top_n=5)

# === PDF ì¸ë±ì‹± ë° ê²€ìƒ‰ê¸° ===
pdf_path = r"C:\Users\jung\Desktop\pdf\í•œêµ­ì—ì„œ ë„ë¦¬ ì“°ì´ëŠ” ì¼ë°˜ì˜ì•½í’ˆ 20ì„ .pdf"

# ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
pdf_structured_docs = []
pdf_product_index = {}

# ìºì‹œ í™•ì¸
if cache_manager.is_vector_cache_valid("pdf", [pdf_path]) and cache_manager.is_docs_cache_valid("pdf"):
    print("ğŸ“‚ PDF ë²¡í„° DB ë° ë¬¸ì„œ ìºì‹œ ì‚¬ìš©")
    pdf_vectordb = cache_manager.load_vector_cache("pdf", embedding_model)
    pdf_structured_docs = cache_manager.load_pdf_docs_cache("pdf")
    
    if pdf_vectordb is None or pdf_structured_docs is None:
        print("âš ï¸ PDF ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤")
        pdf_vectordb = None
        pdf_structured_docs = []
        pdf_product_index = {}
    else:
        print("ğŸ“‚ PDF ë²¡í„° DB ë° ë¬¸ì„œ ìºì‹œ ë¡œë“œë¨")
        # pdf_product_indexë„ ë³µì›
        pdf_product_index = {}
        for doc in pdf_structured_docs:
            name = doc.metadata.get("ì œí’ˆëª…", "")
            if name:
                pdf_product_index.setdefault(name, []).append(doc)
else:
    print("ğŸ”„ PDF ë²¡í„° DB ìƒˆë¡œ ìƒì„±")
    pdf_vectordb = None

if pdf_vectordb is None:
    pdf_docs_raw = PyPDFLoader(pdf_path).load()
    pdf_structured_docs = []
    pdf_product_index = {}

    for doc in pdf_docs_raw:
        blocks = re.findall(r"(\d+\.\s*.+?)(?=\n\d+\.|\Z)", doc.page_content, re.DOTALL)
        for block in blocks:
            name_match = re.match(r"\d+\.\s*([^\n(]+)", block)
            if name_match:
                name = name_match.group(1).strip()
                eff = re.search(r"ì£¼ìš” íš¨ëŠ¥[:ï¼š]\s*(.*?)(?:\n|ì¼ë°˜ì ì¸ ë¶€ì‘ìš©[:ï¼š])", block, re.DOTALL)
                side = re.search(r"ì¼ë°˜ì ì¸ ë¶€ì‘ìš©[:ï¼š]\s*(.*?)(?:\n|ì„±ì¸ ê¸°ì¤€ ë³µìš©ë²•[:ï¼š])", block, re.DOTALL)
                usage = re.search(r"ì„±ì¸ ê¸°ì¤€ ë³µìš©ë²•[:ï¼š]\s*(.*?)(?:\n|$)", block, re.DOTALL)
                content = f"[ì œí’ˆëª…]: {name}\n[íš¨ëŠ¥]: {eff.group(1).strip() if eff else 'ì •ë³´ ì—†ìŒ'}\n[ë¶€ì‘ìš©]: {side.group(1).strip() if side else 'ì •ë³´ ì—†ìŒ'}\n[ì‚¬ìš©ë²•]: {usage.group(1).strip() if usage else 'ì •ë³´ ì—†ìŒ'}"

                for chunk in splitter.split_text(content):
                    doc_obj = Document(page_content=chunk, metadata={"ì œí’ˆëª…": name})
                    pdf_structured_docs.append(doc_obj)

                doc_full = Document(page_content=content, metadata={"ì œí’ˆëª…": name})
                pdf_product_index.setdefault(name, []).append(doc_full)

    pdf_vectordb = FAISS.from_documents(pdf_structured_docs, embedding_model)
    # ìºì‹œ ì €ì¥ (ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
    try:
        cache_manager.save_vector_cache("pdf", [pdf_path], pdf_vectordb)
        cache_manager.save_pdf_docs_cache("pdf", pdf_structured_docs)
        print("âœ… PDF ë²¡í„° DB ë° ë¬¸ì„œ ìºì‹œ ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ PDF ìºì‹œ ì €ì¥ ì‹¤íŒ¨, ê³„ì† ì§„í–‰: {e}")

pdf_retriever = ContextualCompressionRetriever(
    base_retriever=pdf_vectordb.as_retriever(search_type="similarity", k=20),
    base_compressor=compressor
)

# === Excel ì¸ë±ì‹± ë° ê²€ìƒ‰ê¸° ===
excel_files = [rf"C:\Users\jung\Desktop\11\eì•½ì€ìš”ì •ë³´ê²€ìƒ‰{i}.xlsx" for i in range(1, 6)]
required_columns = [
    "ì œí’ˆëª…",
    "ì´ ì•½ì˜ íš¨ëŠ¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?",  # ì •í™•í•œ ì»¬ëŸ¼ëª…
    "ì´ ì•½ì€ ì–´ë–¤ ì´ìƒë°˜ì‘ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?",  # ì •í™•í•œ ì»¬ëŸ¼ëª…
    "ì´ ì•½ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•©ë‹ˆê¹Œ?",
    "ì£¼ì„±ë¶„"  # ì£¼ì„±ë¶„ ì»¬ëŸ¼ ì¶”ê°€
]

# ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
excel_docs = []
excel_product_index = {}
product_names = []
product_names_normalized = []

# ìºì‹œ í™•ì¸
if cache_manager.is_vector_cache_valid("excel", excel_files) and cache_manager.is_docs_cache_valid("excel"):
    print("ğŸ“‚ Excel ë²¡í„° DB ë° ë¬¸ì„œ ìºì‹œ ì‚¬ìš©")
    excel_vectordb = cache_manager.load_vector_cache("excel", embedding_model)
    excel_docs = cache_manager.load_excel_docs_cache("excel")
    
    if excel_vectordb is None or excel_docs is None:
        print("âš ï¸ Excel ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤")
        excel_vectordb = None
        excel_docs = []
        excel_product_index = {}
        product_names = []
        product_names_normalized = []
    else:
        print("ğŸ“‚ Excel ë²¡í„° DB ë° ë¬¸ì„œ ìºì‹œ ë¡œë“œë¨")
        # product_namesì™€ product_names_normalizedë„ ë³µì›
        product_names = [doc.metadata.get("ì œí’ˆëª…", "") for doc in excel_docs if doc.metadata.get("ì œí’ˆëª…")]
        product_names = list(set(product_names))  # ì¤‘ë³µ ì œê±°
        product_names_normalized = [re.sub(r"[^\wê°€-í£]", "", name.lower()) for name in product_names]
        
        # excel_product_indexë„ ë³µì›
        excel_product_index = {}
        for doc in excel_docs:
            name = doc.metadata.get("ì œí’ˆëª…", "")
            if name:
                excel_product_index.setdefault(name, []).append(doc)
else:
    print("ğŸ”„ Excel ë²¡í„° DB ìƒˆë¡œ ìƒì„±")
    excel_vectordb = None

if excel_vectordb is None:
    excel_docs = []
    excel_product_index = {}
    product_names = []
    product_names_normalized = []

    for file in excel_files:
        if not os.path.exists(file): 
            print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file}")
            continue
        df = pd.read_excel(file)
        if not all(col in df.columns for col in required_columns): 
            print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {[col for col in required_columns if col not in df.columns]}")
            continue

        df = df[required_columns].fillna("ì •ë³´ ì—†ìŒ")
        for _, row in df.iterrows():
            name = row["ì œí’ˆëª…"].strip()
            product_names.append(name)
            product_names_normalized.append(re.sub(r"[^\wê°€-í£]", "", name.lower()))

            # ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„í• : ì‚¬ìš©ë²•ì„ ë³„ë„ ì²­í¬ë¡œ ë¶„ë¦¬í•˜ì—¬ ë³´ì¡´
            efficacy = row['ì´ ì•½ì˜ íš¨ëŠ¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?']
            side_effects = row['ì´ ì•½ì€ ì–´ë–¤ ì´ìƒë°˜ì‘ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?']
            usage = row['ì´ ì•½ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•©ë‹ˆê¹Œ?']
            main_ingredient = row.get('ì£¼ì„±ë¶„', 'ì •ë³´ ì—†ìŒ')  # ì£¼ì„±ë¶„ ì¶”ê°€
            
            # ë©”ì¸ ë‚´ìš© (íš¨ëŠ¥ + ë¶€ì‘ìš©)
            content_main = (
                f"[ì œí’ˆëª…]: {name}\n"
                f"[ì£¼ì„±ë¶„]: {main_ingredient}\n"
                f"[íš¨ëŠ¥]: {efficacy}\n"
                f"[ë¶€ì‘ìš©]: {side_effects}"
            )
            
            # ì‚¬ìš©ë²• ë‚´ìš© (ë³„ë„ ì²­í¬)
            content_usage = (
                f"[ì œí’ˆëª…]: {name}\n"
                f"[ì£¼ì„±ë¶„]: {main_ingredient}\n"
                f"[ì‚¬ìš©ë²•]: {usage}"
            )
            
            # ë©”ì¸ ì²­í¬ ë¶„í• 
            main_chunks = splitter.split_text(content_main)
            for chunk in main_chunks:
                doc_obj = Document(page_content=chunk, metadata={
                    "ì œí’ˆëª…": name, 
                    "ì£¼ì„±ë¶„": main_ingredient,
                    "type": "main"
                })
                excel_docs.append(doc_obj)
            
            # ì‚¬ìš©ë²• ì²­í¬ ë¶„í•  (ë” í° ì²­í¬ í¬ê¸° ì‚¬ìš©)
            usage_chunks = splitter.split_text(content_usage)
            for chunk in usage_chunks:
                doc_obj = Document(page_content=chunk, metadata={
                    "ì œí’ˆëª…": name, 
                    "ì£¼ì„±ë¶„": main_ingredient,
                    "type": "usage"
                })
                excel_docs.append(doc_obj)

            # ì „ì²´ ë‚´ìš©ë„ ë³´ì¡´ (ê²€ìƒ‰ìš©)
            doc_full = Document(page_content=f"{content_main}\n{content_usage}", metadata={"ì œí’ˆëª…": name})
            excel_product_index.setdefault(name, []).append(doc_full)

    # ë°°ì¹˜ë³„ ì„ë² ë”© ì²˜ë¦¬ (í† í° ì œí•œ ë°©ì§€)
    print(f"ğŸ”„ Excel ë°ì´í„° ë°°ì¹˜ë³„ ì„ë² ë”© ì²˜ë¦¬: ì´ {len(excel_docs)}ê°œ ë¬¸ì„œ")
    batch_size = 50
    excel_vectordb = None
    
    for i in range(0, len(excel_docs), batch_size):
        batch = excel_docs[i:i+batch_size]
        
        try:
            if excel_vectordb is None:
                # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„° DB ì´ˆê¸°í™”
                excel_vectordb = FAISS.from_documents(batch, embedding_model)
            else:
                # ê¸°ì¡´ ë²¡í„° DBì— ë°°ì¹˜ ì¶”ê°€
                excel_vectordb.add_documents(batch)
        except Exception as e:
            print(f"âš ï¸ ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue
    
    if excel_vectordb is None:
        print("âŒ ëª¨ë“  ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨")
        excel_vectordb = FAISS.from_documents([], embedding_model)  # ë¹ˆ ë²¡í„° DB ìƒì„±
    
    # ìºì‹œ ì €ì¥ (ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
    try:
        cache_manager.save_vector_cache("excel", excel_files, excel_vectordb)
        cache_manager.save_excel_docs_cache("excel", excel_docs)
        print("âœ… Excel ë²¡í„° DB ë° ë¬¸ì„œ ìºì‹œ ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ Excel ìºì‹œ ì €ì¥ ì‹¤íŒ¨, ê³„ì† ì§„í–‰: {e}")

excel_retriever = ContextualCompressionRetriever(
    base_retriever=excel_vectordb.as_retriever(search_type="similarity", k=20),
    base_compressor=compressor
)

# === ì™¸ë¶€ ê²€ìƒ‰ê¸° ===
search_agent = initialize_agent(
    tools=[TavilySearchResults()],
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    handle_parsing_errors=True,
    verbose=False
)

# === LLM ìš”ì•½ê¸° ===
def extract_active_ingredients_from_medicine(medicine_name: str) -> List[str]:
    """ì•½í’ˆëª…ìœ¼ë¡œë¶€í„° ì£¼ì„±ë¶„ ì¶”ì¶œ"""
    ingredients = []
    
    try:
        # Excel DBì—ì„œ í•´ë‹¹ ì•½í’ˆì˜ ì£¼ì„±ë¶„ ì°¾ê¸°
        for doc in excel_docs:
            if doc.metadata.get("ì œí’ˆëª…") == medicine_name:
                # ì£¼ì„±ë¶„ ì •ë³´ê°€ ë©”íƒ€ë°ì´í„°ì— ìˆëŠ”ì§€ í™•ì¸
                if "ì£¼ì„±ë¶„" in doc.metadata:
                    ingredient = doc.metadata["ì£¼ì„±ë¶„"]
                    if ingredient and ingredient != "ì •ë³´ ì—†ìŒ":
                        # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì„±ë¶„ë“¤ì„ ë¶„ë¦¬
                        if ',' in ingredient:
                            ingredients = [ing.strip() for ing in ingredient.split(',') if ing.strip()]
                        else:
                            ingredients = [ingredient.strip()]
                        break
        
        # ì£¼ì„±ë¶„ì´ ì—†ìœ¼ë©´ ë¬¸ì„œ ë‚´ìš©ì—ì„œ ì¶”ì¶œ ì‹œë„
        if not ingredients:
            for doc in excel_docs:
                if doc.metadata.get("ì œí’ˆëª…") == medicine_name:
                    content = doc.page_content
                    # ì£¼ì„±ë¶„ ê´€ë ¨ íŒ¨í„´ ì°¾ê¸°
                    import re
                    patterns = [
                        r'ì£¼ì„±ë¶„[:\s]*([^,\n]+)',
                        r'ì„±ë¶„[:\s]*([^,\n]+)',
                        r'ì£¼ìš”ì„±ë¶„[:\s]*([^,\n]+)'
                    ]
                    
                    for pattern in patterns:
                        matches = re.findall(pattern, content)
                        if matches:
                            # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì„±ë¶„ë“¤ì„ ë¶„ë¦¬
                            for match in matches:
                                if ',' in match:
                                    ingredients.extend([ing.strip() for ing in match.split(',') if ing.strip()])
                                else:
                                    ingredients.append(match.strip())
                            break
        
        print(f"ğŸ” {medicine_name} ì£¼ì„±ë¶„ ì¶”ì¶œ: {ingredients}")
        return ingredients
        
    except Exception as e:
        print(f"âŒ ì£¼ì„±ë¶„ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return []

def summarize_structured_json(text: str) -> dict:
    prompt = f"""
    ë‹¤ìŒ ì•½í’ˆ ê´€ë ¨ í…ìŠ¤íŠ¸ì—ì„œ í•­ëª©ë³„ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì¤˜.
    í•­ëª©ì€ 'ì œí’ˆëª…', 'íš¨ëŠ¥', 'ë¶€ì‘ìš©', 'ì‚¬ìš©ë²•'ì´ë©°, ì—†ìœ¼ë©´ "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ í‘œê¸°í•´ì¤˜.

    í…ìŠ¤íŠ¸:
    {text}

    ê²°ê³¼ í˜•ì‹:
    {{
      "ì œí’ˆëª…": "...",
      "íš¨ëŠ¥": "...",
      "ë¶€ì‘ìš©": "...",
      "ì‚¬ìš©ë²•": "..."
    }}
    """
    try:
        response = llm.invoke(prompt)
        return json.loads(response.content.strip())
    except:
        return {
            "ì œí’ˆëª…": "",
            "íš¨ëŠ¥": "ì •ë³´ ì—†ìŒ",
            "ë¶€ì‘ìš©": "ì •ë³´ ì—†ìŒ",
            "ì‚¬ìš©ë²•": "ì •ë³´ ì—†ìŒ"
        }

# === ì„±ë¶„ ìƒ‰ì¸ êµ¬ì¶• (ë™ì ) ===
def build_ingredient_index():
    """Excel DBì—ì„œ ëª¨ë“  ì„±ë¶„ëª…ì„ ë™ì ìœ¼ë¡œ ì¶”ì¶œí•˜ê³  ì„±ë¶„â†’ì œí’ˆ ë§¤í•‘ ìƒì„±"""
    all_ingredients = set()
    ingredient_to_products = {}
    
    print("ğŸ“Š ì„±ë¶„ ìƒ‰ì¸ êµ¬ì¶• ì¤‘...")
    
    for doc in excel_docs:
        product_name = doc.metadata.get("ì œí’ˆëª…", "")
        ingredients_str = doc.metadata.get("ì£¼ì„±ë¶„", "")
        
        if ingredients_str and ingredients_str != "ì •ë³´ ì—†ìŒ":
            # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì„±ë¶„ë“¤ ë¶„ë¦¬
            ingredients = [ing.strip() for ing in ingredients_str.split(',') if ing.strip()]
            
            for ingredient in ingredients:
                all_ingredients.add(ingredient)
                
                # ì„±ë¶„ â†’ ì œí’ˆ ë§¤í•‘
                if ingredient not in ingredient_to_products:
                    ingredient_to_products[ingredient] = []
                if product_name and product_name not in ingredient_to_products[ingredient]:
                    ingredient_to_products[ingredient].append(product_name)
    
    print(f"âœ… ì¶”ì¶œëœ ì„±ë¶„ ì´ {len(all_ingredients)}ê°œ")
    print(f"âœ… ì„±ë¶„â†’ì œí’ˆ ë§¤í•‘ {len(ingredient_to_products)}ê°œ ìƒì„±")
    
    return all_ingredients, ingredient_to_products

# ì „ì—­ ë³€ìˆ˜ë¡œ ì €ì¥ (ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)
known_ingredients, ingredient_to_products_map = build_ingredient_index()

def find_products_by_ingredient(ingredient_name: str) -> List[str]:
    """íŠ¹ì • ì„±ë¶„ì´ í¬í•¨ëœ ì œí’ˆ ëª©ë¡ ë°˜í™˜"""
    return ingredient_to_products_map.get(ingredient_name, [])

# === ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ë°ì´í„° ì²˜ë¦¬ ===
dosage_warning_ingredients = {}  # ì„±ë¶„ëª… -> ìš©ëŸ‰ ì •ë³´ ë§¤í•‘
dosage_warning_loaded = False

def load_dosage_warning_data():
    """ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ"""
    global dosage_warning_ingredients, dosage_warning_loaded
    
    print(f"ğŸ” ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì‹œë„ - í˜„ì¬ ìƒíƒœ: loaded={dosage_warning_loaded}")
    
    if dosage_warning_loaded:
        print(f"ğŸ“‚ ì´ë¯¸ ë¡œë“œë¨ - ì´ {len(dosage_warning_ingredients)}ê°œ ì„±ë¶„")
        return dosage_warning_ingredients
    
    try:
        # ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ (ì‹¤ì œ íŒŒì¼ ìœ„ì¹˜ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
        dosage_file_path = r"C:\Users\jung\Desktop\22\ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ë¦¬ìŠ¤íŠ¸_250530.xlsx"
        
        print(f"ğŸ” íŒŒì¼ ì¡´ì¬ í™•ì¸: {dosage_file_path}")
        print(f"ğŸ” íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(dosage_file_path)}")
        
        if not os.path.exists(dosage_file_path):
            print(f"âš ï¸ ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dosage_file_path}")
            dosage_warning_loaded = True
            return dosage_warning_ingredients
        
        print("ğŸ“Š ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì¤‘...")
        df = pd.read_excel(dosage_file_path)
        print(f"ğŸ“Š ì—‘ì…€ íŒŒì¼ ë¡œë“œ ì™„ë£Œ - í–‰ ìˆ˜: {len(df)}, ì»¬ëŸ¼: {list(df.columns)}")
        
        # ì‹¤ì œ ë°ì´í„°ê°€ ì‹œì‘ë˜ëŠ” í–‰ ì°¾ê¸° (í—¤ë” í–‰ ê±´ë„ˆë›°ê¸°)
        data_start_row = 0
        for idx, row in df.iterrows():
            # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì— ìˆ«ìê°€ ìˆëŠ” í–‰ì„ ì°¾ê¸° (ì—°ë²ˆ)
            first_col = str(row.iloc[0]).strip()
            if first_col.isdigit():
                data_start_row = idx
                break
        
        print(f"ğŸ” ë°ì´í„° ì‹œì‘ í–‰: {data_start_row}")
        
        # ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš© (í—¤ë” í–‰ ì œì™¸)
        if data_start_row > 0:
            df = df.iloc[data_start_row:].reset_index(drop=True)
            print(f"ğŸ” í—¤ë” ì œê±° í›„ í–‰ ìˆ˜: {len(df)}")
        
        # ì»¬ëŸ¼ëª…ì„ ìˆ˜ë™ìœ¼ë¡œ ë§¤í•‘ (Unnamed ì»¬ëŸ¼ë“¤)
        # ì¼ë°˜ì ìœ¼ë¡œ ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ëŠ” ë‹¤ìŒ ìˆœì„œ: ì—°ë²ˆ, ì„±ë¶„ëª…(êµ­ë¬¸), ì„±ë¶„ëª…(ì˜ë¬¸), ì œí˜•, 1ì¼ ìµœëŒ€ìš©ëŸ‰, ë¹„ê³ 
        actual_columns = {
            'korean_name': df.columns[1] if len(df.columns) > 1 else None,  # ë‘ ë²ˆì§¸ ì»¬ëŸ¼
            'english_name': df.columns[2] if len(df.columns) > 2 else None,  # ì„¸ ë²ˆì§¸ ì»¬ëŸ¼
            'formulation': df.columns[3] if len(df.columns) > 3 else None,    # ë„¤ ë²ˆì§¸ ì»¬ëŸ¼
            'max_daily_dose': df.columns[4] if len(df.columns) > 4 else None, # ë‹¤ì„¯ ë²ˆì§¸ ì»¬ëŸ¼
            'remarks': df.columns[5] if len(df.columns) > 5 else None         # ì—¬ì„¯ ë²ˆì§¸ ì»¬ëŸ¼
        }
        
        print(f"ğŸ” ìˆ˜ë™ ì»¬ëŸ¼ ë§¤í•‘ ê²°ê³¼: {actual_columns}")
        
        # None ê°’ ì œê±°
        actual_columns = {k: v for k, v in actual_columns.items() if v is not None}
        
        if not actual_columns:
            print("âŒ ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            dosage_warning_loaded = True
            return dosage_warning_ingredients
        
        # ë°ì´í„° ì²˜ë¦¬
        processed_count = 0
        print(f"ğŸ” ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ - ì´ {len(df)}í–‰")
        
        for idx, row in df.iterrows():
            korean_name = str(row.get(actual_columns.get('korean_name', ''), '')).strip()
            english_name = str(row.get(actual_columns.get('english_name', ''), '')).strip()
            formulation = str(row.get(actual_columns.get('formulation', ''), '')).strip()
            max_dose = str(row.get(actual_columns.get('max_daily_dose', ''), '')).strip()
            remarks = str(row.get(actual_columns.get('remarks', ''), '')).strip()
            
            if idx < 5:  # ì²˜ìŒ 5ê°œ í–‰ë§Œ ë¡œê·¸ ì¶œë ¥
                print(f"ğŸ” í–‰ {idx}: í•œê¸€='{korean_name}', ì˜ë¬¸='{english_name}', ìš©ëŸ‰='{max_dose}'")
            
            if not korean_name or korean_name == 'nan':
                continue
            
            # í•œêµ­ì–´ ì„±ë¶„ëª…ìœ¼ë¡œ ë§¤í•‘
            dosage_warning_ingredients[korean_name] = {
                'korean_name': korean_name,
                'english_name': english_name,
                'formulation': formulation,
                'max_daily_dose': max_dose,
                'remarks': remarks
            }
            
            # ì˜ì–´ ì„±ë¶„ëª…ìœ¼ë¡œë„ ë§¤í•‘ (ìˆëŠ” ê²½ìš°)
            if english_name and english_name != 'nan':
                dosage_warning_ingredients[english_name] = {
                    'korean_name': korean_name,
                    'english_name': english_name,
                    'formulation': formulation,
                    'max_daily_dose': max_dose,
                    'remarks': remarks
                }
            
            processed_count += 1
        
        print(f"âœ… ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ {len(dosage_warning_ingredients)}ê°œ ë¡œë“œ ì™„ë£Œ (ì²˜ë¦¬ëœ í–‰: {processed_count}ê°œ)")
        print(f"ğŸ” ë¡œë“œëœ ì„±ë¶„ ì˜ˆì‹œ: {list(dosage_warning_ingredients.keys())[:5]}")
        dosage_warning_loaded = True
        
    except Exception as e:
        print(f"âŒ ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        dosage_warning_loaded = True
    
    return dosage_warning_ingredients

def find_dosage_warning_info(ingredient_name: str) -> dict:
    """íŠ¹ì • ì„±ë¶„ì˜ ìš©ëŸ‰ì£¼ì˜ ì •ë³´ ì°¾ê¸°"""
    if not dosage_warning_loaded:
        load_dosage_warning_data()
    
    # ì •í™•í•œ ë§¤ì¹­ ì‹œë„
    if ingredient_name in dosage_warning_ingredients:
        return dosage_warning_ingredients[ingredient_name]
    
    # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„ (ì„±ë¶„ëª…ì´ í¬í•¨ëœ ê²½ìš°)
    for key, value in dosage_warning_ingredients.items():
        if ingredient_name in key or key in ingredient_name:
            return value
    
    # ì •ê·œí™”ëœ ë§¤ì¹­ ì‹œë„
    normalized_ingredient = re.sub(r'[^\wê°€-í£]', '', ingredient_name.lower())
    for key, value in dosage_warning_ingredients.items():
        normalized_key = re.sub(r'[^\wê°€-í£]', '', key.lower())
        if normalized_ingredient in normalized_key or normalized_key in normalized_ingredient:
            return value
    
    return None

def get_medicine_dosage_warnings(medicine_name: str) -> List[dict]:
    """ì•½í’ˆì˜ ì£¼ì„±ë¶„ë“¤ ì¤‘ ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ì´ ìˆëŠ”ì§€ í™•ì¸"""
    print(f"ğŸ” ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ í™•ì¸ ì‹œì‘: '{medicine_name}'")
    
    warnings = []
    
    # ì•½í’ˆì˜ ì£¼ì„±ë¶„ ì¶”ì¶œ
    ingredients = extract_active_ingredients_from_medicine(medicine_name)
    print(f"ğŸ” ì¶”ì¶œëœ ì£¼ì„±ë¶„: {ingredients}")
    
    for ingredient in ingredients:
        print(f"ğŸ” ì„±ë¶„ '{ingredient}' ìš©ëŸ‰ì£¼ì˜ í™•ì¸ ì¤‘...")
        dosage_info = find_dosage_warning_info(ingredient)
        if dosage_info:
            print(f"âœ… ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ë°œê²¬: '{ingredient}' - {dosage_info['max_daily_dose']}")
            warnings.append({
                'ingredient': ingredient,
                'dosage_info': dosage_info
            })
        else:
            print(f"âŒ ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ì•„ë‹˜: '{ingredient}'")
    
    print(f"ğŸ” ìµœì¢… ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ê°œìˆ˜: {len(warnings)}")
    return warnings

# === Export ëŒ€ìƒ ===
__all__ = [
    "pdf_retriever",
    "excel_retriever",
    "product_names",
    "product_names_normalized",
    "search_agent",
    "summarize_structured_json",
    "extract_active_ingredients_from_medicine",
    "pdf_product_index",
    "excel_product_index",
    "pdf_structured_docs",
    "excel_docs",
    "known_ingredients",
    "ingredient_to_products_map",
    "find_products_by_ingredient",
    "load_dosage_warning_data",
    "find_dosage_warning_info",
    "get_medicine_dosage_warnings"
]
