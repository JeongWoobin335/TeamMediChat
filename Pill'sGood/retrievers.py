import os
import re
import json
import pandas as pd
from typing import List
from langchain_core.documents import Document
from langchain.text_splitter import TokenTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain.retrievers import ContextualCompressionRetriever
from langchain_community.document_loaders import PyPDFLoader
from langchain.agents import initialize_agent, AgentType
from langchain_community.tools.tavily_search import TavilySearchResults
from cache_manager import cache_manager

# === ê³µí†µ ì„¤ì • ===
splitter = TokenTextSplitter(chunk_size=600, chunk_overlap=100)
embedding_model = OpenAIEmbeddings()
llm = ChatOpenAI(model="gpt-4o", temperature=0)
hf_model = HuggingFaceCrossEncoder(model_name="cross-encoder/ms-marco-MiniLM-L-6-v2")
compressor = CrossEncoderReranker(model=hf_model, top_n=5)

# === PDF ì¸ë±ì‹± ë° ê²€ìƒ‰ê¸° ===
pdf_path = r"C:\Users\jung\Desktop\pdf\í•œêµ­ì—ì„œ ë„ë¦¬ ì“°ì´ëŠ” ì¼ë°˜ì˜ì•½í’ˆ 20ì„ .pdf"

# ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
pdf_structured_docs = []
pdf_product_index = {}

# ìºì‹œ í™•ì¸
if cache_manager.is_vector_cache_valid("pdf", [pdf_path]):
    print("ğŸ“‚ PDF ë²¡í„° DB ìºì‹œ ì‚¬ìš©")
    pdf_vectordb = cache_manager.load_vector_cache("pdf", embedding_model)
    if pdf_vectordb is None:
        print("âš ï¸ PDF ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤")
        pdf_vectordb = None
    else:
        print("ğŸ“‚ pdf ë²¡í„° DB ìºì‹œ ë¡œë“œë¨")
        # ìºì‹œì—ì„œ ë¡œë“œëœ ê²½ìš°ì—ë„ pdf_structured_docs ì„¤ì •
        pdf_docs_raw = PyPDFLoader(pdf_path).load()
        pdf_structured_docs = []
        pdf_product_index = {}

        for doc in pdf_docs_raw:
            blocks = re.findall(r"(\d+\.\s*.+?)(?=\n\d+\.|\Z)", doc.page_content, re.DOTALL)
            for block in blocks:
                name_match = re.match(r"\d+\.\s*([^\n(]+)", block)
                if name_match:
                    name = name_match.group(1).strip()
                    eff = re.search(r"ì£¼ìš” íš¨ëŠ¥[:ï¼š]\s*(.*?)(?:\n|ì¼ë°˜ì ì¸ ë¶€ì‘ìš©[:ï¼š])", block, re.DOTALL)
                    side = re.search(r"ì¼ë°˜ì ì¸ ë¶€ì‘ìš©[:ï¼š]\s*(.*?)(?:\n|ì„±ì¸ ê¸°ì¤€ ë³µìš©ë²•[:ï¼š])", block, re.DOTALL)
                    usage = re.search(r"ì„±ì¸ ê¸°ì¤€ ë³µìš©ë²•[:ï¼š]\s*(.*?)(?:\n|$)", block, re.DOTALL)
                    content = f"[ì œí’ˆëª…]: {name}\n[íš¨ëŠ¥]: {eff.group(1).strip() if eff else 'ì •ë³´ ì—†ìŒ'}\n[ë¶€ì‘ìš©]: {side.group(1).strip() if side else 'ì •ë³´ ì—†ìŒ'}\n[ì‚¬ìš©ë²•]: {usage.group(1).strip() if usage else 'ì •ë³´ ì—†ìŒ'}"

                    for chunk in splitter.split_text(content):
                        doc_obj = Document(page_content=chunk, metadata={"ì œí’ˆëª…": name})
                        pdf_structured_docs.append(doc_obj)

                    doc_full = Document(page_content=content, metadata={"ì œí’ˆëª…": name})
                    pdf_product_index.setdefault(name, []).append(doc_full)
else:
    print("ğŸ”„ PDF ë²¡í„° DB ìƒˆë¡œ ìƒì„±")
    pdf_vectordb = None

if pdf_vectordb is None:
    pdf_docs_raw = PyPDFLoader(pdf_path).load()
    pdf_structured_docs = []
    pdf_product_index = {}

    for doc in pdf_docs_raw:
        blocks = re.findall(r"(\d+\.\s*.+?)(?=\n\d+\.|\Z)", doc.page_content, re.DOTALL)
        for block in blocks:
            name_match = re.match(r"\d+\.\s*([^\n(]+)", block)
            if name_match:
                name = name_match.group(1).strip()
                eff = re.search(r"ì£¼ìš” íš¨ëŠ¥[:ï¼š]\s*(.*?)(?:\n|ì¼ë°˜ì ì¸ ë¶€ì‘ìš©[:ï¼š])", block, re.DOTALL)
                side = re.search(r"ì¼ë°˜ì ì¸ ë¶€ì‘ìš©[:ï¼š]\s*(.*?)(?:\n|ì„±ì¸ ê¸°ì¤€ ë³µìš©ë²•[:ï¼š])", block, re.DOTALL)
                usage = re.search(r"ì„±ì¸ ê¸°ì¤€ ë³µìš©ë²•[:ï¼š]\s*(.*?)(?:\n|$)", block, re.DOTALL)
                content = f"[ì œí’ˆëª…]: {name}\n[íš¨ëŠ¥]: {eff.group(1).strip() if eff else 'ì •ë³´ ì—†ìŒ'}\n[ë¶€ì‘ìš©]: {side.group(1).strip() if side else 'ì •ë³´ ì—†ìŒ'}\n[ì‚¬ìš©ë²•]: {usage.group(1).strip() if usage else 'ì •ë³´ ì—†ìŒ'}"

                for chunk in splitter.split_text(content):
                    doc_obj = Document(page_content=chunk, metadata={"ì œí’ˆëª…": name})
                    pdf_structured_docs.append(doc_obj)

                doc_full = Document(page_content=content, metadata={"ì œí’ˆëª…": name})
                pdf_product_index.setdefault(name, []).append(doc_full)

    pdf_vectordb = FAISS.from_documents(pdf_structured_docs, embedding_model)
    # ìºì‹œ ì €ì¥ (ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
    try:
        cache_manager.save_vector_cache("pdf", [pdf_path], pdf_vectordb)
    except Exception as e:
        print(f"âš ï¸ PDF ìºì‹œ ì €ì¥ ì‹¤íŒ¨, ê³„ì† ì§„í–‰: {e}")

pdf_retriever = ContextualCompressionRetriever(
    base_retriever=pdf_vectordb.as_retriever(search_type="similarity", k=20),
    base_compressor=compressor
)

# === Excel ì¸ë±ì‹± ë° ê²€ìƒ‰ê¸° ===
excel_files = [rf"C:\Users\jung\Desktop\11\eì•½ì€ìš”ì •ë³´ê²€ìƒ‰{i}.xlsx" for i in range(1, 6)]
required_columns = [
    "ì œí’ˆëª…",
    "ì´ ì•½ì˜ íš¨ëŠ¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?",
    "ì´ ì•½ì€ ì–´ë–¤ ì´ìƒë°˜ì‘ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?",
    "ì´ ì•½ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•©ë‹ˆê¹Œ?"
]

# ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
excel_docs = []
excel_product_index = {}
product_names = []
product_names_normalized = []

# ìºì‹œ í™•ì¸
if cache_manager.is_vector_cache_valid("excel", excel_files):
    print("ğŸ“‚ Excel ë²¡í„° DB ìºì‹œ ì‚¬ìš©")
    excel_vectordb = cache_manager.load_vector_cache("excel", embedding_model)
    if excel_vectordb is None:
        print("âš ï¸ Excel ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤")
        excel_vectordb = None
    else:
        print("ğŸ“‚ Excel ë²¡í„° DB ìºì‹œ ë¡œë“œë¨")
        # ìºì‹œì—ì„œ ë¡œë“œëœ ê²½ìš°ì—ë„ excel_docs ì„¤ì •
        excel_docs = []
        excel_product_index = {}
        product_names = []
        product_names_normalized = []
        
        for file in excel_files:
            if not os.path.exists(file): 
                print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file}")
                continue
            df = pd.read_excel(file)
            if not all(col in df.columns for col in required_columns): 
                print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {[col for col in required_columns if col not in df.columns]}")
                continue

            df = df[required_columns].fillna("ì •ë³´ ì—†ìŒ")
            for _, row in df.iterrows():
                name = row["ì œí’ˆëª…"].strip()
                product_names.append(name)
                product_names_normalized.append(re.sub(r"[^\wê°€-í£]", "", name.lower()))

                # ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„í• : ì‚¬ìš©ë²•ì„ ë³„ë„ ì²­í¬ë¡œ ë¶„ë¦¬í•˜ì—¬ ë³´ì¡´
                efficacy = row['ì´ ì•½ì˜ íš¨ëŠ¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?']
                side_effects = row['ì´ ì•½ì€ ì–´ë–¤ ì´ìƒë°˜ì‘ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?']
                usage = row['ì´ ì•½ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•©ë‹ˆê¹Œ?']
                
                # ë©”ì¸ ë‚´ìš© (íš¨ëŠ¥ + ë¶€ì‘ìš©)
                content_main = (
                    f"[ì œí’ˆëª…]: {name}\n"
                    f"[íš¨ëŠ¥]: {efficacy}\n"
                    f"[ë¶€ì‘ìš©]: {side_effects}"
                )
                
                # ì‚¬ìš©ë²• ë‚´ìš© (ë³„ë„ ì²­í¬)
                content_usage = (
                    f"[ì œí’ˆëª…]: {name}\n"
                    f"[ì‚¬ìš©ë²•]: {usage}"
                )
                
                # ë©”ì¸ ì²­í¬ ë¶„í• 
                main_chunks = splitter.split_text(content_main)
                for chunk in main_chunks:
                    doc_obj = Document(page_content=chunk, metadata={"ì œí’ˆëª…": name, "type": "main"})
                    excel_docs.append(doc_obj)
                
                # ì‚¬ìš©ë²• ì²­í¬ ë¶„í• 
                usage_chunks = splitter.split_text(content_usage)
                for chunk in usage_chunks:
                    doc_obj = Document(page_content=chunk, metadata={"ì œí’ˆëª…": name, "type": "usage"})
                    excel_docs.append(doc_obj)

                # ì „ì²´ ë‚´ìš©ë„ ë³´ì¡´ (ê²€ìƒ‰ìš©)
                doc_full = Document(page_content=f"{content_main}\n{content_usage}", metadata={"ì œí’ˆëª…": name})
                excel_product_index.setdefault(name, []).append(doc_full)
else:
    print("ğŸ”„ Excel ë²¡í„° DB ìƒˆë¡œ ìƒì„±")
    excel_vectordb = None

if excel_vectordb is None:
    excel_docs = []
    excel_product_index = {}
    product_names = []
    product_names_normalized = []

    for file in excel_files:
        if not os.path.exists(file): 
            print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file}")
            continue
        df = pd.read_excel(file)
        if not all(col in df.columns for col in required_columns): 
            print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {[col for col in required_columns if col not in df.columns]}")
            continue

        df = df[required_columns].fillna("ì •ë³´ ì—†ìŒ")
        for _, row in df.iterrows():
            name = row["ì œí’ˆëª…"].strip()
            product_names.append(name)
            product_names_normalized.append(re.sub(r"[^\wê°€-í£]", "", name.lower()))

            # ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„í• : ì‚¬ìš©ë²•ì„ ë³„ë„ ì²­í¬ë¡œ ë¶„ë¦¬í•˜ì—¬ ë³´ì¡´
            efficacy = row['ì´ ì•½ì˜ íš¨ëŠ¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?']
            side_effects = row['ì´ ì•½ì€ ì–´ë–¤ ì´ìƒë°˜ì‘ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?']
            usage = row['ì´ ì•½ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•©ë‹ˆê¹Œ?']
            
            # ë©”ì¸ ë‚´ìš© (íš¨ëŠ¥ + ë¶€ì‘ìš©)
            content_main = (
                f"[ì œí’ˆëª…]: {name}\n"
                f"[íš¨ëŠ¥]: {efficacy}\n"
                f"[ë¶€ì‘ìš©]: {side_effects}"
            )
            
            # ì‚¬ìš©ë²• ë‚´ìš© (ë³„ë„ ì²­í¬)
            content_usage = (
                f"[ì œí’ˆëª…]: {name}\n"
                f"[ì‚¬ìš©ë²•]: {usage}"
            )
            
            # ë©”ì¸ ì²­í¬ ë¶„í• 
            main_chunks = splitter.split_text(content_main)
            for chunk in main_chunks:
                doc_obj = Document(page_content=chunk, metadata={"ì œí’ˆëª…": name, "type": "main"})
                excel_docs.append(doc_obj)
            
            # ì‚¬ìš©ë²• ì²­í¬ ë¶„í•  (ë” í° ì²­í¬ í¬ê¸° ì‚¬ìš©)
            usage_chunks = splitter.split_text(content_usage)
            for chunk in usage_chunks:
                doc_obj = Document(page_content=chunk, metadata={"ì œí’ˆëª…": name, "type": "usage"})
                excel_docs.append(doc_obj)

            # ì „ì²´ ë‚´ìš©ë„ ë³´ì¡´ (ê²€ìƒ‰ìš©)
            doc_full = Document(page_content=f"{content_main}\n{content_usage}", metadata={"ì œí’ˆëª…": name})
            excel_product_index.setdefault(name, []).append(doc_full)

    # ë°°ì¹˜ë³„ ì„ë² ë”© ì²˜ë¦¬ (í† í° ì œí•œ ë°©ì§€)
    print(f"ğŸ”„ Excel ë°ì´í„° ë°°ì¹˜ë³„ ì„ë² ë”© ì²˜ë¦¬: ì´ {len(excel_docs)}ê°œ ë¬¸ì„œ")
    batch_size = 50
    excel_vectordb = None
    
    for i in range(0, len(excel_docs), batch_size):
        batch = excel_docs[i:i+batch_size]
        
        try:
            if excel_vectordb is None:
                # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„° DB ì´ˆê¸°í™”
                excel_vectordb = FAISS.from_documents(batch, embedding_model)
            else:
                # ê¸°ì¡´ ë²¡í„° DBì— ë°°ì¹˜ ì¶”ê°€
                excel_vectordb.add_documents(batch)
        except Exception as e:
            print(f"âš ï¸ ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue
    
    if excel_vectordb is None:
        print("âŒ ëª¨ë“  ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨")
        excel_vectordb = FAISS.from_documents([], embedding_model)  # ë¹ˆ ë²¡í„° DB ìƒì„±
    
    # ìºì‹œ ì €ì¥ (ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
    try:
        cache_manager.save_vector_cache("excel", excel_files, excel_vectordb)
        print("âœ… Excel ë²¡í„° DB ìºì‹œ ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ Excel ìºì‹œ ì €ì¥ ì‹¤íŒ¨, ê³„ì† ì§„í–‰: {e}")

excel_retriever = ContextualCompressionRetriever(
    base_retriever=excel_vectordb.as_retriever(search_type="similarity", k=20),
    base_compressor=compressor
)

# === ì™¸ë¶€ ê²€ìƒ‰ê¸° ===
search_agent = initialize_agent(
    tools=[TavilySearchResults()],
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    handle_parsing_errors=True,
    verbose=False
)

# === LLM ìš”ì•½ê¸° ===
def summarize_structured_json(text: str) -> dict:
    prompt = f"""
ë‹¤ìŒ ì•½í’ˆ ê´€ë ¨ í…ìŠ¤íŠ¸ì—ì„œ í•­ëª©ë³„ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì¤˜.
í•­ëª©ì€ 'ì œí’ˆëª…', 'íš¨ëŠ¥', 'ë¶€ì‘ìš©', 'ì‚¬ìš©ë²•'ì´ë©°, ì—†ìœ¼ë©´ "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ í‘œê¸°í•´ì¤˜.

í…ìŠ¤íŠ¸:
{text}

ê²°ê³¼ í˜•ì‹:
{{
  "ì œí’ˆëª…": "...",
  "íš¨ëŠ¥": "...",
  "ë¶€ì‘ìš©": "...",
  "ì‚¬ìš©ë²•": "..."
}}
"""
    try:
        response = llm.invoke(prompt)
        return json.loads(response.content.strip())
    except:
        return {
            "ì œí’ˆëª…": "",
            "íš¨ëŠ¥": "ì •ë³´ ì—†ìŒ",
            "ë¶€ì‘ìš©": "ì •ë³´ ì—†ìŒ",
            "ì‚¬ìš©ë²•": "ì •ë³´ ì—†ìŒ"
        }

# === Export ëŒ€ìƒ ===
__all__ = [
    "pdf_retriever",
    "excel_retriever",
    "product_names",
    "product_names_normalized",
    "search_agent",
    "summarize_structured_json",
    "pdf_product_index",
    "excel_product_index",
    "pdf_structured_docs",
    "excel_docs"
]
