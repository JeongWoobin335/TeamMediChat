import os
import re
import json
import pandas as pd
from typing import List
from langchain_core.documents import Document
from langchain.text_splitter import TokenTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain.retrievers import ContextualCompressionRetriever
from langchain_community.document_loaders import PyPDFLoader
from langchain.agents import initialize_agent, AgentType
from langchain_community.tools.tavily_search import TavilySearchResults
from cache_manager import cache_manager

# === ê³µí†µ ì„¤ì • ===
splitter = TokenTextSplitter(chunk_size=600, chunk_overlap=100)
embedding_model = OpenAIEmbeddings()
llm = ChatOpenAI(model="gpt-4o", temperature=0)
hf_model = HuggingFaceCrossEncoder(model_name="cross-encoder/ms-marco-MiniLM-L-6-v2")
compressor = CrossEncoderReranker(model=hf_model, top_n=5)

# === PDF ì¸ë±ì‹± ë° ê²€ìƒ‰ê¸° ===
pdf_path = r"C:\Users\jung\Desktop\pdf\í•œêµ­ì—ì„œ ë„ë¦¬ ì“°ì´ëŠ” ì¼ë°˜ì˜ì•½í’ˆ 20ì„ .pdf"

# ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
pdf_structured_docs = []
pdf_product_index = {}

# ìºì‹œ í™•ì¸
if cache_manager.is_vector_cache_valid("pdf", [pdf_path]) and cache_manager.is_docs_cache_valid("pdf"):
    print("ğŸ“‚ PDF ë²¡í„° DB ë° ë¬¸ì„œ ìºì‹œ ì‚¬ìš©")
    pdf_vectordb = cache_manager.load_vector_cache("pdf", embedding_model)
    pdf_structured_docs = cache_manager.load_pdf_docs_cache("pdf")
    
    if pdf_vectordb is None or pdf_structured_docs is None:
        print("âš ï¸ PDF ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤")
        pdf_vectordb = None
        pdf_structured_docs = []
        pdf_product_index = {}
    else:
        print("ğŸ“‚ PDF ë²¡í„° DB ë° ë¬¸ì„œ ìºì‹œ ë¡œë“œë¨")
        # pdf_product_indexë„ ë³µì›
        pdf_product_index = {}
        for doc in pdf_structured_docs:
            name = doc.metadata.get("ì œí’ˆëª…", "")
            if name:
                pdf_product_index.setdefault(name, []).append(doc)
else:
    print("ğŸ”„ PDF ë²¡í„° DB ìƒˆë¡œ ìƒì„±")
    pdf_vectordb = None

if pdf_vectordb is None:
    pdf_docs_raw = PyPDFLoader(pdf_path).load()
    pdf_structured_docs = []
    pdf_product_index = {}

    for doc in pdf_docs_raw:
        blocks = re.findall(r"(\d+\.\s*.+?)(?=\n\d+\.|\Z)", doc.page_content, re.DOTALL)
        for block in blocks:
            name_match = re.match(r"\d+\.\s*([^\n(]+)", block)
            if name_match:
                name = name_match.group(1).strip()
                eff = re.search(r"ì£¼ìš” íš¨ëŠ¥[:ï¼š]\s*(.*?)(?:\n|ì¼ë°˜ì ì¸ ë¶€ì‘ìš©[:ï¼š])", block, re.DOTALL)
                side = re.search(r"ì¼ë°˜ì ì¸ ë¶€ì‘ìš©[:ï¼š]\s*(.*?)(?:\n|ì„±ì¸ ê¸°ì¤€ ë³µìš©ë²•[:ï¼š])", block, re.DOTALL)
                usage = re.search(r"ì„±ì¸ ê¸°ì¤€ ë³µìš©ë²•[:ï¼š]\s*(.*?)(?:\n|$)", block, re.DOTALL)
                content = f"[ì œí’ˆëª…]: {name}\n[íš¨ëŠ¥]: {eff.group(1).strip() if eff else 'ì •ë³´ ì—†ìŒ'}\n[ë¶€ì‘ìš©]: {side.group(1).strip() if side else 'ì •ë³´ ì—†ìŒ'}\n[ì‚¬ìš©ë²•]: {usage.group(1).strip() if usage else 'ì •ë³´ ì—†ìŒ'}"

                for chunk in splitter.split_text(content):
                    doc_obj = Document(page_content=chunk, metadata={"ì œí’ˆëª…": name})
                    pdf_structured_docs.append(doc_obj)

                doc_full = Document(page_content=content, metadata={"ì œí’ˆëª…": name})
                pdf_product_index.setdefault(name, []).append(doc_full)

    pdf_vectordb = FAISS.from_documents(pdf_structured_docs, embedding_model)
    # ìºì‹œ ì €ì¥ (ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
    try:
        cache_manager.save_vector_cache("pdf", [pdf_path], pdf_vectordb)
        cache_manager.save_pdf_docs_cache("pdf", pdf_structured_docs)
        print("âœ… PDF ë²¡í„° DB ë° ë¬¸ì„œ ìºì‹œ ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ PDF ìºì‹œ ì €ì¥ ì‹¤íŒ¨, ê³„ì† ì§„í–‰: {e}")

pdf_retriever = ContextualCompressionRetriever(
    base_retriever=pdf_vectordb.as_retriever(search_type="similarity", k=20),
    base_compressor=compressor
)

# === Excel ì¸ë±ì‹± ë° ê²€ìƒ‰ê¸° ===
# ê¸°ì¡´ Excel íŒŒì¼ë“¤
excel_files = [rf"C:\Users\jung\Desktop\11\eì•½ì€ìš”ì •ë³´ê²€ìƒ‰{i}.xlsx" for i in range(1, 6)]

# ============================================
# ìƒˆ Excel íŒŒì¼ ì¶”ê°€í•˜ê¸°
# ============================================
# ìƒˆ Excel íŒŒì¼ ê²½ë¡œ ì¶”ê°€
new_excel_file = r"C:\Users\jung\Desktop\33\OpenData_ItemPermitDetail20251115.xls"
excel_files.append(new_excel_file)

# íŒŒì¼ë³„ ì»¬ëŸ¼ëª… ë§¤í•‘ (íŒŒì¼ ê²½ë¡œë¥¼ í‚¤ë¡œ ì‚¬ìš©)
file_column_mappings = {}  # {íŒŒì¼ê²½ë¡œ: ì»¬ëŸ¼ë§¤í•‘}

# ê¸°ë³¸ ì»¬ëŸ¼ëª… (ê¸°ì¡´ íŒŒì¼ìš©)
default_columns = {
    "ì œí’ˆëª…": "ì œí’ˆëª…",
    "íš¨ëŠ¥": "ì´ ì•½ì˜ íš¨ëŠ¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?",
    "ë¶€ì‘ìš©": "ì´ ì•½ì€ ì–´ë–¤ ì´ìƒë°˜ì‘ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?",
    "ì‚¬ìš©ë²•": "ì´ ì•½ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•©ë‹ˆê¹Œ?",
    "ì£¼ì„±ë¶„": "ì£¼ì„±ë¶„"
}

# ìƒˆ Excel íŒŒì¼ì˜ ì»¬ëŸ¼ëª… ë§¤í•‘ (íš¨ëŠ¥íš¨ê³¼, ìš©ë²•ìš©ëŸ‰, ì£¼ì˜ì‚¬í•­ë§Œ ì¶”ì¶œ)
file_column_mappings[new_excel_file] = {
    "ì œí’ˆëª…": "í’ˆëª©ëª…",      # ì œí’ˆëª… ì»¬ëŸ¼ (í•„ìˆ˜)
    "íš¨ëŠ¥": "íš¨ëŠ¥íš¨ê³¼",      # íš¨ëŠ¥íš¨ê³¼ ì»¬ëŸ¼
    "ë¶€ì‘ìš©": "ì£¼ì˜ì‚¬í•­",    # ì£¼ì˜ì‚¬í•­ ì»¬ëŸ¼
    "ì‚¬ìš©ë²•": "ìš©ë²•ìš©ëŸ‰",    # ìš©ë²•ìš©ëŸ‰ ì»¬ëŸ¼
    "ì£¼ì„±ë¶„": ""             # ì£¼ì„±ë¶„ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ë¹ˆ ë¬¸ìì—´)
}

# ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
excel_docs = []
excel_product_index = {}
product_names = []
product_names_normalized = []

# ìºì‹œ í™•ì¸
if cache_manager.is_vector_cache_valid("excel", excel_files) and cache_manager.is_docs_cache_valid("excel"):
    print("ğŸ“‚ Excel ë²¡í„° DB ë° ë¬¸ì„œ ìºì‹œ ì‚¬ìš©")
    excel_vectordb = cache_manager.load_vector_cache("excel", embedding_model)
    excel_docs = cache_manager.load_excel_docs_cache("excel")
    
    if excel_vectordb is None or excel_docs is None:
        print("âš ï¸ Excel ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤")
        excel_vectordb = None
        excel_docs = []
        excel_product_index = {}
        product_names = []
        product_names_normalized = []
    else:
        print("ğŸ“‚ Excel ë²¡í„° DB ë° ë¬¸ì„œ ìºì‹œ ë¡œë“œë¨")
        # product_namesì™€ product_names_normalizedë„ ë³µì›
        product_names = [doc.metadata.get("ì œí’ˆëª…", "") for doc in excel_docs if doc.metadata.get("ì œí’ˆëª…")]
        product_names = list(set(product_names))  # ì¤‘ë³µ ì œê±°
        product_names_normalized = [re.sub(r"[^\wê°€-í£]", "", name.lower()) for name in product_names]
        
        # excel_product_indexë„ ë³µì›
        excel_product_index = {}
        for doc in excel_docs:
            name = doc.metadata.get("ì œí’ˆëª…", "")
            if name:
                excel_product_index.setdefault(name, []).append(doc)
else:
    print("ğŸ”„ Excel ë²¡í„° DB ìƒˆë¡œ ìƒì„±")
    excel_vectordb = None

if excel_vectordb is None:
    excel_docs = []
    excel_product_index = {}
    product_names = []
    product_names_normalized = []

    for file in excel_files:
        if not os.path.exists(file): 
            print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file}")
            continue
        
        df = pd.read_excel(file)
        
        # íŒŒì¼ë³„ ì»¬ëŸ¼ ë§¤í•‘ í™•ì¸
        if file in file_column_mappings:
            col_mapping = file_column_mappings[file]
        else:
            # ê¸°ë³¸ ë§¤í•‘ ì‚¬ìš© (ê¸°ì¡´ íŒŒì¼)
            col_mapping = default_columns
        
        # ì‹¤ì œ ì»¬ëŸ¼ëª… í™•ì¸ (ë¹ˆ ë¬¸ìì—´ ì œì™¸)
        required_cols = [col for col in col_mapping.values() if col]  # ë¹ˆ ë¬¸ìì—´ ì œì™¸
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            print(f"âš ï¸ íŒŒì¼ '{os.path.basename(file)}'ì—ì„œ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols}")
            print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
            # ëˆ„ë½ëœ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
            continue
        
        # ë§¤í•‘ëœ ì»¬ëŸ¼ìœ¼ë¡œ ë°ì´í„° ì¶”ì¶œ
        # ì£¼ì„±ë¶„ì´ ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš°ë¥¼ ìœ„í•´ ì‹¤ì œ ì‚¬ìš©í•  ì»¬ëŸ¼ë§Œ ì„ íƒ
        actual_cols = []
        actual_keys = []
        for key, col_name in col_mapping.items():
            if col_name:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                actual_cols.append(col_name)
                actual_keys.append(key)
        
        df_selected = df[actual_cols].fillna("ì •ë³´ ì—†ìŒ")
        df_selected.columns = actual_keys  # í‘œì¤€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€ê²½
        
        # ì£¼ì„±ë¶„ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ìœ„í•´ ê¸°ë³¸ê°’ ì¶”ê°€
        if 'ì£¼ì„±ë¶„' not in df_selected.columns:
            df_selected['ì£¼ì„±ë¶„'] = 'ì •ë³´ ì—†ìŒ'
        
        for row_idx, row in df_selected.iterrows():
            name = row["ì œí’ˆëª…"].strip()
            if not name or name == "ì •ë³´ ì—†ìŒ":
                continue
                
            product_names.append(name)
            product_names_normalized.append(re.sub(r"[^\wê°€-í£]", "", name.lower()))

            # ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„í• : ì‚¬ìš©ë²•ì„ ë³„ë„ ì²­í¬ë¡œ ë¶„ë¦¬í•˜ì—¬ ë³´ì¡´
            efficacy = row['íš¨ëŠ¥']
            side_effects = row['ë¶€ì‘ìš©']
            usage = row['ì‚¬ìš©ë²•']
            main_ingredient = row.get('ì£¼ì„±ë¶„', 'ì •ë³´ ì—†ìŒ')
            
            # PDF ë§í¬ëŠ” ë‚˜ì¤‘ì— í•„ìš”í•  ë•Œ ë‹¤ìš´ë¡œë“œí•˜ë„ë¡ URLë§Œ ì €ì¥
            # Excel ë¡œë“œ ì‹œì—ëŠ” PDF ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•ŠìŒ (ì„±ëŠ¥ í–¥ìƒ)
            
            # ë©”ì¸ ë‚´ìš© (íš¨ëŠ¥ + ë¶€ì‘ìš©)
            content_main = (
                f"[ì œí’ˆëª…]: {name}\n"
                f"[ì£¼ì„±ë¶„]: {main_ingredient}\n"
                f"[íš¨ëŠ¥]: {efficacy}\n"
                f"[ë¶€ì‘ìš©]: {side_effects}"
            )
            
            # ì‚¬ìš©ë²• ë‚´ìš© (ë³„ë„ ì²­í¬)
            content_usage = (
                f"[ì œí’ˆëª…]: {name}\n"
                f"[ì£¼ì„±ë¶„]: {main_ingredient}\n"
                f"[ì‚¬ìš©ë²•]: {usage}"
            )
            
            # ë©”ì¸ ì²­í¬ ë¶„í• 
            main_chunks = splitter.split_text(content_main)
            for chunk in main_chunks:
                doc_obj = Document(page_content=chunk, metadata={
                    "ì œí’ˆëª…": name, 
                    "ì£¼ì„±ë¶„": main_ingredient,
                    "type": "main",
                    "excel_file": file,  # ì›ë³¸ Excel íŒŒì¼ ê²½ë¡œ
                    "excel_row_index": row_idx  # Excel í–‰ ì¸ë±ìŠ¤
                })
                excel_docs.append(doc_obj)
            
            # ì‚¬ìš©ë²• ì²­í¬ ë¶„í•  (ë” í° ì²­í¬ í¬ê¸° ì‚¬ìš©)
            usage_chunks = splitter.split_text(content_usage)
            for chunk in usage_chunks:
                doc_obj = Document(page_content=chunk, metadata={
                    "ì œí’ˆëª…": name, 
                    "ì£¼ì„±ë¶„": main_ingredient,
                    "type": "usage",
                    "excel_file": file,  # ì›ë³¸ Excel íŒŒì¼ ê²½ë¡œ
                    "excel_row_index": row_idx  # Excel í–‰ ì¸ë±ìŠ¤
                })
                excel_docs.append(doc_obj)

            # ì „ì²´ ë‚´ìš©ë„ ë³´ì¡´ (ê²€ìƒ‰ìš©)
            doc_full = Document(page_content=f"{content_main}\n{content_usage}", metadata={"ì œí’ˆëª…": name})
            excel_product_index.setdefault(name, []).append(doc_full)

    # ë°°ì¹˜ë³„ ì„ë² ë”© ì²˜ë¦¬ (í† í° ì œí•œ ë°©ì§€)
    print(f"ğŸ”„ Excel ë°ì´í„° ë°°ì¹˜ë³„ ì„ë² ë”© ì²˜ë¦¬: ì´ {len(excel_docs)}ê°œ ë¬¸ì„œ")
    batch_size = 50
    excel_vectordb = None
    
    for i in range(0, len(excel_docs), batch_size):
        batch = excel_docs[i:i+batch_size]
        
        try:
            if excel_vectordb is None:
                # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„° DB ì´ˆê¸°í™”
                excel_vectordb = FAISS.from_documents(batch, embedding_model)
            else:
                # ê¸°ì¡´ ë²¡í„° DBì— ë°°ì¹˜ ì¶”ê°€
                excel_vectordb.add_documents(batch)
        except Exception as e:
            print(f"âš ï¸ ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue
    
    if excel_vectordb is None:
        print("âŒ ëª¨ë“  ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨")
        excel_vectordb = FAISS.from_documents([], embedding_model)  # ë¹ˆ ë²¡í„° DB ìƒì„±
    
    # ìºì‹œ ì €ì¥ (ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
    try:
        cache_manager.save_vector_cache("excel", excel_files, excel_vectordb)
        cache_manager.save_excel_docs_cache("excel", excel_docs)
        print("âœ… Excel ë²¡í„° DB ë° ë¬¸ì„œ ìºì‹œ ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ Excel ìºì‹œ ì €ì¥ ì‹¤íŒ¨, ê³„ì† ì§„í–‰: {e}")

excel_retriever = ContextualCompressionRetriever(
    base_retriever=excel_vectordb.as_retriever(search_type="similarity", k=20),
    base_compressor=compressor
)

# === ì™¸ë¶€ ê²€ìƒ‰ê¸° ===
search_agent = initialize_agent(
    tools=[TavilySearchResults()],
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    handle_parsing_errors=True,
    verbose=False
)

# === LLM ìš”ì•½ê¸° ===
def extract_active_ingredients_from_medicine(medicine_name: str) -> List[str]:
    """ì•½í’ˆëª…ìœ¼ë¡œë¶€í„° ì£¼ì„±ë¶„ ì¶”ì¶œ"""
    ingredients = []
    
    try:
        # Excel DBì—ì„œ í•´ë‹¹ ì•½í’ˆì˜ ì£¼ì„±ë¶„ ì°¾ê¸°
        for doc in excel_docs:
            if doc.metadata.get("ì œí’ˆëª…") == medicine_name:
                # ì£¼ì„±ë¶„ ì •ë³´ê°€ ë©”íƒ€ë°ì´í„°ì— ìˆëŠ”ì§€ í™•ì¸
                if "ì£¼ì„±ë¶„" in doc.metadata:
                    ingredient = doc.metadata["ì£¼ì„±ë¶„"]
                    if ingredient and ingredient != "ì •ë³´ ì—†ìŒ":
                        # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì„±ë¶„ë“¤ì„ ë¶„ë¦¬
                        if ',' in ingredient:
                            ingredients = [ing.strip() for ing in ingredient.split(',') if ing.strip()]
                        else:
                            ingredients = [ingredient.strip()]
                        break
        
        # ì£¼ì„±ë¶„ì´ ì—†ìœ¼ë©´ ë¬¸ì„œ ë‚´ìš©ì—ì„œ ì¶”ì¶œ ì‹œë„
        if not ingredients:
            for doc in excel_docs:
                if doc.metadata.get("ì œí’ˆëª…") == medicine_name:
                    content = doc.page_content
                    # ì£¼ì„±ë¶„ ê´€ë ¨ íŒ¨í„´ ì°¾ê¸°
                    import re
                    patterns = [
                        r'ì£¼ì„±ë¶„[:\s]*([^,\n]+)',
                        r'ì„±ë¶„[:\s]*([^,\n]+)',
                        r'ì£¼ìš”ì„±ë¶„[:\s]*([^,\n]+)'
                    ]
                    
                    for pattern in patterns:
                        matches = re.findall(pattern, content)
                        if matches:
                            # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì„±ë¶„ë“¤ì„ ë¶„ë¦¬
                            for match in matches:
                                if ',' in match:
                                    ingredients.extend([ing.strip() for ing in match.split(',') if ing.strip()])
                                else:
                                    ingredients.append(match.strip())
                            break
        
        print(f"ğŸ” {medicine_name} ì£¼ì„±ë¶„ ì¶”ì¶œ: {ingredients}")
        return ingredients
        
    except Exception as e:
        print(f"âŒ ì£¼ì„±ë¶„ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return []

def summarize_structured_json(text: str) -> dict:
    prompt = f"""
    ë‹¤ìŒ ì•½í’ˆ ê´€ë ¨ í…ìŠ¤íŠ¸ì—ì„œ í•­ëª©ë³„ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì¤˜.
    í•­ëª©ì€ 'ì œí’ˆëª…', 'íš¨ëŠ¥', 'ë¶€ì‘ìš©', 'ì‚¬ìš©ë²•'ì´ë©°, ì—†ìœ¼ë©´ "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ í‘œê¸°í•´ì¤˜.

    í…ìŠ¤íŠ¸:
    {text}

    ê²°ê³¼ í˜•ì‹:
    {{
      "ì œí’ˆëª…": "...",
      "íš¨ëŠ¥": "...",
      "ë¶€ì‘ìš©": "...",
      "ì‚¬ìš©ë²•": "..."
    }}
    """
    try:
        response = llm.invoke(prompt)
        return json.loads(response.content.strip())
    except:
        return {
            "ì œí’ˆëª…": "",
            "íš¨ëŠ¥": "ì •ë³´ ì—†ìŒ",
            "ë¶€ì‘ìš©": "ì •ë³´ ì—†ìŒ",
            "ì‚¬ìš©ë²•": "ì •ë³´ ì—†ìŒ"
        }

# === ì„±ë¶„ ìƒ‰ì¸ êµ¬ì¶• (ë™ì ) ===
def build_ingredient_index():
    """Excel DBì—ì„œ ëª¨ë“  ì„±ë¶„ëª…ì„ ë™ì ìœ¼ë¡œ ì¶”ì¶œí•˜ê³  ì„±ë¶„â†’ì œí’ˆ ë§¤í•‘ ìƒì„±"""
    all_ingredients = set()
    ingredient_to_products = {}
    
    print("ğŸ“Š ì„±ë¶„ ìƒ‰ì¸ êµ¬ì¶• ì¤‘...")
    
    for doc in excel_docs:
        product_name = doc.metadata.get("ì œí’ˆëª…", "")
        ingredients_str = doc.metadata.get("ì£¼ì„±ë¶„", "")
        
        if ingredients_str and ingredients_str != "ì •ë³´ ì—†ìŒ":
            # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì„±ë¶„ë“¤ ë¶„ë¦¬
            ingredients = [ing.strip() for ing in ingredients_str.split(',') if ing.strip()]
            
            for ingredient in ingredients:
                all_ingredients.add(ingredient)
                
                # ì„±ë¶„ â†’ ì œí’ˆ ë§¤í•‘
                if ingredient not in ingredient_to_products:
                    ingredient_to_products[ingredient] = []
                if product_name and product_name not in ingredient_to_products[ingredient]:
                    ingredient_to_products[ingredient].append(product_name)
    
    print(f"âœ… ì¶”ì¶œëœ ì„±ë¶„ ì´ {len(all_ingredients)}ê°œ")
    print(f"âœ… ì„±ë¶„â†’ì œí’ˆ ë§¤í•‘ {len(ingredient_to_products)}ê°œ ìƒì„±")
    
    return all_ingredients, ingredient_to_products

# ì „ì—­ ë³€ìˆ˜ë¡œ ì €ì¥ (ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)
known_ingredients, ingredient_to_products_map = build_ingredient_index()

def find_products_by_ingredient(ingredient_name: str) -> List[str]:
    """íŠ¹ì • ì„±ë¶„ì´ í¬í•¨ëœ ì œí’ˆ ëª©ë¡ ë°˜í™˜"""
    return ingredient_to_products_map.get(ingredient_name, [])

# === ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ë°ì´í„° ì²˜ë¦¬ ===
dosage_warning_ingredients = {}  # ì„±ë¶„ëª… -> ìš©ëŸ‰ ì •ë³´ ë§¤í•‘
dosage_warning_loaded = False

def load_dosage_warning_data():
    """ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ (ìƒˆ íŒŒì¼ í˜•ì‹: OpenData_PotOpenDurIngr_D20251115.xls)"""
    global dosage_warning_ingredients, dosage_warning_loaded
    
    print(f"ğŸ” ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì‹œë„ - í˜„ì¬ ìƒíƒœ: loaded={dosage_warning_loaded}")
    
    if dosage_warning_loaded:
        print(f"ğŸ“‚ ì´ë¯¸ ë¡œë“œë¨ - ì´ {len(dosage_warning_ingredients)}ê°œ ì„±ë¶„")
        return dosage_warning_ingredients
    
    try:
        # ìƒˆ ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
        dosage_file_path = r"C:\Users\jung\Desktop\22\OpenData_PotOpenDurIngr_D20251115.xls"
        
        print(f"ğŸ” íŒŒì¼ ì¡´ì¬ í™•ì¸: {dosage_file_path}")
        print(f"ğŸ” íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(dosage_file_path)}")
        
        if not os.path.exists(dosage_file_path):
            print(f"âš ï¸ ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dosage_file_path}")
            dosage_warning_loaded = True
            return dosage_warning_ingredients
        
        print("ğŸ“Š ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì¤‘...")
        df = pd.read_excel(dosage_file_path)
        print(f"ğŸ“Š ì—‘ì…€ íŒŒì¼ ë¡œë“œ ì™„ë£Œ - í–‰ ìˆ˜: {len(df)}, ì»¬ëŸ¼: {list(df.columns)}")
        
        # ì‚¬ìš©í•  ì»¬ëŸ¼ í™•ì¸
        required_columns = ['ë‹¨ì¼ë³µí•©êµ¬ë¶„ì½”ë“œ', 'DURì„±ë¶„ëª…', 'ë³µí•©ì œ', '1ì¼ìµœëŒ€ìš©ëŸ‰']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}")
            print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
            dosage_warning_loaded = True
            return dosage_warning_ingredients
        
        # ë°ì´í„° ì²˜ë¦¬
        processed_count = 0
        print(f"ğŸ” ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ - ì´ {len(df)}í–‰")
        
        for idx, row in df.iterrows():
            # ì‚¬ìš©í•  ì»¬ëŸ¼ ì¶”ì¶œ
            single_complex = str(row.get('ë‹¨ì¼ë³µí•©êµ¬ë¶„ì½”ë“œ', '')).strip()
            ingredient_name = str(row.get('DURì„±ë¶„ëª…', '')).strip()
            complex_medicine = str(row.get('ë³µí•©ì œ', '')).strip()
            max_dose = str(row.get('1ì¼ìµœëŒ€ìš©ëŸ‰', '')).strip()
            
            # NaN ê°’ ì²˜ë¦¬
            if pd.isna(row.get('DURì„±ë¶„ëª…')) or ingredient_name == 'nan' or not ingredient_name:
                continue
            
            if idx < 5:  # ì²˜ìŒ 5ê°œ í–‰ë§Œ ë¡œê·¸ ì¶œë ¥
                print(f"ğŸ” í–‰ {idx}: ì„±ë¶„ëª…='{ingredient_name}', ë‹¨ì¼/ë³µí•©='{single_complex}', ë³µí•©ì œ='{complex_medicine}', ìš©ëŸ‰='{max_dose}'")
            
            # ë°ì´í„° êµ¬ì¡° êµ¬ì„± (ê¸°ì¡´ êµ¬ì¡°ì™€ í˜¸í™˜ì„± ìœ ì§€)
            ingredient_data = {
                'korean_name': ingredient_name,
                'english_name': '',  # ìƒˆ íŒŒì¼ì—ëŠ” ì˜ë¬¸ëª…ì´ ë³„ë„ ì»¬ëŸ¼ìœ¼ë¡œ ì—†ìŒ
                'formulation': '',  # ì œí˜• ì •ë³´ëŠ” ë³„ë„ ì»¬ëŸ¼ìœ¼ë¡œ ì—†ìŒ
                'max_daily_dose': max_dose if max_dose != 'nan' else '',
                'remarks': f"ë‹¨ì¼/ë³µí•©: {single_complex}" + (f", ë³µí•©ì œ: {complex_medicine}" if complex_medicine != 'nan' and complex_medicine else ""),
                'single_complex': single_complex,  # ìƒˆ í•„ë“œ ì¶”ê°€
                'complex_medicine': complex_medicine if complex_medicine != 'nan' else ''  # ìƒˆ í•„ë“œ ì¶”ê°€
            }
            
            # í•œêµ­ì–´ ì„±ë¶„ëª…ìœ¼ë¡œ ë§¤í•‘
            dosage_warning_ingredients[ingredient_name] = ingredient_data
            
            # ë³µí•©ì œì¸ ê²½ìš° ë³µí•©ì œ ì„±ë¶„ëª…ë„ ë§¤í•‘ (ê´€ê³„ì„±ë¶„ ì •ë³´ í™œìš©)
            if single_complex == 'ë³µí•©' and complex_medicine and complex_medicine != 'nan':
                # ë³µí•©ì œ ì •ë³´ì—ì„œ ì„±ë¶„ëª… ì¶”ì¶œ ì‹œë„ (ì˜ˆ: "[D001312]Naltrexone(ë‚ íŠ¸ë ‰ì†)" í˜•ì‹)
                # ê´„í˜¸ ì•ˆì˜ í•œê¸€ëª… ì¶”ì¶œ
                korean_match = re.search(r'\(([ê°€-í£]+)\)', complex_medicine)
                if korean_match:
                    complex_ingredient_name = korean_match.group(1)
                    # ë³µí•©ì œ ì„±ë¶„ë„ ë³„ë„ë¡œ ë§¤í•‘ (ìš©ëŸ‰ ì •ë³´ëŠ” ì£¼ì„±ë¶„ê³¼ ë™ì¼)
                    if complex_ingredient_name not in dosage_warning_ingredients:
                        dosage_warning_ingredients[complex_ingredient_name] = {
                            **ingredient_data,
                            'korean_name': complex_ingredient_name,
                            'remarks': f"ë³µí•©ì œ êµ¬ì„± ì„±ë¶„ (ì£¼ì„±ë¶„: {ingredient_name})"
                        }
            
            processed_count += 1
        
        print(f"âœ… ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ {len(dosage_warning_ingredients)}ê°œ ë¡œë“œ ì™„ë£Œ (ì²˜ë¦¬ëœ í–‰: {processed_count}ê°œ)")
        print(f"ğŸ” ë¡œë“œëœ ì„±ë¶„ ì˜ˆì‹œ: {list(dosage_warning_ingredients.keys())[:5]}")
        dosage_warning_loaded = True
        
    except Exception as e:
        print(f"âŒ ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        dosage_warning_loaded = True
    
    return dosage_warning_ingredients

def find_dosage_warning_info(ingredient_name: str) -> dict:
    """íŠ¹ì • ì„±ë¶„ì˜ ìš©ëŸ‰ì£¼ì˜ ì •ë³´ ì°¾ê¸°"""
    if not dosage_warning_loaded:
        load_dosage_warning_data()
    
    # ì •í™•í•œ ë§¤ì¹­ ì‹œë„
    if ingredient_name in dosage_warning_ingredients:
        return dosage_warning_ingredients[ingredient_name]
    
    # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„ (ì„±ë¶„ëª…ì´ í¬í•¨ëœ ê²½ìš°)
    for key, value in dosage_warning_ingredients.items():
        if ingredient_name in key or key in ingredient_name:
            return value
    
    # ì •ê·œí™”ëœ ë§¤ì¹­ ì‹œë„
    normalized_ingredient = re.sub(r'[^\wê°€-í£]', '', ingredient_name.lower())
    for key, value in dosage_warning_ingredients.items():
        normalized_key = re.sub(r'[^\wê°€-í£]', '', key.lower())
        if normalized_ingredient in normalized_key or normalized_key in normalized_ingredient:
            return value
    
    return None

def get_medicine_dosage_warnings(medicine_name: str) -> List[dict]:
    """ì•½í’ˆì˜ ì£¼ì„±ë¶„ë“¤ ì¤‘ ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ì´ ìˆëŠ”ì§€ í™•ì¸"""
    print(f"ğŸ” ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ í™•ì¸ ì‹œì‘: '{medicine_name}'")
    
    warnings = []
    
    # ì•½í’ˆì˜ ì£¼ì„±ë¶„ ì¶”ì¶œ
    ingredients = extract_active_ingredients_from_medicine(medicine_name)
    print(f"ğŸ” ì¶”ì¶œëœ ì£¼ì„±ë¶„: {ingredients}")
    
    for ingredient in ingredients:
        print(f"ğŸ” ì„±ë¶„ '{ingredient}' ìš©ëŸ‰ì£¼ì˜ í™•ì¸ ì¤‘...")
        dosage_info = find_dosage_warning_info(ingredient)
        if dosage_info:
            print(f"âœ… ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ë°œê²¬: '{ingredient}' - {dosage_info['max_daily_dose']}")
            warnings.append({
                'ingredient': ingredient,
                'dosage_info': dosage_info
            })
        else:
            print(f"âŒ ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ì•„ë‹˜: '{ingredient}'")
    
    print(f"ğŸ” ìµœì¢… ìš©ëŸ‰ì£¼ì˜ ì„±ë¶„ ê°œìˆ˜: {len(warnings)}")
    return warnings

# === ì—°ë ¹ëŒ€ ê¸ˆê¸° ì„±ë¶„ ë°ì´í„° ì²˜ë¦¬ ===
age_contraindication_ingredients = {}  # ì„±ë¶„ëª… -> ì—°ë ¹ëŒ€ë³„ ê¸ˆê¸° ì •ë³´ ë§¤í•‘
age_contraindication_loaded = False

def load_age_contraindication_data():
    """ì—°ë ¹ëŒ€ ê¸ˆê¸° ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ (OpenData_PotOpenDurIngr_B20251117.xls)"""
    global age_contraindication_ingredients, age_contraindication_loaded
    
    print(f"ğŸ” ì—°ë ¹ëŒ€ ê¸ˆê¸° ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì‹œë„ - í˜„ì¬ ìƒíƒœ: loaded={age_contraindication_loaded}")
    
    if age_contraindication_loaded:
        print(f"ğŸ“‚ ì´ë¯¸ ë¡œë“œë¨ - ì´ {len(age_contraindication_ingredients)}ê°œ ì„±ë¶„")
        return age_contraindication_ingredients
    
    try:
        # ì—°ë ¹ëŒ€ ê¸ˆê¸° ì„±ë¶„ íŒŒì¼ ê²½ë¡œ
        age_contraindication_file = r"C:\Users\jung\Desktop\44\OpenData_PotOpenDurIngr_B20251117.xls"
        
        print(f"ğŸ” íŒŒì¼ ì¡´ì¬ í™•ì¸: {age_contraindication_file}")
        
        if not os.path.exists(age_contraindication_file):
            print(f"âš ï¸ ì—°ë ¹ëŒ€ ê¸ˆê¸° ì„±ë¶„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {age_contraindication_file}")
            age_contraindication_loaded = True
            return age_contraindication_ingredients
        
        print("ğŸ“Š ì—°ë ¹ëŒ€ ê¸ˆê¸° ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì¤‘...")
        df = pd.read_excel(age_contraindication_file)
        print(f"ğŸ“Š ì—‘ì…€ íŒŒì¼ ë¡œë“œ ì™„ë£Œ - í–‰ ìˆ˜: {len(df)}, ì»¬ëŸ¼: {list(df.columns)}")
        
        # ì‚¬ìš©í•  ì»¬ëŸ¼ í™•ì¸
        required_columns = ['DURì„±ë¶„ëª…', 'ì—°ë ¹ê¸°ì¤€', 'ê¸ˆê¸°ë‚´ìš©']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}")
            print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
            age_contraindication_loaded = True
            return age_contraindication_ingredients
        
        # ë°ì´í„° ì²˜ë¦¬
        processed_count = 0
        print(f"ğŸ” ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ - ì´ {len(df)}í–‰")
        
        for idx, row in df.iterrows():
            ingredient_name = str(row.get('DURì„±ë¶„ëª…', '')).strip()
            age_criteria = str(row.get('ì—°ë ¹ê¸°ì¤€', '')).strip()
            contraindication = str(row.get('ê¸ˆê¸°ë‚´ìš©', '')).strip()
            
            # NaN ê°’ ì²˜ë¦¬
            if pd.isna(row.get('DURì„±ë¶„ëª…')) or ingredient_name == 'nan' or not ingredient_name:
                continue
            
            if idx < 5:  # ì²˜ìŒ 5ê°œ í–‰ë§Œ ë¡œê·¸ ì¶œë ¥
                print(f"ğŸ” í–‰ {idx}: ì„±ë¶„ëª…='{ingredient_name}', ì—°ë ¹ê¸°ì¤€='{age_criteria}', ê¸ˆê¸°ë‚´ìš©='{contraindication[:50] if contraindication else 'ì—†ìŒ'}...'")
            
            # ì„±ë¶„ëª…ì´ ì´ë¯¸ ìˆìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            if ingredient_name not in age_contraindication_ingredients:
                age_contraindication_ingredients[ingredient_name] = {
                    'korean_name': ingredient_name,
                    'age_contraindications': []  # ì—¬ëŸ¬ ì—°ë ¹ëŒ€ë³„ ê¸ˆê¸° ì •ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
                }
            
            # ì—°ë ¹ëŒ€ë³„ ê¸ˆê¸° ì •ë³´ ì¶”ê°€
            if age_criteria and age_criteria != 'nan' and contraindication and contraindication != 'nan':
                age_contraindication_ingredients[ingredient_name]['age_contraindications'].append({
                    'age_criteria': age_criteria,
                    'contraindication': contraindication
                })
            
            processed_count += 1
        
        print(f"âœ… ì—°ë ¹ëŒ€ ê¸ˆê¸° ì„±ë¶„ {len(age_contraindication_ingredients)}ê°œ ë¡œë“œ ì™„ë£Œ (ì²˜ë¦¬ëœ í–‰: {processed_count}ê°œ)")
        print(f"ğŸ” ë¡œë“œëœ ì„±ë¶„ ì˜ˆì‹œ: {list(age_contraindication_ingredients.keys())[:5]}")
        age_contraindication_loaded = True
        
    except Exception as e:
        print(f"âŒ ì—°ë ¹ëŒ€ ê¸ˆê¸° ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        age_contraindication_loaded = True
    
    return age_contraindication_ingredients

def find_age_contraindication_info(ingredient_name: str) -> dict:
    """íŠ¹ì • ì„±ë¶„ì˜ ì—°ë ¹ëŒ€ë³„ ê¸ˆê¸° ì •ë³´ ì°¾ê¸°"""
    if not age_contraindication_loaded:
        load_age_contraindication_data()
    
    # ì •í™•í•œ ë§¤ì¹­ ì‹œë„
    if ingredient_name in age_contraindication_ingredients:
        return age_contraindication_ingredients[ingredient_name]
    
    # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
    for key, value in age_contraindication_ingredients.items():
        if ingredient_name in key or key in ingredient_name:
            return value
    
    # ì •ê·œí™”ëœ ë§¤ì¹­ ì‹œë„
    normalized_ingredient = re.sub(r'[^\wê°€-í£]', '', ingredient_name.lower())
    for key, value in age_contraindication_ingredients.items():
        normalized_key = re.sub(r'[^\wê°€-í£]', '', key.lower())
        if normalized_ingredient in normalized_key or normalized_key in normalized_ingredient:
            return value
    
    return None

def get_medicine_age_contraindications(medicine_name: str) -> List[dict]:
    """ì•½í’ˆì˜ ì£¼ì„±ë¶„ë“¤ ì¤‘ ì—°ë ¹ëŒ€ ê¸ˆê¸° ì„±ë¶„ì´ ìˆëŠ”ì§€ í™•ì¸"""
    print(f"ğŸ” ì—°ë ¹ëŒ€ ê¸ˆê¸° ì„±ë¶„ í™•ì¸ ì‹œì‘: '{medicine_name}'")
    
    contraindications = []
    
    # ì•½í’ˆì˜ ì£¼ì„±ë¶„ ì¶”ì¶œ
    ingredients = extract_active_ingredients_from_medicine(medicine_name)
    print(f"ğŸ” ì¶”ì¶œëœ ì£¼ì„±ë¶„: {ingredients}")
    
    for ingredient in ingredients:
        print(f"ğŸ” ì„±ë¶„ '{ingredient}' ì—°ë ¹ëŒ€ ê¸ˆê¸° í™•ì¸ ì¤‘...")
        age_info = find_age_contraindication_info(ingredient)
        if age_info and age_info.get('age_contraindications'):
            print(f"âœ… ì—°ë ¹ëŒ€ ê¸ˆê¸° ì„±ë¶„ ë°œê²¬: '{ingredient}' - {len(age_info['age_contraindications'])}ê°œ ê¸ˆê¸° ì •ë³´")
            contraindications.append({
                'ingredient': ingredient,
                'age_contraindication_info': age_info
            })
        else:
            print(f"âŒ ì—°ë ¹ëŒ€ ê¸ˆê¸° ì„±ë¶„ ì•„ë‹˜: '{ingredient}'")
    
    print(f"ğŸ” ìµœì¢… ì—°ë ¹ëŒ€ ê¸ˆê¸° ì„±ë¶„ ê°œìˆ˜: {len(contraindications)}")
    return contraindications

# === ì¼ì¼ ìµœëŒ€ íˆ¬ì—¬ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ===
daily_max_dosage_ingredients = {}  # ì„±ë¶„ëª… -> ì¼ì¼ ìµœëŒ€ íˆ¬ì—¬ëŸ‰ ì •ë³´ ë§¤í•‘
daily_max_dosage_loaded = False

def load_daily_max_dosage_data():
    """ì¼ì¼ ìµœëŒ€ íˆ¬ì—¬ëŸ‰ ì •ë³´ ë¡œë“œ (OpenData_DayMaxDosgQyInfo20251116.xls)"""
    global daily_max_dosage_ingredients, daily_max_dosage_loaded
    
    print(f"ğŸ” ì¼ì¼ ìµœëŒ€ íˆ¬ì—¬ëŸ‰ ì •ë³´ ë¡œë“œ ì‹œë„ - í˜„ì¬ ìƒíƒœ: loaded={daily_max_dosage_loaded}")
    
    if daily_max_dosage_loaded:
        print(f"ğŸ“‚ ì´ë¯¸ ë¡œë“œë¨ - ì´ {len(daily_max_dosage_ingredients)}ê°œ ì„±ë¶„")
        return daily_max_dosage_ingredients
    
    try:
        # ì¼ì¼ ìµœëŒ€ íˆ¬ì—¬ëŸ‰ íŒŒì¼ ê²½ë¡œ
        daily_max_dosage_file = r"C:\Users\jung\Desktop\55\OpenData_DayMaxDosgQyInfo20251116.xls"
        
        print(f"ğŸ” íŒŒì¼ ì¡´ì¬ í™•ì¸: {daily_max_dosage_file}")
        
        if not os.path.exists(daily_max_dosage_file):
            print(f"âš ï¸ ì¼ì¼ ìµœëŒ€ íˆ¬ì—¬ëŸ‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {daily_max_dosage_file}")
            daily_max_dosage_loaded = True
            return daily_max_dosage_ingredients
        
        print("ğŸ“Š ì¼ì¼ ìµœëŒ€ íˆ¬ì—¬ëŸ‰ ì •ë³´ ë¡œë“œ ì¤‘...")
        df = pd.read_excel(daily_max_dosage_file)
        print(f"ğŸ“Š ì—‘ì…€ íŒŒì¼ ë¡œë“œ ì™„ë£Œ - í–‰ ìˆ˜: {len(df)}, ì»¬ëŸ¼: {list(df.columns)}")
        
        # ì‚¬ìš©í•  ì»¬ëŸ¼ í™•ì¸
        required_columns = ['ì„±ë¶„ëª…(í•œê¸€)', 'ì œí˜•ëª…', 'íˆ¬ì—¬ë‹¨ìœ„', '1ì¼ìµœëŒ€íˆ¬ì—¬ëŸ‰']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}")
            print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
            daily_max_dosage_loaded = True
            return daily_max_dosage_ingredients
        
        # ë°ì´í„° ì²˜ë¦¬
        processed_count = 0
        print(f"ğŸ” ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ - ì´ {len(df)}í–‰")
        
        for idx, row in df.iterrows():
            ingredient_name = str(row.get('ì„±ë¶„ëª…(í•œê¸€)', '')).strip()
            formulation = str(row.get('ì œí˜•ëª…', '')).strip()
            dosage_unit = str(row.get('íˆ¬ì—¬ë‹¨ìœ„', '')).strip()
            max_daily_dosage = str(row.get('1ì¼ìµœëŒ€íˆ¬ì—¬ëŸ‰', '')).strip()
            
            # NaN ê°’ ì²˜ë¦¬
            if pd.isna(row.get('ì„±ë¶„ëª…(í•œê¸€)')) or ingredient_name == 'nan' or not ingredient_name:
                continue
            
            if idx < 5:  # ì²˜ìŒ 5ê°œ í–‰ë§Œ ë¡œê·¸ ì¶œë ¥
                print(f"ğŸ” í–‰ {idx}: ì„±ë¶„ëª…='{ingredient_name}', ì œí˜•='{formulation}', ë‹¨ìœ„='{dosage_unit}', ìµœëŒ€íˆ¬ì—¬ëŸ‰='{max_daily_dosage}'")
            
            # ì„±ë¶„ëª…ì´ ì´ë¯¸ ìˆìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            if ingredient_name not in daily_max_dosage_ingredients:
                daily_max_dosage_ingredients[ingredient_name] = {
                    'korean_name': ingredient_name,
                    'dosage_info': []  # ì—¬ëŸ¬ ì œí˜•ë³„ íˆ¬ì—¬ëŸ‰ ì •ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
                }
            
            # ì œí˜•ë³„ íˆ¬ì—¬ëŸ‰ ì •ë³´ ì¶”ê°€
            if max_daily_dosage and max_daily_dosage != 'nan':
                daily_max_dosage_ingredients[ingredient_name]['dosage_info'].append({
                    'formulation': formulation if formulation != 'nan' else '',
                    'dosage_unit': dosage_unit if dosage_unit != 'nan' else '',
                    'max_daily_dosage': max_daily_dosage
                })
            
            processed_count += 1
        
        print(f"âœ… ì¼ì¼ ìµœëŒ€ íˆ¬ì—¬ëŸ‰ ì •ë³´ {len(daily_max_dosage_ingredients)}ê°œ ì„±ë¶„ ë¡œë“œ ì™„ë£Œ (ì²˜ë¦¬ëœ í–‰: {processed_count}ê°œ)")
        print(f"ğŸ” ë¡œë“œëœ ì„±ë¶„ ì˜ˆì‹œ: {list(daily_max_dosage_ingredients.keys())[:5]}")
        daily_max_dosage_loaded = True
        
    except Exception as e:
        print(f"âŒ ì¼ì¼ ìµœëŒ€ íˆ¬ì—¬ëŸ‰ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        daily_max_dosage_loaded = True
    
    return daily_max_dosage_ingredients

def find_daily_max_dosage_info(ingredient_name: str, formulation: str = None) -> dict:
    """íŠ¹ì • ì„±ë¶„ì˜ ì¼ì¼ ìµœëŒ€ íˆ¬ì—¬ëŸ‰ ì •ë³´ ì°¾ê¸°"""
    if not daily_max_dosage_loaded:
        load_daily_max_dosage_data()
    
    # ì •í™•í•œ ë§¤ì¹­ ì‹œë„
    if ingredient_name in daily_max_dosage_ingredients:
        ingredient_info = daily_max_dosage_ingredients[ingredient_name]
        
        # ì œí˜•ì´ ì§€ì •ëœ ê²½ìš° í•´ë‹¹ ì œí˜• ì •ë³´ë§Œ ë°˜í™˜
        if formulation:
            for dosage_info in ingredient_info.get('dosage_info', []):
                if formulation in dosage_info.get('formulation', '') or dosage_info.get('formulation', '') in formulation:
                    return {
                        'ingredient': ingredient_name,
                        'formulation': dosage_info.get('formulation', ''),
                        'dosage_unit': dosage_info.get('dosage_unit', ''),
                        'max_daily_dosage': dosage_info.get('max_daily_dosage', '')
                    }
        
        # ì œí˜•ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë§¤ì¹­ë˜ì§€ ì•Šì€ ê²½ìš° ì²« ë²ˆì§¸ ì •ë³´ ë°˜í™˜
        if ingredient_info.get('dosage_info'):
            first_info = ingredient_info['dosage_info'][0]
            return {
                'ingredient': ingredient_name,
                'formulation': first_info.get('formulation', ''),
                'dosage_unit': first_info.get('dosage_unit', ''),
                'max_daily_dosage': first_info.get('max_daily_dosage', ''),
                'all_formulations': ingredient_info.get('dosage_info', [])  # ëª¨ë“  ì œí˜• ì •ë³´ë„ í¬í•¨
            }
        
        return ingredient_info
    
    # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
    for key, value in daily_max_dosage_ingredients.items():
        if ingredient_name in key or key in ingredient_name:
            if value.get('dosage_info'):
                first_info = value['dosage_info'][0]
                return {
                    'ingredient': key,
                    'formulation': first_info.get('formulation', ''),
                    'dosage_unit': first_info.get('dosage_unit', ''),
                    'max_daily_dosage': first_info.get('max_daily_dosage', ''),
                    'all_formulations': value.get('dosage_info', [])
                }
    
    # ì •ê·œí™”ëœ ë§¤ì¹­ ì‹œë„
    normalized_ingredient = re.sub(r'[^\wê°€-í£]', '', ingredient_name.lower())
    for key, value in daily_max_dosage_ingredients.items():
        normalized_key = re.sub(r'[^\wê°€-í£]', '', key.lower())
        if normalized_ingredient in normalized_key or normalized_key in normalized_ingredient:
            if value.get('dosage_info'):
                first_info = value['dosage_info'][0]
                return {
                    'ingredient': key,
                    'formulation': first_info.get('formulation', ''),
                    'dosage_unit': first_info.get('dosage_unit', ''),
                    'max_daily_dosage': first_info.get('max_daily_dosage', ''),
                    'all_formulations': value.get('dosage_info', [])
                }
    
    return None

def get_medicine_daily_max_dosage(medicine_name: str) -> List[dict]:
    """ì•½í’ˆì˜ ì£¼ì„±ë¶„ë“¤ ì¤‘ ì¼ì¼ ìµœëŒ€ íˆ¬ì—¬ëŸ‰ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸"""
    print(f"ğŸ” ì¼ì¼ ìµœëŒ€ íˆ¬ì—¬ëŸ‰ ì •ë³´ í™•ì¸ ì‹œì‘: '{medicine_name}'")
    
    dosage_infos = []
    
    # ì•½í’ˆì˜ ì£¼ì„±ë¶„ ì¶”ì¶œ
    ingredients = extract_active_ingredients_from_medicine(medicine_name)
    print(f"ğŸ” ì¶”ì¶œëœ ì£¼ì„±ë¶„: {ingredients}")
    
    for ingredient in ingredients:
        print(f"ğŸ” ì„±ë¶„ '{ingredient}' ì¼ì¼ ìµœëŒ€ íˆ¬ì—¬ëŸ‰ í™•ì¸ ì¤‘...")
        dosage_info = find_daily_max_dosage_info(ingredient)
        if dosage_info:
            print(f"âœ… ì¼ì¼ ìµœëŒ€ íˆ¬ì—¬ëŸ‰ ì •ë³´ ë°œê²¬: '{ingredient}' - {dosage_info.get('max_daily_dosage', 'ì •ë³´ ì—†ìŒ')}")
            dosage_infos.append(dosage_info)
        else:
            print(f"âŒ ì¼ì¼ ìµœëŒ€ íˆ¬ì—¬ëŸ‰ ì •ë³´ ì—†ìŒ: '{ingredient}'")
    
    print(f"ğŸ” ìµœì¢… ì¼ì¼ ìµœëŒ€ íˆ¬ì—¬ëŸ‰ ì •ë³´ ê°œìˆ˜: {len(dosage_infos)}")
    return dosage_infos

# === Export ëŒ€ìƒ ===
__all__ = [
    "pdf_retriever",
    "excel_retriever",
    "product_names",
    "product_names_normalized",
    "search_agent",
    "summarize_structured_json",
    "extract_active_ingredients_from_medicine",
    "pdf_product_index",
    "excel_product_index",
    "pdf_structured_docs",
    "excel_docs",
    "known_ingredients",
    "ingredient_to_products_map",
    "find_products_by_ingredient",
    "load_dosage_warning_data",
    "find_dosage_warning_info",
    "get_medicine_dosage_warnings",
    "load_age_contraindication_data",
    "find_age_contraindication_info",
    "get_medicine_age_contraindications",
    "load_daily_max_dosage_data",
    "find_daily_max_dosage_info",
    "get_medicine_daily_max_dosage"
]
