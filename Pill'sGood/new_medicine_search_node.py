# new_medicine_search_node.py - ì‹ ì•½ ê´€ë ¨ ì§ˆë¬¸ ì „ìš© ê²€ìƒ‰ ë…¸ë“œ
# PLUS í´ë”ì˜ sns_node.pyë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
# ê¸°ì¡´ sns_search_nodeëŠ” ì•½í’ˆ ì‚¬ìš© ê°€ëŠ¥ì„± íŒë‹¨ì˜ ë³´ì¡° ì •ë³´ ìˆ˜ì§‘ìš©ìœ¼ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€

import os
import re
from typing import List, Dict, Optional
import requests
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qa_state import QAState
from medical_patterns import *
from dotenv import load_dotenv
from answer_utils import generate_response_llm_from_prompt

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë„¤ì´ë²„ ë‰´ìŠ¤ API
from naver_news_api import NaverNewsAPI

# ==================== API ì„¤ì • í•¨ìˆ˜ ====================

def setup_youtube_api():
    """ìœ íŠœë¸Œ API ì„¤ì •"""
    api_key = os.getenv("YOUTUBE_API_KEY")
    if not api_key:
        raise ValueError("YOUTUBE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return api_key

# ==================== ìœ íŠœë¸Œ ê²€ìƒ‰ í•¨ìˆ˜ ====================

def extract_keywords(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    keywords = re.findall(r'\b\w+\b', text.lower())
    return keywords

def search_youtube_videos(query: str, max_videos: int = 10) -> List[Dict]:
    """ìœ íŠœë¸Œì—ì„œ ì•½í’ˆ ê´€ë ¨ ì˜ìƒ ê²€ìƒ‰"""
    try:
        api_key = setup_youtube_api()
        videos = []
        
        # ìœ íŠœë¸Œ ê²€ìƒ‰ API ì—”ë“œí¬ì¸íŠ¸
        search_url = "https://www.googleapis.com/youtube/v3/search"
        
        # ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
        params = {
            'part': 'snippet',
            'q': query,
            'key': api_key,
            'maxResults': max_videos,
            'type': 'video',
            'relevanceLanguage': 'ko',  # í•œêµ­ì–´ ìš°ì„ 
            'videoDuration': 'medium',  # ì¤‘ê°„ ê¸¸ì´ ì˜ìƒ (5-20ë¶„)
            'order': 'relevance'
        }
        
        # ê²€ìƒ‰ ìš”ì²­
        response = requests.get(search_url, params=params)
        response.raise_for_status()
        
        search_results = response.json()
        
        if 'items' not in search_results:
            print(f"âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤: {query}")
            return []
        
        for item in search_results['items']:
            snippet = item['snippet']
            video_id = item['id']['videoId']
            
            # ì˜ìƒ ì •ë³´ ì¶”ì¶œ
            video_info = {
                "title": snippet['title'],
                "description": snippet['description'],
                "channel_title": snippet['channelTitle'],
                "published_at": snippet['publishedAt'],
                "video_id": video_id,
                "thumbnail": snippet['thumbnails']['medium']['url'],
                "source": "youtube",
                "keywords": extract_keywords(snippet['title'] + " " + snippet['description'])
            }
            
            videos.append(video_info)
        
        print(f"âœ… '{query}' ê²€ìƒ‰ ê²°ê³¼: {len(videos)}ê°œ ì˜ìƒ")
        return videos
        
    except Exception as e:
        print(f"âŒ ìœ íŠœë¸Œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

def get_video_transcript(video_id: str) -> str:
    """ìœ íŠœë¸Œ ì˜ìƒì˜ ìë§‰/ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
    try:
        from youtube_transcript_api import YouTubeTranscriptApi
        from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound, VideoUnavailable
        
        # YouTubeTranscriptApi ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        ytt_api = YouTubeTranscriptApi()
        
        # ë°©ë²• 1: fetch ë©”ì„œë“œë¡œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸° (ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•)
        try:
            transcript = ytt_api.fetch(video_id, languages=['ko', 'en'])
            
            if transcript:
                # ìë§‰ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
                # transcriptëŠ” FetchedTranscript ê°ì²´ì´ê³ , ê° itemì€ FetchedTranscriptSnippet ê°ì²´
                full_transcript = ""
                for snippet in transcript:
                    # FetchedTranscriptSnippet ê°ì²´ëŠ” text ì†ì„±ì„ ê°€ì§
                    if hasattr(snippet, 'text'):
                        full_transcript += snippet.text + " "
                    elif isinstance(snippet, dict) and 'text' in snippet:
                        full_transcript += snippet['text'] + " "
                    elif isinstance(snippet, str):
                        full_transcript += snippet + " "
                
                if full_transcript.strip():
                    print(f"âœ… ì˜ìƒ {video_id} ìë§‰ ì¶”ì¶œ ì„±ê³µ: {len(full_transcript)}ì")
                    return full_transcript.strip()
                else:
                    return ""
        except (TranscriptsDisabled, NoTranscriptFound, VideoUnavailable) as e:
            # ìë§‰ì´ ë¹„í™œì„±í™”ë˜ì—ˆê±°ë‚˜ ì—†ëŠ” ê²½ìš°
            print(f"âš ï¸ ì˜ìƒ {video_id} ìë§‰ ì—†ìŒ: {type(e).__name__}")
            return ""
        except Exception as e1:
            # ë‹¤ë¥¸ ì˜ˆì™¸ ë°œìƒ ì‹œ list ë©”ì„œë“œë¡œ ì‹œë„
            print(f"âš ï¸ fetch ì‹¤íŒ¨, list ë©”ì„œë“œë¡œ ì‹œë„: {type(e1).__name__}")
            
            # ë°©ë²• 2: list ë©”ì„œë“œë¡œ ìë§‰ ëª©ë¡ ê°€ì ¸ì˜¨ í›„ ì„ íƒ
            try:
                transcript_list = ytt_api.list(video_id)
                
                # í•œêµ­ì–´ ìë§‰ ìš°ì„ , ì—†ìœ¼ë©´ ì˜ì–´ ìë§‰
                transcript = transcript_list.find_transcript(['ko', 'en'])
                transcript_data = transcript.fetch()
                
                if transcript_data:
                    # ìë§‰ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
                    # transcript_dataëŠ” FetchedTranscript ê°ì²´
                    full_transcript = ""
                    for snippet in transcript_data:
                        # FetchedTranscriptSnippet ê°ì²´ëŠ” text ì†ì„±ì„ ê°€ì§
                        if hasattr(snippet, 'text'):
                            full_transcript += snippet.text + " "
                        elif isinstance(snippet, dict) and 'text' in snippet:
                            full_transcript += snippet['text'] + " "
                        elif isinstance(snippet, str):
                            full_transcript += snippet + " "
                    
                    if full_transcript.strip():
                        print(f"âœ… ì˜ìƒ {video_id} ìë§‰ ì¶”ì¶œ ì„±ê³µ (list): {len(full_transcript)}ì")
                        return full_transcript.strip()
                    else:
                        return ""
            except (TranscriptsDisabled, NoTranscriptFound, VideoUnavailable) as e2:
                print(f"âš ï¸ ì˜ìƒ {video_id} ìë§‰ ì—†ìŒ (list): {type(e2).__name__}")
                return ""
            except Exception as e2:
                print(f"âŒ ìë§‰ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (list): {type(e2).__name__}: {e2}")
                return ""
        
        print(f"âš ï¸ ì˜ìƒ {video_id} ìë§‰ì´ ì—†ìŠµë‹ˆë‹¤")
        return ""
            
    except Exception as e:
        print(f"âŒ ìë§‰ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return ""

# ==================== ìš”ì•½ ë° ë¶„ì„ í•¨ìˆ˜ ====================

def summarize_video_content(content: str, max_length: int = 500) -> str:
    """ì˜ìƒ ë‚´ìš©ì„ ìš”ì•½"""
    try:
        if len(content) <= max_length:
            return content
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì‚¬ìš©
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        
        chunks = text_splitter.split_text(content)
        
        # ì²« ë²ˆì§¸ ì²­í¬ì™€ ë§ˆì§€ë§‰ ì²­í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì•½
        if len(chunks) >= 2:
            summary = chunks[0][:max_length//2] + "...\n\n" + chunks[-1][:max_length//2]
        else:
            summary = chunks[0][:max_length]
        
        return summary
        
    except Exception as e:
        print(f"âŒ ë‚´ìš© ìš”ì•½ ì‹¤íŒ¨: {e}")
        return content[:max_length] if len(content) > max_length else content

def extract_disease_name_with_llm(query: str) -> Optional[str]:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì—ì„œ ì§ˆë³‘ëª… ì¶”ì¶œ"""
    try:
        extraction_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ì‹ ì•½ê³¼ ê´€ë ¨ëœ ì§ˆë³‘ëª…ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

**ì§ˆë¬¸:** {query}

**ì§€ì‹œì‚¬í•­:**
1. ì§ˆë¬¸ì—ì„œ ì§ˆë³‘ëª…(ì˜ˆ: ì¹˜ë§¤, ë‹¹ë‡¨, ë‹¹ë‡¨ë³‘, ê³ í˜ˆì••, ì•”, ì•Œì¸ í•˜ì´ë¨¸ ë“±)ì„ ì°¾ì•„ì£¼ì„¸ìš”.
2. "ê·¸ëŸ¼", "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ê´€ë ¨", "ëŒ€í•œ", "ì— ê´€í•œ", "ì— ëŒ€í•œ", "ì •ë³´", "ë‰´ìŠ¤", "ì•Œë ¤ì¤˜" ê°™ì€ ì¡°ì‚¬ë‚˜ ì¼ë°˜ ë‹¨ì–´ëŠ” ë¬´ì‹œí•˜ì„¸ìš”.
3. ì§ˆë³‘ëª…ë§Œ ì¶”ì¶œí•˜ê³ , ë‹¤ë¥¸ ë‹¨ì–´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
4. ì§ˆë³‘ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ "ì—†ìŒ"ì´ë¼ê³  ë‹µí•˜ì„¸ìš”.

**ì‘ë‹µ í˜•ì‹:**
ì§ˆë³‘ëª…ë§Œ ë‹µí•˜ì„¸ìš”. ì˜ˆ: ì¹˜ë§¤, ë‹¹ë‡¨, ê³ í˜ˆì••, ì—†ìŒ
"""
        
        response = generate_response_llm_from_prompt(
            prompt=extraction_prompt,
            temperature=0.1,
            max_tokens=50,
            cache_type="disease_extraction",
            use_cache=True
        )
        
        # ì‘ë‹µ ì •ë¦¬
        disease_name = response.strip()
        
        # "ì—†ìŒ" ë˜ëŠ” ë¹ˆ ë¬¸ìì—´ ì²´í¬
        if not disease_name or disease_name.lower() in ["ì—†ìŒ", "none", "ì—†ì–´", "ì°¾ì„ ìˆ˜ ì—†ìŒ"]:
            return None
        
        # ë¶ˆí•„ìš”í•œ ì„¤ëª… ì œê±° (ì˜ˆ: "ì§ˆë³‘ëª…: ì¹˜ë§¤" â†’ "ì¹˜ë§¤")
        if ":" in disease_name:
            disease_name = disease_name.split(":")[-1].strip()
        
        # ê³µë°± ì œê±°
        disease_name = disease_name.strip()
        
        # ë„ˆë¬´ ê¸´ ê²½ìš° (ì˜ëª»ëœ ì¶”ì¶œ ê°€ëŠ¥ì„±) ì œì™¸
        if len(disease_name) > 10:
            print(f"âš ï¸ ì¶”ì¶œëœ ì§ˆë³‘ëª…ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: '{disease_name}', ë¬´ì‹œí•©ë‹ˆë‹¤.")
            return None
        
        return disease_name if disease_name else None
        
    except Exception as e:
        print(f"âŒ LLM ì§ˆë³‘ëª… ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def analyze_query_intent(query: str) -> Dict[str, any]:
    """ì¿¼ë¦¬ì˜ ì˜ë„ì™€ í•µì‹¬ ìš”ì†Œë¥¼ ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„ (PLUS ê°œì„  ë²„ì „)"""
    query_lower = query.lower()
    
    # 1. ì˜ë„ë³„ ì ìˆ˜ ê³„ì‚°
    intent_scores = {
        "pain_relief": 0,
        "discomfort_relief": 0,
        "side_effect": 0,
        "experience_review": 0,
        "efficacy": 0,
        "latest_info": 0,
        "general_info": 0
    }
    
    # í†µì¦ ê´€ë ¨ ì˜ë„ ì ìˆ˜
    for pattern in PAIN_PATTERNS:
        if re.search(pattern, query_lower):
            intent_scores["pain_relief"] += 3
            if re.search(r'ë„ˆë¬´|ë§¤ìš°|ì •ë§|ì—„ì²­|ì‹¬í•˜ê²Œ', query_lower):
                intent_scores["pain_relief"] += 2
    
    # ë¶ˆí¸í•¨ ê´€ë ¨ ì˜ë„ ì ìˆ˜
    for pattern in DISCOMFORT_PATTERNS:
        if re.search(pattern, query_lower):
            intent_scores["discomfort_relief"] += 3
    
    # ë¶€ì‘ìš© ê´€ë ¨ ì˜ë„ ì ìˆ˜
    for pattern in SIDE_EFFECT_PATTERNS:
        if re.search(pattern, query_lower):
            intent_scores["side_effect"] += 5
            if re.search(r'ë¶€ì‘ìš©|ë‚˜ë¹ ì¡Œì–´|ì•…í™”|ìƒˆë¡œ\s*ìƒê²¼ì–´', query_lower):
                intent_scores["side_effect"] += 2
    
    # ê²½í—˜ë‹´ ê´€ë ¨ ì˜ë„ ì ìˆ˜
    for pattern in EXPERIENCE_PATTERNS:
        if re.search(pattern, query_lower):
            intent_scores["experience_review"] += 3
            if re.search(r'ê²½í—˜ë‹´|í›„ê¸°|ê²½í—˜|ì‚¬ìš©í›„ê¸°|ë³µìš©í›„ê¸°', query_lower):
                intent_scores["experience_review"] += 1
    
    # íš¨ëŠ¥ ê´€ë ¨ ì˜ë„ ì ìˆ˜
    for pattern in EFFICACY_PATTERNS:
        if re.search(pattern, query_lower):
            intent_scores["efficacy"] += 3
    
    # ìµœì‹  ì •ë³´ ê´€ë ¨ ì˜ë„ ì ìˆ˜ (ì‹ ì•½ ê´€ë ¨ ì§ˆë¬¸ì— ì¤‘ìš”)
    for pattern in LATEST_PATTERNS:
        if re.search(pattern, query_lower):
            intent_scores["latest_info"] += 3
            if re.search(r'2024|2023|ìƒˆë¡œ|ì‹ ì•½', query_lower):
                intent_scores["latest_info"] += 1
    
    # ì¼ë°˜ ì •ë³´ ê¸°ë³¸ ì ìˆ˜
    intent_scores["general_info"] = 1
    
    # 2. ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì˜ë„ ì„ íƒ
    intent = max(intent_scores, key=intent_scores.get)
    
    # 3. ë¶€ì‘ìš© ì˜ë„ê°€ ìˆëŠ” ê²½ìš° ìš°ì„ ìˆœìœ„ ì¡°ì •
    print(f"ğŸ” ë¶€ì‘ìš© í‚¤ì›Œë“œ ì²´í¬: 'ë¶€ì‘ìš©' in '{query_lower}' = {'ë¶€ì‘ìš©' in query_lower}")
    if "ë¶€ì‘ìš©" in query_lower:
        print(f"âœ… ë¶€ì‘ìš© í‚¤ì›Œë“œ ë°œê²¬! í˜„ì¬ ì˜ë„ ì ìˆ˜: {intent_scores}")
        if intent_scores["side_effect"] > 0 and intent_scores["experience_review"] > 0:
            if intent_scores["side_effect"] >= intent_scores["experience_review"]:
                intent = "side_effect"
                print(f"ğŸ¯ ë¶€ì‘ìš© ì˜ë„ë¡œ ì„¤ì • (ì ìˆ˜ ë¹„êµ)")
            else:
                intent = "side_effect_experience"
                print(f"ğŸ¯ ë³µí•© ì˜ë„ë¡œ ì„¤ì •: side_effect_experience")
        elif intent_scores["side_effect"] > 0:
            intent = "side_effect"
            print(f"ğŸ¯ ë¶€ì‘ìš© ì˜ë„ë¡œ ì„¤ì • (ê¸°ì¡´ ì ìˆ˜)")
        else:
            intent = "side_effect"
            intent_scores["side_effect"] = 6
            print(f"ğŸ¯ ë¶€ì‘ìš© ì˜ë„ë¡œ ê°•ì œ ì„¤ì • (í‚¤ì›Œë“œ ê¸°ë°˜)")
    else:
        print(f"âŒ ë¶€ì‘ìš© í‚¤ì›Œë“œ ì—†ìŒ")
    
    # 4. í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (LLM ê¸°ë°˜ ì§ˆë³‘ëª… ì¶”ì¶œ)
    potential_drugs = []
    
    # LLMì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë³‘ëª… ì¶”ì¶œ
    if 'ì‹ ì•½' in query_lower:
        disease_name = extract_disease_name_with_llm(query)
        
        if disease_name:
            potential_drugs.append(f"{disease_name} ì‹ ì•½")
            print(f"âœ… LLM ê¸°ë°˜ ì§ˆë³‘ëª… ì¶”ì¶œ: '{disease_name} ì‹ ì•½'")
        else:
            potential_drugs.append("ì‹ ì•½")
            print(f"âš ï¸ LLM ì§ˆë³‘ëª… ì¶”ì¶œ ì‹¤íŒ¨, ì‹ ì•½ ë‹¨ë… ì‚¬ìš©")
    else:
        # ì‹ ì•½ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ì•½í’ˆëª… ì¶”ì¶œ ì‹œë„
        disease_name = extract_disease_name_with_llm(query)
        if disease_name:
            potential_drugs.append(disease_name)
            print(f"âœ… LLM ê¸°ë°˜ ì§ˆë³‘ëª… ì¶”ì¶œ: '{disease_name}'")
    
    # LLM ê¸°ë°˜ ì¶”ì¶œì´ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ í´ë°± (í•˜ì§€ë§Œ ì´ì œëŠ” LLMì´ ëŒ€ë¶€ë¶„ ì²˜ë¦¬)
    if not potential_drugs:
        print(f"âš ï¸ LLM ê¸°ë°˜ ì¶”ì¶œ ì‹¤íŒ¨, potential_drugsê°€ ë¹„ì–´ìˆìŒ")
    
    # 5. ì¦ìƒ ë¶€ìœ„/ì„±ê²© ì¶”ì¶œ
    body_parts = []
    
    for part_name, patterns in BODY_PART_PATTERNS.items():
        if any(re.search(pattern, query_lower) for pattern in patterns):
            body_parts.append(part_name)
    
    # 6. ì¦ìƒ ê°•ë„/ì„±ê²©
    intensity = "moderate"
    
    for intensity_level, patterns in INTENSITY_PATTERNS.items():
        if any(re.search(pattern, query_lower) for pattern in patterns):
            intensity = intensity_level
            break
    
    return {
        "intent": intent,
        "intent_scores": intent_scores,
        "potential_drugs": potential_drugs,
        "body_parts": body_parts,
        "intensity": intensity,
        "original_query": query
    }

def create_search_terms(analysis: Dict[str, any]) -> List[str]:
    """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²€ìƒ‰ì–´ ìƒì„± (ì‹ ì•½ ê´€ë ¨ ì§ˆë¬¸ ì „ìš©)"""
    search_terms = []
    
    intent = analysis.get("intent")
    potential_drugs = analysis.get("potential_drugs", [])
    body_parts = analysis.get("body_parts", [])
    intensity = analysis.get("intensity")
    
    # 1. í•µì‹¬ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì–´ ìƒì„± (ì‹ ì•½ ê´€ë ¨ ì§ˆë¬¸ ì „ìš©)
    if potential_drugs:
        for keyword in potential_drugs[:2]:  # "ì¹˜ë§¤ ì‹ ì•½" ê°™ì€ ë³µí•© í‚¤ì›Œë“œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if intent == "latest_info":
                # ì‹ ì•½ ê´€ë ¨ ìµœì‹  ì •ë³´ ê²€ìƒ‰
                search_terms.extend([
                    f"{keyword}",
                    f"{keyword} ë‰´ìŠ¤",
                    f"{keyword} ìµœì‹ ",
                    f"{keyword} ê°œë°œ",
                    f"{keyword} ìŠ¹ì¸",
                    f"{keyword} ì¶œì‹œ"
                ])
            elif intent == "side_effect":
                search_terms.extend([
                    f"{keyword} ë¶€ì‘ìš©",
                    f"{keyword} ë¶€ì‘ìš© ê²½í—˜"
                ])
            elif intent == "experience_review":
                search_terms.extend([
                    f"{keyword} ê²½í—˜ë‹´",
                    f"{keyword} ì‚¬ìš© í›„ê¸°"
                ])
            elif intent == "efficacy":
                search_terms.extend([
                    f"{keyword} íš¨ê³¼",
                    f"{keyword} íš¨ëŠ¥"
                ])
    
    # 2. ì¼ë°˜ì ì¸ ì˜ë„ë³„ ê²€ìƒ‰ì–´ (í•µì‹¬ í‚¤ì›Œë“œê°€ ì—†ì„ ë•Œë§Œ ì œí•œì ìœ¼ë¡œ ì‚¬ìš©)
    if not potential_drugs:
        if intent == "latest_info":
            search_terms.extend([
                "ì‹ ì•½ ìŠ¹ì¸",
                "ì‹ ì•½ ì¶œì‹œ",
                "ì‹ ì•½ ê°œë°œ"
            ])
        elif intent == "side_effect":
            search_terms.extend([
                "ì‹ ì•½ ë¶€ì‘ìš©"
            ])
        elif intent == "experience_review":
            search_terms.extend([
                "ì‹ ì•½ ê²½í—˜ë‹´"
            ])
    
    # 3. ë¶€ìœ„ë³„ ê²€ìƒ‰ì–´ ì¶”ê°€
    if body_parts:
        for part in body_parts:
            if intent == "side_effect":
                search_terms.append(f"{part} ë¶€ì‘ìš© ê²½í—˜")
            elif intent == "experience_review":
                search_terms.append(f"{part} ì¹˜ë£Œ ê²½í—˜")
    
    # 4. ê°•ë„ì— ë”°ë¥¸ ê²€ìƒ‰ì–´
    if intensity == "severe":
        search_terms.append("ì‹¬í•œ ë¶€ì‘ìš© ê²½í—˜")
    elif intensity == "mild":
        search_terms.append("ê°€ë²¼ìš´ ë¶€ì‘ìš© ê²½í—˜")
    
    # ì¤‘ë³µ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì •ë ¬
    unique_terms = list(dict.fromkeys(search_terms))
    
    # âœ… ì•½í’ˆëª…ì´ í¬í•¨ëœ ê²€ìƒ‰ì–´ë¥¼ ìš°ì„ ìœ¼ë¡œ ì •ë ¬
    if potential_drugs:
        drug_terms = [t for t in unique_terms if any(drug in t for drug in potential_drugs)]
        other_terms = [t for t in unique_terms if not any(drug in t for drug in potential_drugs)]
        unique_terms = drug_terms + other_terms[:2]  # ì•½í’ˆëª… ê²€ìƒ‰ì–´ + ì¼ë°˜ ê²€ìƒ‰ì–´ ìµœëŒ€ 2ê°œë§Œ
    else:
        # ì•½í’ˆëª…ì´ ì—†ìœ¼ë©´ ê²€ìƒ‰ì–´ë¥¼ ë” ì œí•œ
        unique_terms = unique_terms[:3]
    
    print(f"ğŸ“Š ìµœì¢… ê²€ìƒ‰ì–´ ëª©ë¡ (ìš°ì„ ìˆœìœ„ ì •ë ¬): {unique_terms[:8]}")
    return unique_terms[:8]  # ê²€ìƒ‰ì–´ë¥¼ 8ê°œë¡œ ì œí•œ

def filter_relevant_videos(videos: List[Dict], analysis: Dict[str, any]) -> List[Dict]:
    """ì›ë³¸ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì— ë”°ë¼ ì˜ìƒ í•„í„°ë§"""
    relevant_videos = []
    
    intent = analysis.get("intent")
    potential_drugs = analysis.get("potential_drugs", [])
    body_parts = analysis.get("body_parts", [])
    
    for video in videos:
        content_lower = (video["title"] + " " + video["description"]).lower()
        relevance_score = 0
        
        # 1. ì˜ë„ë³„ ê´€ë ¨ì„± ì ìˆ˜
        if intent == "side_effect":
            side_effect_keywords = ['ë¶€ì‘ìš©', 'adverse', 'negative', 'problem', 'issue', 'trouble', 'bad', 'unwanted', 'reaction']
            if any(keyword in content_lower for keyword in side_effect_keywords):
                relevance_score += 3
            else:
                continue
        
        elif intent == "experience_review":
            experience_keywords = ['ê²½í—˜', 'review', 'í›„ê¸°', 'testimonial', 'story', 'used', 'ë³µìš©', 'ì‚¬ìš©', 'took', 'tried']
            if any(keyword in content_lower for keyword in experience_keywords):
                relevance_score += 3
            else:
                continue
        
        elif intent == "latest_info":
            latest_keywords = ['ì‹ ì•½', 'ìƒˆë¡œìš´', 'ìµœì‹ ', 'ê°œë°œ', 'ìŠ¹ì¸', 'ìƒë¥™', 'ì¶œì‹œ', 'new', 'latest', 'development']
            if any(keyword in content_lower for keyword in latest_keywords):
                relevance_score += 3
            else:
                continue
        
        # 2. ì•½í’ˆëª… ê´€ë ¨ì„±
        if potential_drugs:
            for drug in potential_drugs:
                if drug.lower() in content_lower:
                    relevance_score += 4
                    break
        
        # 3. ë¶€ìœ„ ê´€ë ¨ì„±
        if body_parts:
            for part in body_parts:
                if part in content_lower:
                    relevance_score += 2
        
        # 4. ì œëª© ê´€ë ¨ì„± ì ìˆ˜
        if any(keyword in video["title"].lower() for keyword in ['ì•½', 'ê°ê¸°', 'cold', 'flu', 'medicine', 'drug', 'ì‹ ì•½', 'ì¹˜ë£Œ']):
            relevance_score += 1
        
        # ê´€ë ¨ì„± ì ìˆ˜ê°€ ì¼ì • ìˆ˜ì¤€ ì´ìƒì¸ ì˜ìƒë§Œ í¬í•¨
        if relevance_score >= 3:
            video["relevance_score"] = relevance_score
            relevant_videos.append(video)
    
    # ê´€ë ¨ì„± ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
    relevant_videos.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
    
    # ìµœëŒ€ 5ê°œë¡œ ì œí•œ
    return relevant_videos[:5]

def filter_relevant_news(news_items: List[Dict], analysis: Dict[str, any]) -> List[Dict]:
    """ì›ë³¸ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì— ë”°ë¼ ë„¤ì´ë²„ ë‰´ìŠ¤ í•„í„°ë§ (PLUS ê°œì„  ë²„ì „)"""
    relevant_news = []
    
    intent = analysis.get("intent")
    potential_drugs = analysis.get("potential_drugs", [])
    body_parts = analysis.get("body_parts", [])
    
    print(f"\nğŸ” ë‰´ìŠ¤ í•„í„°ë§ ì‹œì‘")
    print(f"   - ì•½í’ˆëª…: {potential_drugs}")
    print(f"   - ì˜ë„: {intent}")
    print(f"   - ì´ ë‰´ìŠ¤ ìˆ˜: {len(news_items)}")
    
    for idx, news in enumerate(news_items, 1):
        title = news.get("title", "")
        description = news.get("description", "")
        title_lower = title.lower()
        desc_lower = description.lower()
        content_lower = title_lower + " " + desc_lower
        relevance_score = 0
        score_details = []  # ì ìˆ˜ ìƒì„¸ ì •ë³´
        
        # 1. ì•½í’ˆëª… ì²´í¬ (ì•½í’ˆëª…ì´ ìˆëŠ” ê²½ìš°ë§Œ í•„ìˆ˜)
        drug_mentioned = False
        if potential_drugs:
            for drug in potential_drugs:
                # ì œëª©ì— ì•½í’ˆëª…ì´ ìˆìœ¼ë©´ ë†’ì€ ì ìˆ˜
                if drug.lower() in title_lower:
                    relevance_score += 10
                    drug_mentioned = True
                    score_details.append(f"ì•½í’ˆëª…(ì œëª©):+10")
                    break
                # ë‚´ìš©ì—ë§Œ ìˆìœ¼ë©´ ì¤‘ê°„ ì ìˆ˜
                elif drug.lower() in desc_lower:
                    relevance_score += 5
                    drug_mentioned = True
                    score_details.append(f"ì•½í’ˆëª…(ë‚´ìš©):+5")
                    break
            
            # ì•½í’ˆëª…ì´ ìˆëŠ” ì¿¼ë¦¬ì¸ë° ê¸°ì‚¬ì— ì—†ìœ¼ë©´ ì˜í•™ ê´€ë ¨ì´ë©´ ì•½ê°„ì˜ ì ìˆ˜
            if not drug_mentioned:
                medical_general = ['ì•½', 'ì˜ì•½í’ˆ', 'ì œì•½', 'ì„±ë¶„', 'ë³µìš©', 'ì²˜ë°©']
                if any(kw in content_lower for kw in medical_general):
                    relevance_score += 2
                    score_details.append("ì˜í•™ê´€ë ¨:+2")
                else:
                    score_details.append("ì•½í’ˆëª…ì—†ìŒ:ì œì™¸")
                    print(f"  [{idx}] âŒ ì œì™¸ (ì•½í’ˆëª… ì—†ìŒ): {title[:40]}...")
                    continue
        else:
            # ì•½í’ˆëª…ì´ ì—†ëŠ” ì¿¼ë¦¬ë©´ ê¸°ë³¸ ì ìˆ˜
            relevance_score += 3
            score_details.append("ê¸°ë³¸:+3")
        
        # 2. ì˜ë„ë³„ ê´€ë ¨ì„± ì ìˆ˜ (ì™„í™”)
        intent_matched = False
        if intent == "side_effect":
            side_effect_keywords = ['ë¶€ì‘ìš©', 'ì´ìƒë°˜ì‘', 'ìœ„í—˜', 'ì£¼ì˜', 'ê²½ê³ ', 'ë¦¬ì½œ', 'ë¬¸ì œ']
            matched_keywords = [kw for kw in side_effect_keywords if kw in content_lower]
            if matched_keywords:
                score = len(matched_keywords) * 3
                relevance_score += score
                intent_matched = True
                score_details.append(f"ë¶€ì‘ìš©í‚¤ì›Œë“œ({len(matched_keywords)}):+{score}")
        
        elif intent == "experience_review":
            experience_keywords = ['ì‚¬ìš©', 'ë³µìš©', 'íš¨ê³¼', 'ê²°ê³¼', 'ì‚¬ë¡€', 'ì„ìƒ', 'í›„ê¸°', 'ê²½í—˜']
            matched_keywords = [kw for kw in experience_keywords if kw in content_lower]
            if matched_keywords:
                score = len(matched_keywords) * 2
                relevance_score += score
                intent_matched = True
                score_details.append(f"ê²½í—˜í‚¤ì›Œë“œ({len(matched_keywords)}):+{score}")
        
        elif intent == "latest_info":
            latest_keywords = ['ì‹ ì•½', 'ìƒˆë¡œìš´', 'ìµœì‹ ', 'ê°œë°œ', 'ìŠ¹ì¸', 'ì¶œì‹œ', 'ë¡ ì¹­', 'í—ˆê°€', 'ë°œë§¤']
            matched_keywords = [kw for kw in latest_keywords if kw in content_lower]
            if matched_keywords:
                score = len(matched_keywords) * 3
                relevance_score += score
                intent_matched = True
                score_details.append(f"ìµœì‹ í‚¤ì›Œë“œ({len(matched_keywords)}):+{score}")
        
        elif intent == "efficacy":
            efficacy_keywords = ['íš¨ëŠ¥', 'íš¨ê³¼', 'ì‘ìš©', 'ì¹˜ë£Œ', 'ê°œì„ ', 'ì™„í™”', 'ì„ìƒ', 'ë„ì›€']
            matched_keywords = [kw for kw in efficacy_keywords if kw in content_lower]
            if matched_keywords:
                score = len(matched_keywords) * 2
                relevance_score += score
                intent_matched = True
                score_details.append(f"íš¨ëŠ¥í‚¤ì›Œë“œ({len(matched_keywords)}):+{score}")
        
        # ì˜ë„ í‚¤ì›Œë“œê°€ ì—†ì–´ë„ ì˜í•™ ê´€ë ¨ì´ë©´ ì•½ê°„ ê°€ì‚°
        if not intent_matched:
            medical_keywords = ['ì˜ì•½í’ˆ', 'ì œì•½', 'ì„±ë¶„', 'ì•½êµ­', 'ì˜ì‚¬', 'ë³‘ì›', 'í™˜ì', 'ì§ˆí™˜']
            matched_medical = [kw for kw in medical_keywords if kw in content_lower]
            if matched_medical:
                relevance_score += 2
                score_details.append(f"ì˜í•™í‚¤ì›Œë“œ:+2")
        
        # 3. ë¶€ìœ„ ê´€ë ¨ì„±
        if body_parts:
            for part in body_parts:
                if part in content_lower:
                    relevance_score += 2
                    score_details.append(f"ë¶€ìœ„({part}):+2")
        
        # 4. ë¬´ê´€í•œ í‚¤ì›Œë“œ ê°•ë ¥ ê°ì  (ì£¼ì‹/íˆ¬ì ê´€ë ¨ ê°•í™”)
        irrelevant_keywords = ['ì •ì¹˜', 'ì„ ê±°', 'ìŠ¤í¬ì¸ ', 'ì—°ì˜ˆ', 'ê²Œì„', 'ì£¼ì‹', 'ë¶€ë™ì‚°', 
                              'ê²½ì œì „ë§', 'ê¸ˆìœµì‹œì¥', 'íˆ¬ì', 'ì¦ê¶Œ', 'ì½”ì¸', 'ê°€ìƒí™”í',
                              'ìƒì¥', 'ì£¼ê°€', 'ê´€ë ¨ì£¼', 'íŠ¹ì§•ì£¼', 'ì¦ì‹œ', 'ì‹œì¥', 'ê±°ë˜',
                              'ë§¤ìˆ˜', 'ë§¤ë„', 'ì¢…ëª©', 'ê¸°ì—…ë¶„ì„', 'ì‹¤ì ', 'ë°°ë‹¹']
        matched_irrelevant = [kw for kw in irrelevant_keywords if kw in content_lower]
        if matched_irrelevant:
            # ì œëª©ì— ë¬´ê´€ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë” ê°•í•˜ê²Œ ê°ì 
            if any(kw in title_lower for kw in matched_irrelevant):
                relevance_score -= 15  # ì œëª©ì— ìˆìœ¼ë©´ ë” ê°•í•˜ê²Œ ê°ì 
                score_details.append(f"ë¬´ê´€(ì œëª©:{matched_irrelevant[0]}):âˆ’15")
            else:
                relevance_score -= 10
                score_details.append(f"ë¬´ê´€({matched_irrelevant[0]}):âˆ’10")
        
        # 5. ê´‘ê³ ì„± í‚¤ì›Œë“œ ê°ì 
        ad_keywords = ['í• ì¸', 'ì´ë²¤íŠ¸', 'íŠ¹ê°€', 'í”„ë¡œëª¨ì…˜', 'ì¿ í°', 'íŠ¹ë³„ê°€']
        matched_ad = [kw for kw in ad_keywords if kw in content_lower]
        if matched_ad:
            relevance_score -= 5
            score_details.append(f"ê´‘ê³ ({matched_ad[0]}):âˆ’5")
        
        # 6. í•µì‹¬ í‚¤ì›Œë“œê°€ ì œëª©ì— ëª…í™•íˆ ìˆëŠ” ê²½ìš° ì¶”ê°€ ì ìˆ˜ (ì‹ ì•½ ê´€ë ¨ ì§ˆë¬¸ì— ì¤‘ìš”)
        if potential_drugs:
            for drug in potential_drugs:
                # ì œëª©ì— í•µì‹¬ í‚¤ì›Œë“œê°€ ëª…í™•íˆ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì¶”ê°€ ì ìˆ˜
                if drug.lower() in title_lower:
                    # ì´ë¯¸ ìœ„ì—ì„œ ì ìˆ˜ë¥¼ ë°›ì•˜ì§€ë§Œ, ë” ëª…í™•í•œ ë§¤ì¹­ì¸ ê²½ìš° ì¶”ê°€ ì ìˆ˜
                    if len(drug.split()) > 1:  # "ì¹˜ë§¤ ì‹ ì•½" ê°™ì€ ë³µí•© í‚¤ì›Œë“œ
                        relevance_score += 5
                        score_details.append(f"í•µì‹¬í‚¤ì›Œë“œ(ì œëª©):+5")
        
        # ê´€ë ¨ì„± ì ìˆ˜ê°€ ì¼ì • ìˆ˜ì¤€ ì´ìƒì¸ ë‰´ìŠ¤ë§Œ í¬í•¨ (ì„ê³„ê°’ ê°•í™”: 8ì )
        # í•µì‹¬ í‚¤ì›Œë“œê°€ ì œëª©ì— ìˆìœ¼ë©´ 5ì  ì´ìƒë„ í—ˆìš©
        min_score = 8
        if potential_drugs:
            # í•µì‹¬ í‚¤ì›Œë“œê°€ ì œëª©ì— ìˆìœ¼ë©´ ìµœì†Œ ì ìˆ˜ ì™„í™”
            for drug in potential_drugs:
                if drug.lower() in title_lower:
                    min_score = 5
                    break
        
        if relevance_score >= min_score:
            news["relevance_score"] = relevance_score
            relevant_news.append(news)
            score_str = ", ".join(score_details)
            print(f"  [{idx}] âœ… ì„ íƒ [{relevance_score}ì ] ({score_str})")
            print(f"        ì œëª©: {title[:50]}...")
        else:
            score_str = ", ".join(score_details)
            print(f"  [{idx}] âŒ ì œì™¸ [{relevance_score}ì ] ({score_str})")
            print(f"        ì œëª©: {title[:50]}...")
    
    # ê´€ë ¨ì„± ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
    relevant_news.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
    
    # ìµœëŒ€ 10ê°œë¡œ ì œí•œ (ì¢€ ë” ë§ì´)
    print(f"\nğŸ¯ í•„í„°ë§ ì™„ë£Œ: {len(relevant_news)}ê°œ ë‰´ìŠ¤ ì¤‘ ìƒìœ„ {min(len(relevant_news), 10)}ê°œ ì„ íƒ")
    return relevant_news[:10]

# ==================== ì‹ ì•½ ê²€ìƒ‰ ë…¸ë“œ ====================

def new_medicine_search_node(state: QAState) -> QAState:
    """ì‹ ì•½ ê´€ë ¨ ì§ˆë¬¸ ì „ìš© ê²€ìƒ‰ ë…¸ë“œ (ìœ íŠœë¸Œ + ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ë°˜) - ì˜ìƒ ë‚´ìš© ì¶”ì¶œ ë° ìš”ì•½ í¬í•¨"""
    
    print("ğŸ” ì‹ ì•½ ê²€ìƒ‰ ë…¸ë“œ ì‹¤í–‰ ì‹œì‘ (ìœ íŠœë¸Œ + ë„¤ì´ë²„ ë‰´ìŠ¤)")
    
    # ì¿¼ë¦¬ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ (ì›ë³¸ ì¿¼ë¦¬ ìš°ì„  ì‚¬ìš©)
    query = state.get("query", "") or state.get("original_query", "")
    
    print(f"ğŸ“ ë¶„ì„í•  ì¿¼ë¦¬: {query}")
    
    if not query:
        print("âŒ ì¿¼ë¦¬ê°€ ì—†ì–´ì„œ ì‹ ì•½ ê²€ìƒ‰ ê±´ë„ˆëœ€")
        state["sns_results"] = []
        state["sns_count"] = 0
        return state
    
    # 1. ì¿¼ë¦¬ ì˜ë„ ë¶„ì„
    print("ğŸ§  ì¿¼ë¦¬ ì˜ë„ ë¶„ì„ ì‹œì‘")
    analysis = analyze_query_intent(query)
    print(f"ğŸ¯ ê°ì§€ëœ ì˜ë„: {analysis['intent']}")
    print(f"ğŸ“Š ì˜ë„ ì ìˆ˜: {analysis['intent_scores']}")
    print(f"ğŸ’Š ê°ì§€ëœ ì•½í’ˆ: {analysis['potential_drugs']}")
    print(f"ğŸ¦´ ê°ì§€ëœ ë¶€ìœ„: {analysis['body_parts']}")
    
    # 2. ê²€ìƒ‰ì–´ ìƒì„±
    print("ğŸ” ê²€ìƒ‰ì–´ ìƒì„± ì‹œì‘")
    search_terms = create_search_terms(analysis)
    print(f"ğŸ” ìƒì„±ëœ ê²€ìƒ‰ì–´: {search_terms}")
    
    all_videos = []
    all_news = []
    
    # 3. ê° ê²€ìƒ‰ì–´ë¡œ ìœ íŠœë¸Œ ê²€ìƒ‰
    print("ğŸ“º ìœ íŠœë¸Œ ê²€ìƒ‰ ì‹œì‘")
    for search_term in search_terms[:3]:  # ìµœëŒ€ 3ê°œ ê²€ìƒ‰ì–´ë§Œ ì‚¬ìš©
        try:
            print(f"ğŸ” ìœ íŠœë¸Œ '{search_term}' ê²€ìƒ‰ ì¤‘...")
            videos = search_youtube_videos(search_term, max_videos=5)
            print(f"ğŸ“ '{search_term}' ê²€ìƒ‰ ê²°ê³¼: {len(videos)}ê°œ ì˜ìƒ")
            all_videos.extend(videos)
        except Exception as e:
            print(f"âŒ ìœ íŠœë¸Œ '{search_term}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            continue
    
    # 4. ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ (ê´€ë ¨ì„± ìš°ì„ , ì •í™•ë„ìˆœ + ìµœì‹ ìˆœ í˜¼í•©)
    potential_drugs = analysis.get("potential_drugs", [])
    print("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘")
    print(f"   ê°ì§€ëœ í•µì‹¬ í‚¤ì›Œë“œ: {potential_drugs}")
    try:
        naver_api = NaverNewsAPI()
        
        # í•µì‹¬ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ (ì˜ˆ: "ì¹˜ë§¤ ì‹ ì•½")
        if potential_drugs:
            for keyword in potential_drugs[:2]:  # ìµœëŒ€ 2ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
                # ì •í™•ë„ìˆœ ê²€ìƒ‰ (ê´€ë ¨ì„± ë†’ì€ ë‰´ìŠ¤ ìš°ì„ )
                print(f"ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ '{keyword}' ê²€ìƒ‰ ì¤‘... (ì •í™•ë„ìˆœ)")
                news_items_sim = naver_api.search_news(keyword, display=15, sort="sim")
                print(f"ğŸ“ '{keyword}' ì •í™•ë„ìˆœ ê²€ìƒ‰ ê²°ê³¼: {len(news_items_sim)}ê°œ ë‰´ìŠ¤")
                all_news.extend(news_items_sim)
                
                # ìµœì‹ ìˆœ ê²€ìƒ‰ (ìµœì‹  ë‰´ìŠ¤ë„ ì¼ë¶€ í¬í•¨)
                print(f"ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ '{keyword}' ê²€ìƒ‰ ì¤‘... (ìµœì‹ ìˆœ)")
                news_items_date = naver_api.search_news(keyword, display=10, sort="date")
                print(f"ğŸ“ '{keyword}' ìµœì‹ ìˆœ ê²€ìƒ‰ ê²°ê³¼: {len(news_items_date)}ê°œ ë‰´ìŠ¤")
                all_news.extend(news_items_date)
        else:
            # í•µì‹¬ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê²€ìƒ‰ì–´ ì‚¬ìš©
            if search_terms:
                # ì²« ë²ˆì§¸ ê²€ìƒ‰ì–´ë§Œ ì‚¬ìš©
                search_term = search_terms[0]
                print(f"ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ '{search_term}' ê²€ìƒ‰ ì¤‘... (ì •í™•ë„ìˆœ)")
                news_items = naver_api.search_news(search_term, display=15, sort="sim")
                print(f"ğŸ“ '{search_term}' ê²€ìƒ‰ ê²°ê³¼: {len(news_items)}ê°œ ë‰´ìŠ¤")
                all_news.extend(news_items)
            else:
                print("âš ï¸ ê²€ìƒ‰ì–´ê°€ ì—†ì–´ ë‰´ìŠ¤ ê²€ìƒ‰ ê±´ë„ˆëœ€")
    except Exception as e:
        print(f"âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    print(f"ğŸ“Š ì´ ìˆ˜ì§‘ëœ ì˜ìƒ: {len(all_videos)}ê°œ")
    print(f"ğŸ“Š ì´ ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {len(all_news)}ê°œ")
    
    # ì¤‘ë³µ ë‰´ìŠ¤ ì œê±° (link ê¸°ì¤€)
    if all_news:
        seen_links = set()
        unique_news = []
        for news in all_news:
            link = news.get("link", "") or news.get("original_link", "")
            if link and link not in seen_links:
                seen_links.add(link)
                unique_news.append(news)
            elif not link:  # linkê°€ ì—†ìœ¼ë©´ ì œëª©ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
                title = news.get("title", "")
                if title not in [n.get("title", "") for n in unique_news]:
                    unique_news.append(news)
        all_news = unique_news
        print(f"ğŸ“Š ì¤‘ë³µ ì œê±° í›„ ë‰´ìŠ¤: {len(all_news)}ê°œ")
    
    # 5. ì˜ìƒ í•„í„°ë§
    print("ğŸ” ì˜ìƒ í•„í„°ë§ ì‹œì‘")
    filtered_videos = filter_relevant_videos(all_videos, analysis)
    print(f"âœ… í•„í„°ë§ í›„ ì˜ìƒ: {len(filtered_videos)}ê°œ")
    
    # 6. ë‰´ìŠ¤ í•„í„°ë§
    print("ğŸ” ë‰´ìŠ¤ í•„í„°ë§ ì‹œì‘")
    filtered_news = filter_relevant_news(all_news, analysis)
    print(f"âœ… í•„í„°ë§ í›„ ë‰´ìŠ¤: {len(filtered_news)}ê°œ")
    
    # 7. ì˜ìƒ ë‚´ìš© ì¶”ì¶œ ë° ìš”ì•½
    print("ğŸ“¹ ì˜ìƒ ë‚´ìš© ì¶”ì¶œ ë° ìš”ì•½ ì‹œì‘")
    enriched_videos = []
    for video in filtered_videos:
        try:
            # ìë§‰ ì¶”ì¶œ
            transcript = get_video_transcript(video["video_id"])
            
            if transcript:
                # ìë§‰ì´ ìˆìœ¼ë©´ ìš”ì•½
                summarized_content = summarize_video_content(transcript, max_length=800)
                video["transcript"] = transcript
                video["summarized_content"] = summarized_content
                video["has_transcript"] = True
                print(f"âœ… ì˜ìƒ {video['video_id']} ìë§‰ ì¶”ì¶œ ë° ìš”ì•½ ì™„ë£Œ")
            else:
                # ìë§‰ì´ ì—†ìœ¼ë©´ ì œëª©ê³¼ ì„¤ëª…ë§Œ ì‚¬ìš©
                content = f"ì œëª©: {video['title']}\nì„¤ëª…: {video['description']}"
                video["transcript"] = ""
                video["summarized_content"] = content
                video["has_transcript"] = False
                print(f"âš ï¸ ì˜ìƒ {video['video_id']} ìë§‰ ì—†ìŒ, ê¸°ë³¸ ì •ë³´ë§Œ ì‚¬ìš©")
            
            enriched_videos.append(video)
            
        except Exception as e:
            print(f"âŒ ì˜ìƒ {video['video_id']} ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ì •ë³´ëŠ” í¬í•¨
            content = f"ì œëª©: {video['title']}\nì„¤ëª…: {video['description']}"
            video["transcript"] = ""
            video["summarized_content"] = content
            video["has_transcript"] = False
            enriched_videos.append(video)
    
    # 8. Document í˜•íƒœë¡œ ë³€í™˜
    print("ğŸ“„ Document ë³€í™˜ ì‹œì‘")
    sns_docs = []
    
    # ìœ íŠœë¸Œ ì˜ìƒì„ Documentë¡œ ë³€í™˜
    for video in enriched_videos:
        # ìš”ì•½ëœ ë‚´ìš©ì„ ì£¼ìš” ì½˜í…ì¸ ë¡œ ì‚¬ìš©
        content = video["summarized_content"]
        
        doc = Document(
            page_content=content,
            metadata={
                "source": "youtube",
                "title": video.get("title", "ì œëª© ì—†ìŒ"),  # ì œëª© ì¶”ê°€!
                "video_id": video["video_id"],
                "channel_title": video["channel_title"],
                "keywords": video["keywords"],
                "relevance_score": video.get("relevance_score", 0),
                "type": "youtube_video",
                "search_intent": analysis["intent"],
                "detected_drugs": analysis.get("potential_drugs", []),
                "body_parts": analysis.get("body_parts", []),
                "thumbnail": video["thumbnail"],
                "published_at": video["published_at"],
                "has_transcript": video.get("has_transcript", False),
                "transcript_length": len(video.get("transcript", "")),
                "summary_length": len(video.get("summarized_content", "")),
                "summary": video.get("summarized_content", "")  # summaryë„ ì¶”ê°€
            }
        )
        sns_docs.append(doc)
    
    # ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ Documentë¡œ ë³€í™˜ (í•„í„°ë§ëœ ë‰´ìŠ¤ë§Œ ì‚¬ìš©)
    for news in filtered_news:
        content = f"ì œëª©: {news['title']}\në‚´ìš©: {news['description']}\në°œí–‰ì¼: {news.get('pub_date_parsed', news.get('pub_date', ''))}"
        
        doc = Document(
            page_content=content,
            metadata={
                "source": "naver_news",
                "title": news["title"],
                "link": news.get("link", ""),
                "original_link": news.get("original_link", ""),
                "type": "news_article",
                "search_intent": analysis["intent"],
                "detected_drugs": analysis.get("potential_drugs", []),
                "pub_date": news.get("pub_date_parsed", news.get("pub_date", "")),
                "relevance_score": news.get("relevance_score", 0)
            }
        )
        sns_docs.append(doc)
    
    # ê²°ê³¼ë¥¼ stateì— ì €ì¥
    state["sns_results"] = sns_docs
    state["sns_count"] = len(sns_docs)
    state["sns_analysis"] = analysis
    
    print(f"ğŸ‰ ì‹ ì•½ ê²€ìƒ‰ ì™„ë£Œ: {len(sns_docs)}ê°œ ê²°ê³¼")
    print(f"ğŸ“º ìœ íŠœë¸Œ: {len(enriched_videos)}ê°œ ì˜ìƒ")
    print(f"ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤: {len(filtered_news)}ê°œ ê¸°ì‚¬")
    print(f"ğŸ“Š ìë§‰ ìˆëŠ” ì˜ìƒ: {sum(1 for v in enriched_videos if v.get('has_transcript', False))}ê°œ")
    
    return state

